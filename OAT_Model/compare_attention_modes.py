"""
Simple Attention Mode Comparison Script
ë‹¨ìˆœí•œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜ëŠ” hyperparameter_grid_search.py ì‚¬ìš©)
"""

import torch
import torch.nn as nn
from pathlib import Path
import json
import argparse
from typing import Dict, Any, List
import time
import numpy as np
from dataclasses import asdict

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from src.models.decision_transformer import DecisionTransformer
from src.training.trainer import TrainingConfig, QuantumCircuitCollator
from src.data.embedding_pipeline import EmbeddingPipeline, EmbeddingConfig
from src.data.quantum_circuit_dataset import DatasetManager, create_dataloaders
from utils.debug_utils import debug_print


class AttentionModeComparator:
    """ì–´í…ì…˜ ëª¨ë“œë³„ ì„±ëŠ¥ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, config: TrainingConfig, model_path: str = None):
        self.config = config
        self.device = torch.device(config.device)
        
        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì„¤ì •
        embedding_config = EmbeddingConfig(
            d_model=config.d_model,
            n_gate_types=config.n_gate_types
        )
        self.embedding_pipeline = EmbeddingPipeline(embedding_config)
        
        # ëª¨ë¸ ìƒì„± (ê¸°ë³¸ ëª¨ë“œë¡œ)
        self.model = DecisionTransformer(
            d_model=config.d_model,
            n_layers=config.n_layers,
            n_heads=config.n_heads,
            n_gate_types=config.n_gate_types,
            dropout=config.dropout,
            attention_mode="standard"  # ì‹œì‘ì€ í‘œì¤€ ëª¨ë“œ
        ).to(self.device)
        
        # ëª¨ë¸ ë¡œë”©
        if model_path and Path(model_path).exists():
            self.load_model(model_path)
            print(f"âœ… Model loaded from: {model_path}")
        else:
            print("âš ï¸ No model loaded - using random weights")
    
    def load_model(self, model_path: str):
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        checkpoint = torch.load(model_path, map_location=self.device)
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
    
    def compare_modes_on_batch(self, batch: Dict[str, torch.Tensor]) -> Dict[str, Dict[str, Any]]:
        """ë‹¨ì¼ ë°°ì¹˜ì—ì„œ ì–´í…ì…˜ ëª¨ë“œë³„ ë¹„êµ"""
        self.model.eval()
        results = {}
        
        modes = ["standard", "advanced", "hybrid"]
        
        with torch.no_grad():
            for mode in modes:
                print(f"ğŸ”„ Testing {mode} attention mode...")
                
                # ëª¨ë“œ ë³€ê²½
                self.model.set_attention_mode(mode)
                
                # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                
                # ëª¨ë¸ ì‹¤í–‰
                output = self.model(
                    input_sequence=batch['input_sequence'],
                    attention_mask=batch['attention_mask'],
                    action_prediction_mask=batch['action_prediction_mask']
                )
                
                inference_time = time.time() - start_time
                
                # ê²°ê³¼ ì €ì¥
                results[mode] = {
                    'action_logits': output['action_logits'].cpu(),
                    'action_predictions': output['action_predictions'].cpu(),
                    'hidden_states': output['hidden_states'].cpu(),
                    'inference_time': inference_time,
                    'memory_usage': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
                }
                
                print(f"   â±ï¸ Inference time: {inference_time:.4f}s")
                if torch.cuda.is_available():
                    print(f"   ğŸ’¾ Memory usage: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
        
        return results
    
    def analyze_differences(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ì–´í…ì…˜ ëª¨ë“œë³„ ì°¨ì´ì  ë¶„ì„"""
        analysis = {}
        
        # ì„±ëŠ¥ ë¹„êµ
        performance = {}
        for mode in results:
            performance[mode] = {
                'inference_time': results[mode]['inference_time'],
                'memory_usage': results[mode]['memory_usage']
            }
        analysis['performance'] = performance
        
        # ì¶œë ¥ ì°¨ì´ ë¶„ì„
        if len(results) >= 2:
            modes = list(results.keys())
            base_mode = modes[0]
            
            differences = {}
            for mode in modes[1:]:
                # ì•¡ì…˜ ë¡œì§“ ì°¨ì´
                logits_diff = torch.abs(
                    results[mode]['action_logits'] - results[base_mode]['action_logits']
                ).mean().item()
                
                # íˆë“  ìŠ¤í…Œì´íŠ¸ ì°¨ì´
                hidden_diff = torch.abs(
                    results[mode]['hidden_states'] - results[base_mode]['hidden_states']
                ).mean().item()
                
                differences[f"{base_mode}_vs_{mode}"] = {
                    'logits_difference': logits_diff,
                    'hidden_difference': hidden_diff
                }
            
            analysis['output_differences'] = differences
        
        return analysis
    
    def run_comparison_experiment(self, data_path: str, num_batches: int = 5) -> Dict[str, Any]:
        """ì „ì²´ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"ğŸš€ Starting attention mode comparison experiment...")
        print(f"   ğŸ“ Data path: {data_path}")
        print(f"   ğŸ”¢ Number of batches: {num_batches}")
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        dataset_manager = DatasetManager(data_path)
        train_loader, val_loader = create_dataloaders(
            dataset_manager=dataset_manager,
            embedding_pipeline=self.embedding_pipeline,
            batch_size=self.config.batch_size,
            train_split=0.8
        )
        
        all_results = []
        all_analyses = []
        
        # ì—¬ëŸ¬ ë°°ì¹˜ì—ì„œ ì‹¤í—˜
        for i, batch in enumerate(train_loader):
            if i >= num_batches:
                break
                
            print(f"\nğŸ“Š Batch {i+1}/{num_batches}")
            
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # ëª¨ë“œë³„ ë¹„êµ
            batch_results = self.compare_modes_on_batch(batch)
            analysis = self.analyze_differences(batch_results)
            
            all_results.append(batch_results)
            all_analyses.append(analysis)
        
        # ì „ì²´ ê²°ê³¼ ì§‘ê³„
        summary = self.summarize_results(all_analyses)
        
        return {
            'config': asdict(self.config),
            'batch_results': all_results,
            'batch_analyses': all_analyses,
            'summary': summary
        }
    
    def summarize_results(self, analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²°ê³¼ ìš”ì•½"""
        summary = {}
        
        # ì„±ëŠ¥ í‰ê· 
        if analyses:
            modes = list(analyses[0]['performance'].keys())
            avg_performance = {}
            
            for mode in modes:
                times = [a['performance'][mode]['inference_time'] for a in analyses]
                memories = [a['performance'][mode]['memory_usage'] for a in analyses]
                
                avg_performance[mode] = {
                    'avg_inference_time': np.mean(times),
                    'std_inference_time': np.std(times),
                    'avg_memory_usage': np.mean(memories),
                    'std_memory_usage': np.std(memories)
                }
            
            summary['average_performance'] = avg_performance
            
            # ì¶œë ¥ ì°¨ì´ í‰ê· 
            if 'output_differences' in analyses[0]:
                diff_keys = list(analyses[0]['output_differences'].keys())
                avg_differences = {}
                
                for key in diff_keys:
                    logits_diffs = [a['output_differences'][key]['logits_difference'] for a in analyses]
                    hidden_diffs = [a['output_differences'][key]['hidden_difference'] for a in analyses]
                    
                    avg_differences[key] = {
                        'avg_logits_difference': np.mean(logits_diffs),
                        'std_logits_difference': np.std(logits_diffs),
                        'avg_hidden_difference': np.mean(hidden_diffs),
                        'std_hidden_difference': np.std(hidden_diffs)
                    }
                
                summary['average_differences'] = avg_differences
        
        return summary
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        # í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
        def tensor_to_list(obj):
            if isinstance(obj, torch.Tensor):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: tensor_to_list(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [tensor_to_list(item) for item in obj]
            else:
                return obj
        
        serializable_results = tensor_to_list(results)
        
        with open(output_path, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"ğŸ’¾ Results saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Compare attention modes in Decision Transformer")
    parser.add_argument("--data_path", type=str, required=True, help="Path to training data")
    parser.add_argument("--model_path", type=str, help="Path to trained model checkpoint")
    parser.add_argument("--output_path", type=str, default="attention_comparison_results.json", 
                       help="Output path for results")
    parser.add_argument("--num_batches", type=int, default=5, help="Number of batches to test")
    parser.add_argument("--batch_size", type=int, default=8, help="Batch size")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    
    args = parser.parse_args()
    
    # ì„¤ì •
    config = TrainingConfig(
        batch_size=args.batch_size,
        device=args.device,
        attention_mode="standard"  # ì‹œì‘ ëª¨ë“œ
    )
    
    # ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
    comparator = AttentionModeComparator(config, args.model_path)
    results = comparator.run_comparison_experiment(args.data_path, args.num_batches)
    
    # ê²°ê³¼ ì €ì¥
    comparator.save_results(results, args.output_path)
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“ˆ EXPERIMENT SUMMARY")
    print("="*50)
    
    if 'summary' in results and 'average_performance' in results['summary']:
        perf = results['summary']['average_performance']
        for mode, stats in perf.items():
            print(f"\nğŸ”§ {mode.upper()} Attention:")
            print(f"   â±ï¸ Avg inference time: {stats['avg_inference_time']:.4f}Â±{stats['std_inference_time']:.4f}s")
            if stats['avg_memory_usage'] > 0:
                print(f"   ğŸ’¾ Avg memory usage: {stats['avg_memory_usage']/1024**2:.1f}Â±{stats['std_memory_usage']/1024**2:.1f}MB")
    
    if 'summary' in results and 'average_differences' in results['summary']:
        diffs = results['summary']['average_differences']
        print(f"\nğŸ” OUTPUT DIFFERENCES:")
        for comparison, stats in diffs.items():
            print(f"   {comparison}:")
            print(f"     Logits diff: {stats['avg_logits_difference']:.6f}Â±{stats['std_logits_difference']:.6f}")
            print(f"     Hidden diff: {stats['avg_hidden_difference']:.6f}Â±{stats['std_hidden_difference']:.6f}")


if __name__ == "__main__":
    main()
