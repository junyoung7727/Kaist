"""
Decision Transformer Training Script
ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ - ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ì‚¬ìš©ë²•
"""

import argparse
import torch
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ (src í´ë”ë¥¼ ìµœìƒìœ„ íŒ¨í‚¤ì§€ë¡œ ì¸ì‹)
src_path = Path(__file__).parent
sys.path.append(str(src_path))

# ìƒìœ„ ë””ë ‰í† ë¦¬ë„ ì¶”ê°€ (OAT_Model í´ë”)
parent_path = src_path.parent
sys.path.append(str(parent_path))

# ì„í¬íŠ¸ ê²½ë¡œ ë¬¸ì œ í•´ê²°
try:
    # ì ˆëŒ€ ê²½ë¡œ ì‹œë„
    from training.trainer import DecisionTransformerTrainer, TrainingConfig, set_seed, QuantumCircuitCollator
    from data.quantum_circuit_dataset import DatasetManager, create_dataloaders
    from data.embedding_pipeline import create_embedding_pipeline, EmbeddingConfig
    from models.decision_transformer import create_decision_transformer
except ImportError:
    # ìƒëŒ€ ê²½ë¡œ ì‹œë„
    from src.training.trainer import DecisionTransformerTrainer, TrainingConfig, set_seed, QuantumCircuitCollator
    from src.data.quantum_circuit_dataset import DatasetManager, create_dataloaders
    from src.data.embedding_pipeline import create_embedding_pipeline, EmbeddingConfig
    from src.models.decision_transformer import create_decision_transformer


def main():
    parser = argparse.ArgumentParser(description="Train Decision Transformer for Quantum Circuits")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--data_path", type=str, 
                       default="C:\\Users\\jungh\\Documents\\GitHub\\Kaist\\results\\dummy_quantum_dataset.json",
                       help="Path to quantum circuit data JSON file")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--d_model", type=int, default=512, help="Model dimension")
    parser.add_argument("--n_layers", type=int, default=6, help="Number of transformer layers")
    parser.add_argument("--n_heads", type=int, default=8, help="Number of attention heads")
    parser.add_argument("--n_gate_types", type=int, default=20, help="Number of gate types")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout rate")
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="Learning rate")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of epochs")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="Weight decay")
    
    # ê¸°íƒ€ ì„¤ì •
    parser.add_argument("--device", type=str, default="auto", help="Device (cuda/cpu/auto)")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--save_dir", type=str, default="./checkpoints", help="Checkpoint save directory")
    
    # ë¡œê¹… ì„¤ì •
    parser.add_argument("--use_wandb", action="store_true", help="Use Weights & Biases logging")
    parser.add_argument("--project_name", type=str, default="quantum-decision-transformer", help="W&B project name")
    parser.add_argument("--run_name", type=str, default=None, help="W&B run name")
    
    # ë°ì´í„°ì…‹ ë¶„í•  ì„¤ì •
    parser.add_argument("--train_ratio", type=float, default=0.7, help="Training data ratio")
    parser.add_argument("--val_ratio", type=float, default=0.15, help="Validation data ratio")
    parser.add_argument("--test_ratio", type=float, default=0.15, help="Test data ratio")
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    print("=" * 60)
    print("ğŸš€ Quantum Decision Transformer Training")
    print("=" * 60)
    print(f"Device: {device}")
    print(f"Data path: {args.data_path}")
    print(f"Model: d_model={args.d_model}, layers={args.n_layers}, heads={args.n_heads}")
    print(f"Training: batch_size={args.batch_size}, epochs={args.num_epochs}, lr={args.learning_rate}")
    print("=" * 60)
    
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)
    
    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    print("ğŸ“Š Loading and preparing dataset...")
    manager = DatasetManager(args.data_path)
    
    try:
        # ë°ì´í„° ë¡œë”© ë° ì •ë³´ ì¶œë ¥
        circuit_specs = manager.parse_circuits()
        info = manager.get_dataset_info()
        
        print(f"âœ… Dataset loaded successfully!")
        print(f"   Total circuits: {info['total_circuits']}")
        print(f"   Qubits range: {info['num_qubits_range']}")
        print(f"   Gates range: {info['gate_count_range']}")
        print(f"   Gate types: {info['unique_gate_types']}")
        
        # ë°ì´í„°ì…‹ ë¶„í• 
        train_ds, val_ds, test_ds = manager.split_dataset(
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            test_ratio=args.test_ratio
        )
        
        print(f"ğŸ“Š Dataset split:")
        print(f"   Train: {len(train_ds)} circuits")
        print(f"   Validation: {len(val_ds)} circuits")
        print(f"   Test: {len(test_ds)} circuits")
        
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        return
    
    # 2. ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì„¤ì •
    print("\nğŸ”§ Setting up embedding pipeline...")
    embed_config = EmbeddingConfig(
        d_model=args.d_model,
        n_gate_types=args.n_gate_types,
        max_seq_len=1000
    )
    

    embedding_pipeline = create_embedding_pipeline(embed_config)
    print("âœ… Embedding pipeline created successfully!")


    # 3. ë°ì´í„°ë¡œë” ìƒì„±
    print("\nğŸ“¦ Creating data loaders...")
    try:
        train_loader, val_loader, test_loader = create_dataloaders(
            train_ds, val_ds, test_ds,
            batch_size=args.batch_size,
            num_workers=0
        )
        
        # ì½œë ˆì´í„° ì„¤ì •
        collator = QuantumCircuitCollator(embedding_pipeline)
        train_loader.collate_fn = collator
        val_loader.collate_fn = collator
        
        print("âœ… Data loaders created successfully!")
        
    except Exception as e:
        print(f"âŒ Error creating data loaders: {e}")
        return
    
    # 4. ëª¨ë¸ ìƒì„±
    print("\nğŸ¤– Creating Decision Transformer model...")
    try:
        model = create_decision_transformer(
            d_model=args.d_model,
            n_layers=args.n_layers,
            n_heads=args.n_heads,
            n_gate_types=args.n_gate_types,
            dropout=args.dropout
        )
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print("âœ… Model created successfully!")
        print(f"   Total parameters: {total_params:,}")
        print(f"   Trainable parameters: {trainable_params:,}")
        
    except Exception as e:
        print(f"âŒ Error creating model: {e}")
        return
    
    # 5. í•™ìŠµ ì„¤ì •
    print("\nâš™ï¸ Setting up training configuration...")
    config = TrainingConfig(
        d_model=args.d_model,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        n_gate_types=args.n_gate_types,
        dropout=args.dropout,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        batch_size=args.batch_size,
        num_epochs=args.num_epochs,
        device=device,
        seed=args.seed,
        use_wandb=args.use_wandb,
        project_name=args.project_name,
        run_name=args.run_name
    )
    
    # 6. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ Starting training...")
    try:
        trainer = DecisionTransformerTrainer(
            config=config,
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            save_dir=args.save_dir
        )
        
        # í•™ìŠµ ì‹œì‘
        trainer.train()
        
        print("\nğŸ‰ Training completed successfully!")
        print(f"ğŸ’¾ Checkpoints saved in: {args.save_dir}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Training interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error during training: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
