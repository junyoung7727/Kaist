"""
Modular Attention System for Decision Transformer

í‘œì¤€ ì–´í…ì…˜ê³¼ ê³ ê¸‰ ë‹¤ì¸µ ì–´í…ì…˜ì„ ì‰½ê²Œ ì „í™˜í•  ìˆ˜ ìˆëŠ” ëª¨ë“ˆëŸ¬ ì‹œìŠ¤í…œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, List, Literal
from enum import Enum

# ê³µí†µ ë””ë²„ê·¸ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
from utils.debug_utils import debug_print, debug_tensor_info

# ê¸°ì¡´ ì–´í…ì…˜ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from models.attention import (
    GridPositionalAttention, 
    RegisterFlowAttention, 
    EntanglementAttention, 
    SemanticAttention,
    AttentionFusionNetwork
)


class AttentionMode(Enum):
    """ì–´í…ì…˜ ëª¨ë“œ ì •ì˜"""
    STANDARD = "standard"           # í‘œì¤€ ë©€í‹°í—¤ë“œ ì–´í…ì…˜
    ADVANCED = "advanced"           # ê³ ê¸‰ ë‹¤ì¸µ ì–´í…ì…˜ (attention.py)
    HYBRID = "hybrid"               # í•˜ì´ë¸Œë¦¬ë“œ (ë‘˜ ë‹¤ ì‚¬ìš©)


class StandardMultiHeadAttention(nn.Module):
    """í‘œì¤€ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (ìµœì í™”ëœ ì•ˆì •ì„±)"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ìš© ê²½ëŸ‰ dropout
        self.attn_dropout = nn.Dropout(dropout * 0.5)
        self.output_dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, d_model = x.shape
        debug_print(f"      Standard Attention input - shape: {x.shape}")
        
        # Q, K, V ê³„ì‚°
        Q = self.w_q(x)  # [batch, seq_len, d_model]
        K = self.w_k(x)  # [batch, seq_len, d_model]
        V = self.w_v(x)  # [batch, seq_len, d_model]
        
        # ë©€í‹°í—¤ë“œë¡œ ë³€í˜•
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # ì–´í…ì…˜ ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # ë§ˆìŠ¤í¬ ì ìš© (ì•ˆì •í™”ëœ ë§ˆìŠ¤í‚¹)
        if mask is not None:
            mask = mask.unsqueeze(1)  # [batch, 1, seq_len, seq_len]
            scores = scores.masked_fill(~mask, -1e9)
        
        # ì•ˆì •í™”ëœ ì†Œí”„íŠ¸ë§¥ìŠ¤
        scores_max = scores.max(dim=-1, keepdim=True)[0]
        scores_stable = scores - scores_max
        attention_weights = F.softmax(scores_stable, dim=-1)
        attention_weights = self.attn_dropout(attention_weights)
        
        out = torch.matmul(attention_weights, V)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜ ë° dropout
        out = self.w_o(out)
        out = self.output_dropout(out)
        
        return out


class AdvancedMultiLayerAttention(nn.Module):
    """ê³ ê¸‰ ë‹¤ì¸µ ì–´í…ì…˜ (attention.py ê¸°ë°˜)"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        
        # 4ê°€ì§€ ì „ë¬¸í™”ëœ ì–´í…ì…˜
        self.grid_attention = GridPositionalAttention(d_model, n_heads)
        self.register_attention = RegisterFlowAttention(d_model, n_heads)
        self.entangle_attention = EntanglementAttention(d_model, n_heads)
        self.semantic_attention = SemanticAttention(d_model, n_heads)
        
        # ì–´í…ì…˜ ìœµí•© ë„¤íŠ¸ì›Œí¬
        self.attention_fusion = AttentionFusionNetwork(d_model, n_heads)
        
        # ì¶”ê°€ ì •ê·œí™”
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None, 
                grid_structure: Optional[Dict] = None, edges: Optional[List[Dict]] = None) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, d_model]
            mask: attention mask (optional)
            grid_structure: ê·¸ë¦¬ë“œ êµ¬ì¡° ì •ë³´ (optional)
            edges: ì—£ì§€ ì—°ê²° ì •ë³´ (optional)
        """
        batch_size, seq_len, d_model = x.shape
        debug_print(f"      Advanced Attention input - shape: {x.shape}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‹œí€€ìŠ¤ ì°¨ì›ìœ¼ë¡œ ë³€í™˜ [seq_len, batch, d_model]
        x_seq = x.transpose(0, 1)
        
        # ê¸°ë³¸ êµ¬ì¡° ìƒì„± (ì—†ëŠ” ê²½ìš°)
        if grid_structure is None:
            grid_structure = self._create_default_grid_structure(seq_len, batch_size)
        if edges is None:
            edges = self._create_default_edges(seq_len)
        
        # ê±°ë¦¬ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        distance_matrix = self._create_distance_matrix(seq_len)
        
        attention_outputs = {}
        
        # 1. Grid positional attention
        try:
            grid_out = self.grid_attention(x_seq, distance_matrix)
            attention_outputs['grid_attention'] = grid_out
        except Exception as e:
            debug_print(f"Grid attention error: {e}")
            attention_outputs['grid_attention'] = {'output': x_seq, 'attention_weights': None}
        
        # 2. Register flow attention
        try:
            register_out = self.register_attention(x_seq, grid_structure, edges)
            attention_outputs['register_attention'] = register_out
        except Exception as e:
            debug_print(f"Register attention error: {e}")
            attention_outputs['register_attention'] = {'output': x_seq, 'attention_weights': None}
        
        # 3. Entanglement attention
        try:
            entangle_out = self.entangle_attention(x_seq, grid_structure, edges)
            attention_outputs['entangle_attention'] = entangle_out
        except Exception as e:
            debug_print(f"Entanglement attention error: {e}")
            attention_outputs['entangle_attention'] = {'output': x_seq, 'attention_weights': None}
        
        # 4. Semantic attention
        try:
            semantic_out = self.semantic_attention(x_seq)
            attention_outputs['semantic_attention'] = semantic_out
        except Exception as e:
            debug_print(f"Semantic attention error: {e}")
            attention_outputs['semantic_attention'] = {'output': x_seq, 'attention_weights': None}
        
        # ì–´í…ì…˜ ìœµí•©
        try:
            fused_output = self.attention_fusion(attention_outputs)  # [seq_len, d_model]
            
            # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜ [batch, seq_len, d_model]
            if fused_output.dim() == 2:
                # [seq_len, d_model] -> [batch, seq_len, d_model]
                fused_output = fused_output.unsqueeze(0).expand(batch_size, -1, -1)
            else:
                # [seq_len, batch, d_model] -> [batch, seq_len, d_model]
                fused_output = fused_output.transpose(0, 1)
                
        except Exception as e:
            debug_print(f"Attention fusion error: {e}")
            # í´ë°±: ì›ë³¸ ì…ë ¥ ë°˜í™˜
            fused_output = x
        
        # ë“œë¡­ì•„ì›ƒ ì ìš©
        fused_output = self.dropout(fused_output)
        
        return fused_output
    
    def _create_default_grid_structure(self, seq_len: int, batch_size: int) -> Dict:
        """ê¸°ë³¸ ê·¸ë¦¬ë“œ êµ¬ì¡° ìƒì„±"""
        positions = torch.zeros(seq_len, 2)  # [seq_len, 2] (x, y ì¢Œí‘œ)
        for i in range(seq_len):
            positions[i] = torch.tensor([i % 8, i // 8])  # 8x8 ê·¸ë¦¬ë“œ ê°€ì •
        
        return {
            'positions': positions,
            'num_nodes': seq_len
        }
    
    def _create_default_edges(self, seq_len: int) -> List[Dict]:
        """ê¸°ë³¸ ì—£ì§€ ì—°ê²° ìƒì„±"""
        edges = []
        for i in range(seq_len - 1):
            edges.append({
                'type': 'REGISTER_CONNECTION',
                'source': [i % 8, i // 8],
                'target': [(i+1) % 8, (i+1) // 8]
            })
        return edges
    
    def _create_distance_matrix(self, seq_len: int) -> torch.Tensor:
        """ê±°ë¦¬ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        distance_matrix = torch.zeros(seq_len, seq_len)
        for i in range(seq_len):
            for j in range(seq_len):
                distance_matrix[i, j] = abs(i - j)
        return distance_matrix


class ModularAttention(nn.Module):
    """ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì‹œìŠ¤í…œ - ëª¨ë“œ ì „í™˜ ê°€ëŠ¥"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1, 
                 mode: AttentionMode = AttentionMode.STANDARD):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.mode = mode
        
        # ë‘ ê°€ì§€ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ëª¨ë‘ ì´ˆê¸°í™”
        self.standard_attention = StandardMultiHeadAttention(d_model, n_heads, dropout)
        self.advanced_attention = AdvancedMultiLayerAttention(d_model, n_heads, dropout)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œìš© ê°€ì¤‘ì¹˜
        if mode == AttentionMode.HYBRID:
            self.hybrid_weight = nn.Parameter(torch.tensor(0.5))  # 0~1 ì‚¬ì´ ê°€ì¤‘ì¹˜
            self.fusion_proj = nn.Linear(d_model * 2, d_model)
    
    def set_mode(self, mode: AttentionMode):
        """ì–´í…ì…˜ ëª¨ë“œ ë³€ê²½"""
        self.mode = mode
        debug_print(f"Attention mode changed to: {mode.value}")
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None,
                grid_structure: Optional[Dict] = None, edges: Optional[List[Dict]] = None) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, d_model]
            mask: attention mask
            grid_structure: ê·¸ë¦¬ë“œ êµ¬ì¡° (advanced/hybrid ëª¨ë“œìš©)
            edges: ì—£ì§€ ì •ë³´ (advanced/hybrid ëª¨ë“œìš©)
        """
        debug_print(f"ModularAttention forward - mode: {self.mode.value}")
        
        if self.mode == AttentionMode.STANDARD:
            return self.standard_attention(x, mask)
            
        elif self.mode == AttentionMode.ADVANCED:
            return self.advanced_attention(x, mask, grid_structure, edges)
            
        elif self.mode == AttentionMode.HYBRID:
            # ë‘ ì–´í…ì…˜ ê²°ê³¼ë¥¼ ê°€ì¤‘ ê²°í•©
            standard_out = self.standard_attention(x, mask)
            advanced_out = self.advanced_attention(x, mask, grid_structure, edges)
            
            # ê°€ì¤‘ ê²°í•©
            weight = torch.sigmoid(self.hybrid_weight)
            combined = torch.cat([
                standard_out * (1 - weight),
                advanced_out * weight
            ], dim=-1)
            
            # ì°¨ì› ì¶•ì†Œ
            output = self.fusion_proj(combined)
            return output
        
        else:
            raise ValueError(f"Unknown attention mode: {self.mode}")


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_modular_attention(d_model: int, n_heads: int, dropout: float = 0.1, 
                           mode: str = "standard") -> ModularAttention:
    """ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    attention_mode = AttentionMode(mode.lower())
    return ModularAttention(d_model, n_heads, dropout, attention_mode)


def compare_attention_modes(x: torch.Tensor, attention: ModularAttention, 
                          mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
    """ì–´í…ì…˜ ëª¨ë“œë³„ ê²°ê³¼ ë¹„êµ"""
    results = {}
    
    # ê° ëª¨ë“œë³„ë¡œ ì‹¤í–‰
    for mode in AttentionMode:
        attention.set_mode(mode)
        with torch.no_grad():
            output = attention(x, mask)
            results[mode.value] = output
    
    return results


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Modular Attention System Test")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size, seq_len, d_model = 2, 16, 512
    n_heads = 8
    
    x = torch.randn(batch_size, seq_len, d_model)
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).expand(batch_size, -1, -1)
    
    # ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ìƒì„±
    attention = create_modular_attention(d_model, n_heads, mode="standard")
    
    print(f"Input shape: {x.shape}")
    
    # ê° ëª¨ë“œ í…ŒìŠ¤íŠ¸
    for mode in ["standard", "advanced", "hybrid"]:
        attention.set_mode(AttentionMode(mode))
        output = attention(x, mask)
        print(f"{mode.capitalize()} attention output shape: {output.shape}")
    
    print("âœ… All tests passed!")
