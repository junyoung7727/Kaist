"""
Decision Transformer Model
ê°„ë‹¨í•˜ê³  í™•ì¥ì„± ë†’ì€ Decision Transformer êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import math
import numpy as np

# Focal Loss for addressing class imbalance in classification tasks
class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance in classification tasks"""
    
    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, ignore_index: int = -100):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ignore_index = ignore_index
        
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: [N, C] logits
            targets: [N] class indices
        """
        # ê¸°ë³¸ CrossEntropy ê³„ì‚°
        ce_loss = F.cross_entropy(inputs, targets, ignore_index=self.ignore_index, reduction='none')
        
        # p_t ê³„ì‚° (ì •ë‹µ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ )
        pt = torch.exp(-ce_loss)
        
        # Focal Loss ê³„ì‚°: Î± * (1-p_t)^Î³ * CE_loss
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        return focal_loss.mean()

from dataclasses import dataclass
from pathlib import Path

# ê³µí†µ ë””ë²„ê·¸ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
from utils.debug_utils import debug_print, debug_tensor_info
# ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from models.modular_attention import ModularAttention, AttentionMode, create_modular_attention
# Property Prediction ëª¨ë¸ ì„í¬íŠ¸
from models.property_prediction_transformer import PropertyPredictionTransformer

# ğŸ† NEW: ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì„í¬íŠ¸
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "quantumcommon"))
from gates import QuantumGateRegistry

# ğŸ—‘ï¸ REMOVED: Legacy MultiHeadAttention class - now using ModularAttention system


class TransformerBlock(nn.Module):
    """íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ (ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì§€ì›)"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, 
                 attention_mode: str = "standard"):
        super().__init__()
        
        # ğŸ† NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì‚¬ìš©
        self.attention = create_modular_attention(d_model, n_heads, dropout, attention_mode)
        self.attention_mode = attention_mode
        
        # ë¬¸ì œ 6 í•´ê²°: í”¼ë“œí¬ì›Œë“œ ì •ê·œí™” ìµœì í™” (ê³¼ë„í•œ ì •ê·œí™” ì œê±°)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),  # ì¤‘ê°„ dropoutë§Œ ìœ ì§€
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # Pre-norm êµ¬ì¡° (ì•ˆì •ì  epsilon)
        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        # ë¬¸ì œ 4 í•´ê²°: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° ë³µì›
        self.scale = nn.Parameter(torch.ones(1) * 0.5)  # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor, 
                grid_structure: Optional[Dict] = None, edges: Optional[List[Dict]] = None) -> torch.Tensor:
        debug_print(f"  TransformerBlock input - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        debug_print(f"  Using attention mode: {self.attention_mode}")
        
        # ğŸ† NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ (ê³ ê¸‰ ëª¨ë“œìš© ì¶”ê°€ ì¸ì ì§€ì›)
        norm_x = self.norm1(x)
        debug_print(f"    After norm1 - NaN: {torch.isnan(norm_x).any()}, min/max: {norm_x.min().item():.4f}/{norm_x.max().item():.4f}")
        
        attn_out = self.attention(norm_x, mask, grid_structure, edges)
        debug_print(f"    After {self.attention_mode} attention - NaN: {torch.isnan(attn_out).any()}, min/max: {attn_out.min().item():.4f}/{attn_out.max().item():.4f}")
        
        dropout_attn = self.dropout1(attn_out)
        debug_print(f"    After dropout1 - NaN: {torch.isnan(dropout_attn).any()}, min/max: {dropout_attn.min().item():.4f}/{dropout_attn.max().item():.4f}")
        
        scaled_attn = self.scale * dropout_attn
        debug_print(f"    After scale*dropout1 - NaN: {torch.isnan(scaled_attn).any()}, scale: {self.scale.item():.4f}")
        
        x = x + scaled_attn
        debug_print(f"    After residual1 - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        
        # Pre-norm + í”¼ë“œí¬ì›Œë“œ + ìŠ¤ì¼€ì¼ë§ëœ ì”ì°¨ ì—°ê²°
        norm_x = self.norm2(x)
        debug_print(f"    After norm2 - NaN: {torch.isnan(norm_x).any()}, min/max: {norm_x.min().item():.4f}/{norm_x.max().item():.4f}")
        
        ff_out = self.feed_forward(norm_x)
        debug_print(f"    After feedforward - NaN: {torch.isnan(ff_out).any()}, min/max: {ff_out.min().item():.4f}/{ff_out.max().item():.4f}")
        
        dropout_ff = self.dropout2(ff_out)
        debug_print(f"    After dropout2 - NaN: {torch.isnan(dropout_ff).any()}, min/max: {dropout_ff.min().item():.4f}/{dropout_ff.max().item():.4f}")
        
        scaled_ff = self.scale * dropout_ff
        debug_print(f"    After scale*dropout2 - NaN: {torch.isnan(scaled_ff).any()}")
        
        x = x + scaled_ff
        debug_print(f"    TransformerBlock output - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        
        return x
    
    def set_attention_mode(self, mode: str):
        """ì–´í…ì…˜ ëª¨ë“œ ë³€ê²½"""
        self.attention.set_mode(AttentionMode(mode.lower()))
        self.attention_mode = mode
        debug_print(f"TransformerBlock attention mode changed to: {mode}")


class DecisionTransformer(nn.Module):
    """Decision Transformer ëª¨ë¸"""
    
    def __init__(
        self,
        d_model: int = 512,
        n_layers: int = 6,
        n_heads: int = 8,
        d_ff: int = 2048,
        n_gate_types: int = None,  # ğŸ† NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •
        max_qubits: int = 50,  # ğŸ† NEW: ìµœëŒ€ íë¹— ìˆ˜
        position_dim: int = None,  # ğŸ† NEW: ìœ„ì¹˜ ì˜ˆì¸¡ ì¶œë ¥ ì°¨ì› (ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±ìš©)
        dropout: float = 0.1,
        attention_mode: str = "advanced",  # ğŸ† NEW: ì–´í…ì…˜ ëª¨ë“œ ì„ íƒ
        device: str = "cpu",  # ğŸ† NEW: ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì„¤ì •
        property_prediction_model: Optional[PropertyPredictionTransformer] = None
    ):
        super().__init__()
        
        self.d_model = d_model
        self.max_qubits = max_qubits
        self.device = device
        
        # Use the position_dim from the parameter if provided, otherwise use default
        if position_dim is not None:
            self.position_dim = position_dim
        else:
            self.position_dim = d_model // 4
        
        # ğŸ† NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ gate ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        if n_gate_types is None:
            self.n_gate_types = QuantumGateRegistry.get_singleton_gate_count()
            debug_print(f"ğŸ† DecisionTransformer: Using gate vocab singleton, n_gate_types = {self.n_gate_types}")
        else:
            self.n_gate_types = n_gate_types
            debug_print(f"âš ï¸ DecisionTransformer: Using manual n_gate_types = {self.n_gate_types}")
        
        self.attention_mode = attention_mode  # ğŸ† NEW: ì–´í…ì…˜ ëª¨ë“œ ì €ì¥
        
        # Gate registry for qubit/parameter requirements
        self.gate_registry = QuantumGateRegistry()
        
        # ğŸ† NEW: ê¸°ì¡´ attention.pyì˜ ê³ ê¸‰ ì–´í…ì…˜ ì‹œìŠ¤í…œ í™œìš© (ìš”êµ¬ì‚¬í•­ 3)
        # ì¸ì½”ë”: ì–‘ì íšŒë¡œ ì œì•½ ì •ë³´ë¥¼ ê³ ê¸‰ ì–´í…ì…˜ìœ¼ë¡œ ì²˜ë¦¬
        from models.attention import (
            GridPositionalAttention, RegisterFlowAttention, 
            EntanglementAttention, SemanticAttention, AttentionFusionNetwork
        )
        
        # ì¸ì½”ë” ë¸”ë¡ë“¤ (ì–‘ì íšŒë¡œ ì œì•½ ì¸ì‹ ì–´í…ì…˜ ì‚¬ìš©)
        self.constraint_encoder_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout, "advanced")  # ê³ ê¸‰ ì–´í…ì…˜ ëª¨ë“œ
            for _ in range(n_layers // 2)
        ])
        
        # ë””ì½”ë” ë¸”ë¡ë“¤ (ì‹œí€€ìŠ¤ ìƒì„±ìš©)
        self.sequence_decoder_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout, attention_mode)
            for _ in range(n_layers - n_layers // 2)
        ])
        
        # í˜¸í™˜ì„±ì„ ìœ„í•´ transformer_blocks ì¬ì •ì˜ (ì „ì²´ ë¸”ë¡ì˜ ì—°ì†)
        self.transformer_blocks = nn.ModuleList(list(self.constraint_encoder_blocks) + list(self.sequence_decoder_blocks))
        
        # ğŸ† NEW: ì–‘ì íšŒë¡œ ì œì•½ ì •ë³´ë¥¼ ìœ„í•œ íŠ¹í™”ëœ ì–´í…ì…˜ë“¤
        self.grid_attention = GridPositionalAttention(d_model, n_heads)
        self.register_attention = RegisterFlowAttention(d_model, n_heads)  
        self.entangle_attention = EntanglementAttention(d_model, n_heads)
        self.semantic_attention = SemanticAttention(d_model, n_heads)
        self.attention_fusion = AttentionFusionNetwork(d_model, n_heads)
        
        # Cross-attention for constraint-aware sequence generation
        self.constraint_cross_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        
        debug_print(f"ğŸ† DecisionTransformer: Action heads - gates:{self.n_gate_types}, position_dim:{self.position_dim}")
        
        self.action_heads = nn.ModuleDict({
            'gate': nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(),
                nn.Linear(d_model // 2, self.n_gate_types)  # ë¶„ë¥˜ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°ì§€ëœ ê²Œì´íŠ¸ ìˆ˜)
            ),
            'position': nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(), 
                nn.Linear(d_model // 2, self.position_dim)  # ë¶„ë¥˜: ì²´í¬í¬ì¸íŠ¸ì™€ í˜¸í™˜ë˜ëŠ” ìœ„ì¹˜ ì°¨ì›
            ),
            'parameter': nn.Sequential(
                nn.Linear(d_model, d_model // 4),
                nn.GELU(),
                nn.Linear(d_model // 4, 1)  # íšŒê·€: ë‹¨ì¼ ì—°ì†ê°’
            )
        })
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # ì„ë² ë”© ê³„ì¸µ (ìƒíƒœ, ì•¡ì…˜, ë¦¬ì›Œë“œë¥¼ í¬í•¨í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ì‹œí€€ìŠ¤ ìƒì„±)
        self.embedding = QuantumGateSequenceEmbedding(
            d_model=d_model,
            n_gate_types=n_gate_types,
            dropout=dropout,
            property_prediction_model=property_prediction_model
        )
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ë§¤ìš° ë³´ìˆ˜ì )
        self.apply(self._conservative_init_weights)
        
    def _conservative_init_weights(self, module):
        """ë¬¸ì œ 5 í•´ê²°: ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì•ˆì •ì„± + í•™ìŠµ ëŠ¥ë ¥)"""
        if isinstance(module, nn.Linear):
            # ë¬¸ì œ 5 í•´ê²°: gain 0.1 â†’ 0.5ë¡œ ì¦ê°€ (ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë°©ì§€)
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            # ì„ë² ë”© ì´ˆê¸°í™”ë„ ì•½ê°„ ì¦ê°€
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(
        self,
        input_sequence: torch.Tensor,
        attention_mask: torch.Tensor,
        action_prediction_mask: torch.Tensor,
        grid_structure: Optional[torch.Tensor] = None,
        edges: Optional[torch.Tensor] = None,
        circuit_constraints: Optional[torch.Tensor] = None,  # NEW: ì–‘ì íšŒë¡œ ì œì•½ ì •ë³´
        predictions: Optional[Dict] = None,
        targets: Optional[Dict] = None,
        num_qubits: Optional[List[int]] = None,
        num_gates: Optional[List[int]] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            input_sequence: [batch, seq_len, d_model]
            attention_mask: [batch, seq_len, seq_len] 
            action_prediction_mask: [batch, seq_len]
            circuit_constraints: [batch, constraint_len, d_model] - ì–‘ì íšŒë¡œ ì œì•½ ì •ë³´
        
        Returns:
            Dict with predictions and logits
        """
        debug_print(f"DecisionTransformer forward - input shape: {input_sequence.shape}")
        debug_print(f"  Input - NaN: {torch.isnan(input_sequence).any()}, min/max: {input_sequence.min().item():.4f}/{input_sequence.max().item():.4f}")
        
        # í…ì„œ ì°¨ì› ì •ê·œí™” - 4D í…ì„œë¥¼ 3Dë¡œ ë³€í™˜
        if len(input_sequence.shape) == 4:
            # [batch, 1, seq_len, d_model] -> [batch, seq_len, d_model]
            input_sequence = input_sequence.squeeze(1)
            debug_print(f"  Squeezed input shape: {input_sequence.shape}")
        elif len(input_sequence.shape) != 3:
            raise ValueError(f"Expected 3D or 4D input tensor, got {len(input_sequence.shape)}D: {input_sequence.shape}")
        
        # ë§ˆìŠ¤í¬ ì°¨ì› ì •ê·œí™”
        debug_print(f"  Original action_prediction_mask shape: {action_prediction_mask.shape}")
        if len(action_prediction_mask.shape) == 3:
            # [batch, 1, seq_len] -> [batch, seq_len]
            action_prediction_mask = action_prediction_mask.squeeze(1)
            debug_print(f"  Squeezed action_prediction_mask shape: {action_prediction_mask.shape}")
        elif len(action_prediction_mask.shape) != 2:
            raise ValueError(f"Expected 2D or 3D action_prediction_mask, got {len(action_prediction_mask.shape)}D: {action_prediction_mask.shape}")
        
        batch_size, seq_len, _ = input_sequence.shape
        
        # NEW: ê³ ê¸‰ ì–´í…ì…˜ ê¸°ë°˜ ì œì•½ ì¸ì½”ë”© (ìš”êµ¬ì‚¬í•­ 3)
        # 1. ì–‘ì íšŒë¡œ ì œì•½ ì •ë³´ë¥¼ ê³ ê¸‰ ì–´í…ì…˜ìœ¼ë¡œ ì²˜ë¦¬
        constraint_features = None
        if grid_structure is not None and edges is not None:
            # ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ constraint encoderë¡œ ì²˜ë¦¬
            constraint_input = input_sequence.transpose(0, 1)  # [seq_len, batch, d_model] for attention modules
            
            # ê° íŠ¹í™”ëœ ì–´í…ì…˜ ì ìš©
            attention_outputs = {}
            
            # Grid positional attention
            grid_out = self.grid_attention(constraint_input, grid_structure, edges)
            attention_outputs['grid_attention'] = grid_out
            
            # Register flow attention  
            register_out = self.register_attention(constraint_input, grid_structure, edges)
            attention_outputs['register_attention'] = register_out
            
            # Entanglement attention
            entangle_out = self.entangle_attention(constraint_input, grid_structure, edges)
            attention_outputs['entanglement_attention'] = entangle_out
            
            # Semantic attention
            semantic_out = self.semantic_attention(constraint_input)
            attention_outputs['semantic_attention'] = semantic_out
            
            # ëª¨ë“  ì–´í…ì…˜ ê²°ê³¼ ìœµí•©
            constraint_features = self.attention_fusion(attention_outputs)  # [seq_len, d_model]
            constraint_features = constraint_features.transpose(0, 1)  # [batch, seq_len, d_model]
            
            debug_print(f"  Constraint features from advanced attention: {constraint_features.shape}")
        
        # 2. ì‹œí€€ìŠ¤ ë””ì½”ë” ì²˜ë¦¬
        x = self.dropout(input_sequence)
        debug_print(f"  After input dropout - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        
        # ì‹œí€€ìŠ¤ ë””ì½”ë” ë¸”ë¡ë“¤ ì²˜ë¦¬
        for i, decoder_block in enumerate(self.sequence_decoder_blocks):
            debug_print(f"  Processing sequence decoder block {i}")
            x = decoder_block(x, attention_mask, grid_structure, edges)
            
            # Cross-attention with constraint features
            if constraint_features is not None:
                x_norm = F.layer_norm(x, x.shape[-1:])
                cross_attn_out, _ = self.constraint_cross_attention(
                    x_norm, constraint_features, constraint_features,
                    need_weights=False
                )
                x = x + self.dropout(cross_attn_out)
            
            debug_print(f"    Sequence decoder block {i} output - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        
        # Get predictions from predict_actions
        predictions = self.predict_actions(x, action_prediction_mask, num_qubits)
        
        # Add hidden_states to the output for predict_next_action compatibility
        predictions['hidden_states'] = x
        
        return predictions
    
    def predict_actions(self, hidden_states: torch.Tensor, action_mask: torch.Tensor, num_qubits: Optional[List[int]] = None) -> Dict[str, torch.Tensor]:
        """ ì•¡ì…˜ ì˜ˆì¸¡ ì „ìš© ë©”ì„œë“œ - ì „ì²´ ì‹œí€€ìŠ¤ ê¸¸ì´ ìœ ì§€"""
        batch_size, seq_len, d_model = hidden_states.shape
        
        debug_print(f"ğŸš€ predict_actions - hidden_states: {hidden_states.shape}, action_mask: {action_mask.shape}")
        
        # ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ (ë§ˆìŠ¤í¬ë¡œ í•„í„°ë§)
        # ëª¨ë“  ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡í•˜ê³ , ì†ì‹¤ ê³„ì‚° ì‹œ ë§ˆìŠ¤í¬ë¡œ í•„í„°ë§
        
        # 3ê°€ì§€ ì•¡ì…˜ ì˜ˆì¸¡ - ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´
        gate_logits = self.action_heads['gate'](hidden_states)  # [batch, seq_len, n_gate_types]
        
        # Position head ì¶œë ¥
        position_raw = self.action_heads['position'](hidden_states)  # [batch, seq_len, position_dim]
        
        # ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•œ ìœ ì—°í•œ position reshape
        if hasattr(self, 'position_dim') and self.position_dim != self.max_qubits * 2:
            # ì²´í¬í¬ì¸íŠ¸ì™€ í˜¸í™˜ë˜ëŠ” ì°¨ì› ìœ ì§€
            position_reshaped = position_raw.view(batch_size, seq_len, -1, 2)
            debug_print(f"â„¹ï¸ Using checkpoint-compatible position shape: {position_reshaped.shape}")
        else:
            # ê¸°ë³¸ ì„¤ì • - max_qubits * 2 ì°¨ì›
            position_reshaped = position_raw.view(batch_size, seq_len, self.max_qubits, 2)
        
        parameter_preds = self.action_heads['parameter'](hidden_states).squeeze(-1)  # [batch, seq_len]
        
        predictions = {
            'gate_logits': gate_logits,        # [batch, seq_len, n_gate_types]
            'position_preds': position_reshaped,  # [batch, seq_len, max_qubits, 2]
            'parameter_preds': parameter_preds    # [batch, seq_len]
        }
        
        debug_print(f"ğŸš€ predictions shapes - gate: {gate_logits.shape}, position: {position_reshaped.shape}, param: {parameter_preds.shape}")
        
        # ğŸš€ ë°°ì¹˜ ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•œ ë™ì  ë§ˆìŠ¤í‚¹ ì ìš©
        if num_qubits is not None:
            predictions = self._apply_dynamic_qubit_masking(predictions, num_qubits, action_mask)
        
        debug_print(f"ğŸš€ Final predictions ready - gate: {predictions['gate_logits'].shape}")
        
        return predictions
    
    def _apply_dynamic_qubit_masking(
        self, 
        predictions: Dict[str, torch.Tensor], 
        num_qubits: List[int], 
        action_mask: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        ğŸš€ ë°°ì¹˜ë³„ íë¹— ìˆ˜ì— ë”°ë¥¸ ë™ì  ë§ˆìŠ¤í‚¹ ì ìš©
        
        Args:
            predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
            num_qubits: ê° íšŒë¡œì˜ íë¹— ìˆ˜ [batch_size]
            action_mask: ì•¡ì…˜ ë§ˆìŠ¤í¬ [batch, num_actions]
        
        Returns:
            ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ ì˜ˆì¸¡ ê²°ê³¼
        """
        batch_size = len(num_qubits)
        position_preds = predictions['position_preds']  # [batch, num_actions, max_qubits, 2]
        
        # ë°°ì¹˜ í¬ê¸° ê²€ì¦
        if position_preds.shape[0] != batch_size:
            raise ValueError(
                f"âŒ CRITICAL ERROR: ë°°ì¹˜ í¬ê¸° ë¶ˆì¼ì¹˜!\n"
                f"   position_preds batch size: {position_preds.shape[0]}\n"
                f"   num_qubits length: {batch_size}"
            )
        
        # ê° íšŒë¡œë³„ë¡œ ë™ì  ë§ˆìŠ¤í‚¹ ì ìš©
        for batch_idx, circuit_qubits in enumerate(num_qubits):
            # íë¹— ìˆ˜ ê²€ì¦
            if circuit_qubits <= 0 or circuit_qubits > self.max_qubits:
                raise ValueError(
                    f"âŒ CRITICAL ERROR: íšŒë¡œ {batch_idx}ì˜ íë¹— ìˆ˜ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
                    f"   circuit_qubits: {circuit_qubits}\n"
                    f"   max_qubits: {self.max_qubits}"
                )
            
            # ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ë¥¼ -infë¡œ ë§ˆìŠ¤í‚¹ (softmaxì—ì„œ í™•ë¥  0)
            # position_preds[batch_idx, :, circuit_qubits:, :] = float('-inf')
            if circuit_qubits < self.max_qubits:
                position_preds[batch_idx, :, circuit_qubits:, :] = float('-inf')
        
        predictions['position_preds'] = position_preds
        return predictions
    
    def compute_loss(
        self,
        predictions: Dict[str, torch.Tensor],
        targets: Dict,
        action_prediction_mask: torch.Tensor,
        num_qubits: Optional[List[int]] = None,
        num_gates: Optional[List[int]] = None
    ) -> Dict[str, torch.Tensor]:
        """
        ğŸ¯ ë³„ë„ì˜ ì†ì‹¤ ê³„ì‚° ë©”ì„œë“œ (ì˜ˆì¸¡ê³¼ ë¶„ë¦¬)
        
        Args:
            predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
            targets: íƒ€ê²Ÿ ë°ì´í„°
            action_prediction_mask: ì•¡ì…˜ ì˜ˆì¸¡ ë§ˆìŠ¤í¬
            num_qubits: ë°°ì¹˜ë³„ íë¹— ìˆ˜ ì •ë³´
        
        Returns:
            ì†ì‹¤ ê³„ì‚° ê²°ê³¼ dict
        """
        loss_computer = ActionLossComputer()
        
        # EOS ë§ˆìŠ¤í¬ ìƒì„± (í•„ìš”ì‹œ)
        eos_mask = None
        if hasattr(self, 'gate_registry'):
            gate_vocab = self.gate_registry.get_gate_vocab()
            eos_token_id = gate_vocab.get('[EOS]', -1)
            if eos_token_id != -1 and 'gate_targets' in targets:
                eos_mask = targets['gate_targets'] != eos_token_id
        
        # ë§ˆìŠ¤í¬ ê²°í•©
        combined_mask = action_prediction_mask
        if eos_mask is not None:
            combined_mask = action_prediction_mask & eos_mask
        
        # ì†ì‹¤ ê³„ì‚°
        return loss_computer.compute(
            predictions=predictions,
            targets=targets,
            mask=combined_mask,
            num_qubits=num_qubits,
            num_gates=num_gates
        )
    
    def set_attention_mode(self, mode: str):
        """ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ ì–´í…ì…˜ ëª¨ë“œ ë³€ê²½"""
        self.attention_mode = mode
        for block in self.transformer_blocks:
            block.set_attention_mode(mode)
        debug_print(f"DecisionTransformer attention mode changed to: {mode}")
    
    def get_attention_mode(self) -> str:
        """í˜„ì¬ ì–´í…ì…˜ ëª¨ë“œ ë°˜í™˜"""
        return self.attention_mode
    
    def compare_attention_modes(self, input_sequence: torch.Tensor, attention_mask: torch.Tensor, 
                              action_prediction_mask: torch.Tensor) -> Dict[str, Dict[str, torch.Tensor]]:
        """ì–´í…ì…˜ ëª¨ë“œë³„ ê²°ê³¼ ë¹„êµ"""
        original_mode = self.attention_mode
        results = {}
        
        for mode in ["standard", "advanced", "hybrid"]:
            self.set_attention_mode(mode)
            with torch.no_grad():
                output = self.forward(input_sequence, attention_mask, action_prediction_mask)
                results[mode] = {
                    'action_logits': output['action_logits'].clone(),
                    'action_predictions': output['action_predictions'].clone()
                }
        
        # ì›ë˜ ëª¨ë“œë¡œ ë³µêµ¬
        self.set_attention_mode(original_mode)
    def predict_next_action(
        self,
        input_sequence: torch.Tensor,
        attention_mask: torch.Tensor,
        circuit_constraints: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """ë‹¤ìŒ ì•¡ì…˜ ì˜ˆì¸¡ (ì¶”ë¡ ìš©) - ë©€í‹°íƒœìŠ¤í¬ ì˜ˆì¸¡ ì§€ì›
        ê¸°ëŒ€ í˜•íƒœ:
          - input_sequence: [1, seq_len, d_model]
          - attention_mask: [1, seq_len, seq_len]
        """
        with torch.no_grad():
            # ë§ˆì§€ë§‰ ìœ„ì¹˜ì—ì„œ ì•¡ì…˜ ì˜ˆì¸¡
            action_prediction_mask = torch.zeros(input_sequence.shape[:2], dtype=torch.bool, device=input_sequence.device)
            action_prediction_mask[0, -1] = True  # ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ ì˜ˆì¸¡
            
            outputs = self.forward(input_sequence, attention_mask, action_prediction_mask, circuit_constraints=circuit_constraints)
            
            # ë§ˆì§€ë§‰ ì•¡ì…˜ ìœ„ì¹˜ì˜ í™•ë¥  ë¶„í¬ ë°˜í™˜
            action_positions = torch.where(action_prediction_mask)
            last_action_pos = int(action_positions[-1].item())
            
            # ğŸ”¥ NEW: ìƒˆë¡œìš´ í‚¤ êµ¬ì¡°ì— ë§ëŠ” ì˜ˆì¸¡ ì¶”ì¶œ
            gate_logits = outputs['gate_logits'][:, last_action_pos, :]  # [1, n_gate_types]
            gate_probs = F.softmax(gate_logits, dim=-1)
            
            # íë¹— ìœ„ì¹˜ ì˜ˆì¸¡ (ìƒˆë¡œìš´ êµ¬ì¡°: ìœ„ì¹˜ ë²¡í„°)
            position_preds = outputs['position_preds'][:, last_action_pos, :]  # [1, 3]
            
            # íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ (ìƒˆë¡œìš´ êµ¬ì¡°: ë‹¨ì¼ ì—°ì†ê°’)
            param_value = outputs['parameter_preds'][:, last_action_pos]  # [1]
            
            return {
                'gate_probs': gate_probs,
                'position_preds': position_preds,
                'param_value': param_value,
                'gate_logits': gate_logits,
                'hidden_states': outputs['hidden_states']
            }
    
    def sample_gate_with_constraints(
        self, 
        gate_probs: torch.Tensor,
        temperature: float = 1.0
    ) -> Tuple[int, str, int, int]:
        """ê²Œì´íŠ¸ íƒ€ì…ì„ ìƒ˜í”Œë§í•˜ê³  í•´ë‹¹ ê²Œì´íŠ¸ì˜ íë¹—/íŒŒë¼ë¯¸í„° ìš”êµ¬ì‚¬í•­ ë°˜í™˜
        
        Returns:
            gate_idx: ìƒ˜í”Œë§ëœ ê²Œì´íŠ¸ ì¸ë±ìŠ¤
            gate_name: ê²Œì´íŠ¸ ì´ë¦„
            required_qubits: í•„ìš”í•œ íë¹— ìˆ˜
            required_params: í•„ìš”í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        """
        # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
        if temperature != 1.0:
            gate_probs = gate_probs / temperature
            gate_probs = F.softmax(gate_probs, dim=-1)
        
        # ê²Œì´íŠ¸ ìƒ˜í”Œë§
        gate_idx = torch.multinomial(gate_probs.squeeze(0), 1).item()
        
        # ê²Œì´íŠ¸ ì •ë³´ ì¡°íšŒ
        gate_vocab = self.gate_registry.get_gate_vocab()
        gate_names = list(gate_vocab.keys())
        
        if gate_idx < len(gate_names):
            gate_name = gate_names[gate_idx]
            gate_def = self.gate_registry.get_gate(gate_name)
            
            if gate_def:
                return gate_idx, gate_name, gate_def.num_qubits, gate_def.num_parameters
        
        # ê¸°ë³¸ê°’ (ì•Œ ìˆ˜ ì—†ëŠ” ê²Œì´íŠ¸)
        return gate_idx, "unknown", 1, 0
    
    def sample_qubits_for_gate(
        self,
        qubit_probs: List[torch.Tensor],
        gate_name: str,
        num_qubits_required: int,
        available_qubits: int,
        temperature: float = 1.0
    ) -> List[int]:
        """ê²Œì´íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” íë¹— ìœ„ì¹˜ë“¤ì„ ìƒ˜í”Œë§
        
        Args:
            qubit_probs: ê° íë¹— ìœ„ì¹˜ë³„ í™•ë¥  ë¶„í¬ ë¦¬ìŠ¤íŠ¸
            gate_name: ê²Œì´íŠ¸ ì´ë¦„
            num_qubits_required: í•„ìš”í•œ íë¹— ìˆ˜
            available_qubits: ì‚¬ìš© ê°€ëŠ¥í•œ ì´ íë¹— ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            
        Returns:
            ì„ íƒëœ íë¹— ì¸ë±ìŠ¤ë“¤ (ìš”êµ¬ì‚¬í•­ 1: [n], [n,n], [n,n,n] í˜•íƒœ)
        """
        selected_qubits = []
        used_qubits = set()
        
        for i in range(min(num_qubits_required, len(qubit_probs))):
            probs = qubit_probs[i].clone()
            
            # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
            if temperature != 1.0:
                probs = probs / temperature
                probs = F.softmax(probs, dim=-1)
            
            # ì´ë¯¸ ì‚¬ìš©ëœ íë¹—ê³¼ "no qubit" í† í° ë§ˆìŠ¤í‚¹
            for used_qubit in used_qubits:
                if used_qubit < probs.shape[-1] - 1:  # -1ì€ "no qubit" í† í°
                    probs[0, used_qubit] = 0.0
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ íë¹— ë²”ìœ„ ë°–ì€ ë§ˆìŠ¤í‚¹
            if available_qubits < probs.shape[-1] - 1:
                probs[0, available_qubits:-1] = 0.0
            
            # í™•ë¥  ì¬ì •ê·œí™”
            probs = probs / probs.sum()
            
            # íë¹— ìƒ˜í”Œë§
            qubit_idx = torch.multinomial(probs.squeeze(0), 1).item()
            
            # "no qubit" í† í°ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
            if qubit_idx < available_qubits:
                selected_qubits.append(qubit_idx)
                used_qubits.add(qubit_idx)
        
        return selected_qubits
    
    def sample_parameters_for_gate(
        self,
        param_values: List[torch.Tensor],
        gate_name: str,
        num_params_required: int
    ) -> List[float]:
        """ê²Œì´íŠ¸ì— í•„ìš”í•œ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¶”ì¶œ
        
        Args:
            param_values: ì˜ˆì¸¡ëœ íŒŒë¼ë¯¸í„° ê°’ë“¤
            gate_name: ê²Œì´íŠ¸ ì´ë¦„  
            num_params_required: í•„ìš”í•œ íŒŒë¼ë¯¸í„° ìˆ˜
            
        Returns:
            íŒŒë¼ë¯¸í„° ê°’ë“¤ (ìš”êµ¬ì‚¬í•­ 2)
        """
        parameters = []
        
        for i in range(min(num_params_required, len(param_values))):
            param_val = param_values[i].squeeze().item()
            
            # íŒŒë¼ë¯¸í„° ë²”ìœ„ ì œí•œ (íšŒì „ ê²Œì´íŠ¸ì˜ ê²½ìš° 0 ~ 2Ï€)
            if gate_name.startswith('r') or gate_name in ['p']:  # rx, ry, rz, p ê²Œì´íŠ¸
                param_val = param_val % (2 * math.pi)
            
            parameters.append(param_val)
        
        return parameters

    def generate_autoregressive(
        self,
        prompt_tokens: List[int] = None,
        max_length: int = 100,
        temperature: float = 1.0,
        top_k: int = 50,
        reward_calculator=None,
        target_properties: Dict[str, float] = None,
        num_qubits: int = 4
    ) -> List[Dict]:
        """
        Property-guided Autoregressive ìƒì„± (Decision Transformer ê¸°ë°˜)
        
        Args:
            prompt_tokens: ì´ˆê¸° í† í° ì‹œí€€ìŠ¤ (ì„ íƒì )
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_k: Top-k ìƒ˜í”Œë§
            reward_calculator: ë³´ìƒ ê³„ì‚°ê¸°
            target_properties: ëª©í‘œ ì†ì„±
            num_qubits: íë¹— ìˆ˜
            
        Returns:
            ìƒì„±ëœ ê²Œì´íŠ¸ ì‹œí€€ìŠ¤ [{'gate': str, 'qubits': List[int], 'params': List[float]}]
        """
        self.eval()
        
        # ì´ˆê¸° ì‹œí€€ìŠ¤ ì„¤ì •
        generated_gates = []
        current_context = {
            'states': [],
            'actions': [],
            'rewards': []
        }
        
        # ë³´ìƒ ê°€ì´ë˜ìŠ¤ ì„¤ì •
        use_reward_guidance = reward_calculator is not None and target_properties is not None
        if use_reward_guidance:
            reward_calculator.set_target_properties(target_properties)
            print(f"ğŸ¯ Using reward guidance with targets: {target_properties}")
        
        with torch.no_grad():
            for step in range(max_length):
                print(f"\n--- Generation Step {step} ---")
                
                # 1. í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
                if len(current_context['states']) == 0:
                    # ì´ˆê¸° ìƒíƒœ (ë¹ˆ íšŒë¡œ)
                    device = torch.device(self.device)
                    state_emb = torch.zeros(1, 1, self.d_model, device=device)
                    action_emb = torch.zeros(1, 1, self.d_model, device=device)
                    reward_emb = torch.zeros(1, 1, self.d_model, device=device)
                else:
                    # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ì¬ì„ë² ë”©
                    state_emb = torch.stack(current_context['states'], dim=1)  # [1, seq_len, d_model]
                    action_emb = torch.stack(current_context['actions'], dim=1)
                    reward_emb = torch.stack(current_context['rewards'], dim=1)
                
                # 2. SAR ì‹œí€€ìŠ¤ êµ¬ì„±
                seq_len = state_emb.shape[1]
                device = torch.device(self.device)
                sar_sequence = torch.zeros(1, seq_len * 3, self.d_model, device=device)
                
                for i in range(seq_len):
                    sar_sequence[:, i*3] = state_emb[:, i]      # State
                    sar_sequence[:, i*3+1] = action_emb[:, i]   # Action  
                    sar_sequence[:, i*3+2] = reward_emb[:, i]   # Reward
                
                # 3. ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
                device = torch.device(self.device)
                mask = torch.ones(1, seq_len * 3, device=device, dtype=torch.bool)
                
                # 4. íŠ¸ëœìŠ¤í¬ë¨¸ forward
                hidden_states = sar_sequence
                for block in self.transformer_blocks:
                    hidden_states = block(hidden_states, mask)
                
                # 5. í˜„ì¬ ìƒíƒœ ì„ë² ë”© ì¶”ì¶œ (ë§ˆì§€ë§‰ state ìœ„ì¹˜)
                if seq_len > 0:
                    current_state_emb = hidden_states[:, (seq_len-1)*3, :]  # ë§ˆì§€ë§‰ state
                else:
                    current_state_emb = hidden_states[:, 0, :]  # ì²« ë²ˆì§¸ ìœ„ì¹˜
                
                # 6. ë³´ìƒ ê³„ì‚° (ì„ íƒì )
                current_reward = 0.0
                if use_reward_guidance:
                    try:
                        reward_info = reward_calculator.calculate_reward_from_state_embedding(
                            current_state_emb, num_qubits=num_qubits
                        )
                        current_reward = reward_info['total_reward']
                        print(f"   Current reward: {current_reward:.4f}")
                        print(f"   Predicted properties: {reward_info['predicted_properties']}")
                        
                        # ë†’ì€ ë³´ìƒ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
                        if current_reward > 0.8:
                            print(f"ğŸ‰ High reward achieved ({current_reward:.4f}), stopping generation")
                            break
                            
                    except Exception as e:
                        print(f"   Reward calculation failed: {e}")
                        current_reward = 0.0
                
                # 7. ë‹¤ìŒ ì•¡ì…˜ ì˜ˆì¸¡
                gate_logits = self.gate_head(current_state_emb)  # [1, num_gates]
                
                # ë³´ìƒ ê¸°ë°˜ ë°”ì´ì–´ìŠ¤ ì ìš©
                if use_reward_guidance and current_reward > 0:
                    reward_bias = current_reward * 2.0  # ë³´ìƒ ìŠ¤ì¼€ì¼ë§
                    gate_logits = gate_logits + reward_bias
                
                # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§ ë° ìƒ˜í”Œë§
                gate_probs = F.softmax(gate_logits / temperature, dim=-1)
                
                # Top-k ìƒ˜í”Œë§
                if top_k > 0:
                    top_k_probs, top_k_indices = torch.topk(gate_probs, min(top_k, gate_probs.shape[-1]))
                    gate_idx = top_k_indices[0, torch.multinomial(top_k_probs[0], 1)].item()
                else:
                    gate_idx = torch.multinomial(gate_probs[0], 1).item()
                
                # 8. ê²Œì´íŠ¸ ì •ë³´ ì¶”ì¶œ
                gate_registry = QuantumGateRegistry()
                gate_name = gate_registry.get_gate_name_by_index(gate_idx)
                
                if gate_name is None:
                    print(f"   Invalid gate index: {gate_idx}, stopping generation")
                    break
                
                print(f"   Selected gate: {gate_name} (idx: {gate_idx})")
                
                # 9. íë¹— ìœ„ì¹˜ ì˜ˆì¸¡
                position_logits = self.position_head(current_state_emb)  # [1, max_qubits]
                selected_qubits = self.sample_qubits_for_gate(
                    position_logits, gate_name, num_qubits, temperature
                )
                
                # 10. íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ (í•„ìš”í•œ ê²½ìš°)
                param_values = []
                gate_info = gate_registry.get_gate_info(gate_name)
                if gate_info and gate_info.get('num_params', 0) > 0:
                    for i in range(gate_info['num_params']):
                        param_logit = self.parameter_heads[i](current_state_emb)
                        param_values.append(param_logit)
                    
                    parameters = self.sample_parameters_for_gate(
                        param_values, gate_name, gate_info['num_params']
                    )
                else:
                    parameters = []
                
                # 11. ìƒì„±ëœ ê²Œì´íŠ¸ ì €ì¥
                generated_gate = {
                    'gate': gate_name,
                    'qubits': selected_qubits,
                    'params': parameters
                }
                generated_gates.append(generated_gate)
                
                print(f"   Generated: {generated_gate}")
                
                # 12. ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´)
                # ì˜ˆì¸¡ëœ ê²Œì´íŠ¸ë¥¼ í˜„ì¬ íšŒë¡œì— ì¶”ê°€í•˜ì—¬ ìƒˆë¡œìš´ ìƒíƒœ ì„ë² ë”© ìƒì„±
                predicted_gate_info = {
                    'gate_name': gate_name,
                    'qubits': selected_qubits,
                    'parameter_value': parameters[0] if parameters else 0.0
                }
                
                # í˜„ì¬ê¹Œì§€ì˜ ê²Œì´íŠ¸ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
                current_circuit_gates = []
                for prev_gate in generated_gates[:-1]:  # ë°©ê¸ˆ ì¶”ê°€í•œ ê²Œì´íŠ¸ ì œì™¸
                    current_circuit_gates.append({
                        'gate_name': prev_gate['gate'],
                        'qubits': prev_gate['qubits'],
                        'parameter_value': prev_gate['params'][0] if prev_gate['params'] else 0.0
                    })
                
                # ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ ìƒˆë¡œìš´ ìƒíƒœ ìƒì„±
                new_state_emb = self.embedding.create_incremental_state_embedding(
                    current_circuit_gates, 
                    predicted_gate_info,
                    num_qubits=num_qubits
                )
                
                # ì•¡ì…˜ ì„ë² ë”© (ì˜ˆì¸¡ëœ ê²Œì´íŠ¸)
                device = torch.device(self.device)
                gate_tensor = torch.tensor([[gate_idx, selected_qubits[0], 
                                          selected_qubits[1] if len(selected_qubits) > 1 else selected_qubits[0],
                                          parameters[0] if parameters else 0.0]], 
                                         dtype=torch.float32, device=device)
                action_emb = self.embedding.state(gate_tensor).squeeze(0)
                
                # ë³´ìƒ ì„ë² ë”©
                reward_tensor = torch.tensor([current_reward], device=device)
                reward_emb_new = self.embedding.reward_embed(reward_tensor.unsqueeze(0)).squeeze(0)
                
                # ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                current_context['states'].append(new_state_emb)
                current_context['actions'].append(action_emb)
                current_context['rewards'].append(reward_emb_new)
                
                # ì¢…ë£Œ ì¡°ê±´ ì²´í¬
                if gate_name in ['measure', 'barrier'] or len(generated_gates) >= max_length:
                    break
        
        print(f"\nğŸ¯ Generation completed: {len(generated_gates)} gates generated")
        return generated_gates


class DebugMode:
    """ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •"""
    TENSOR_DIM = "tensor_dim"          # í…ì„œ ì°¨ì› í…ŒìŠ¤íŠ¸
    EMBEDDING = "embedding"            # ì„ë² ë”© ë””ë²„ê·¸
    MODEL_PREDICTION = "model_prediction"  # ëª¨ë¸ ì˜ˆì¸¡ ë””ë²„ê·¸
    MODEL_OUTPUT = "model_output"      # ëª¨ë¸ ì¶œë ¥ ë””ë²„ê·¸
    
    # í˜„ì¬ í™œì„±í™”ëœ ë””ë²„ê·¸ ëª¨ë“œë“¤
    ACTIVE_MODES = {MODEL_OUTPUT}
    
    @staticmethod
    def is_active(mode: str) -> bool:
        """ë””ë²„ê·¸ ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return mode in DebugMode.ACTIVE_MODES


class ActionLossComputer:
    """ì•¡ì…˜ ì†ì‹¤ ê³„ì‚° ì „ìš© í´ë˜ìŠ¤ - í™•ì¥ì„± ê·¹ëŒ€í™”"""
    
    def __init__(self, loss_weights: Dict[str, float] = None, ignore_index: int = -100):
        self.weights = loss_weights or {'gate': 0.8, 'position': 0.1, 'parameter': 0.1}
        self.ignore_index = ignore_index
        self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0, ignore_index=ignore_index)
        self.mse_loss = nn.MSELoss()
    
    def compute(self, predictions: Dict, targets: Dict, mask: torch.Tensor, num_qubits: Optional[List[int]] = None, num_gates: Optional[List[int]] = None) -> Dict:
        """í†µí•© ì†ì‹¤ ê³„ì‚° (ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì§€ì›)"""
        device = mask.device
        
        #  DEBUG: ì•¡ì…˜ ë§ˆìŠ¤í¬ ì°¨ì› ë¶„ì„
        debug_print(f" MASK_SHAPE: {mask.shape}")
        debug_print(f" MASK_DTYPE: {mask.dtype}")
        debug_print(f" MASK_SUM_BEFORE_FLATTEN: {mask.sum().item()}")
        
        # ìœ íš¨í•œ ì˜ˆì¸¡ ìœ„ì¹˜ë§Œ ì„ íƒ
        valid_mask = mask.view(-1)
        debug_print(f" VALID_MASK_SHAPE: {valid_mask.shape}")
        debug_print(f" VALID_MASK_SUM: {valid_mask.sum().item()}")
        
        if valid_mask.sum() == 0:
            return self._empty_loss_dict(device)
        
        # ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì ìš©
        if num_qubits is not None and 'position_preds' in predictions:
            predictions = self._apply_dynamic_qubit_masking(predictions, num_qubits, mask)
        
        #  DEBUG: ì†ì‹¤ ê³„ì‚° ì§„í–‰ ìƒí™© ì¶”ì 
        debug_print(f" [LOSS_DEBUG] ì†ì‹¤ ê³„ì‚° ì‹œì‘ - predictions keys: {list(predictions.keys())}")
        debug_print(f" [LOSS_DEBUG] targets keys: {list(targets.keys())}")
        debug_print(f" [LOSS_DEBUG] valid_mask sum: {valid_mask.sum().item()}")
        
        # ê° ì†ì‹¤ ê³„ì‚°
        debug_print(f" [LOSS_DEBUG] Gate ì†ì‹¤ ê³„ì‚° ì‹œì‘...")
        gate_loss = self._compute_gate_loss(predictions, targets, valid_mask)
        print(f" [LOSS_DEBUG] Gate ì†ì‹¤ ì™„ë£Œ: {gate_loss.item()}")
        
        debug_print(f" [LOSS_DEBUG] Position ì†ì‹¤ ê³„ì‚° ì‹œì‘...")
        position_loss = self._compute_position_loss(predictions, targets, valid_mask, num_qubits, num_gates)
        print(f" [LOSS_DEBUG] Position ì†ì‹¤ ì™„ë£Œ: {position_loss.item()}")
        
        debug_print(f" [LOSS_DEBUG] Parameter ì†ì‹¤ ê³„ì‚° ì‹œì‘...")
        parameter_loss = self._compute_parameter_loss(predictions, targets, valid_mask)
        print(f" [LOSS_DEBUG] Parameter ì†ì‹¤ ì™„ë£Œ: {parameter_loss.item()}")
        
        #  DEBUG: ì²« ë²ˆì§¸ íšŒë¡œì˜ ì˜ˆì¸¡ vs ì •ë‹µ ë¹„êµ
        #self._debug_first_circuit_predictions(predictions, targets, mask, num_gates)
        
        # ê°€ì¤‘ í•©ê³„
        total_loss = (
            self.weights['gate'] * gate_loss + 
            self.weights['position'] * position_loss + 
            self.weights['parameter'] * parameter_loss
        )
        
        debug_print(f" [LOSS_DEBUG] ìµœì¢… ì†ì‹¤ ê³„ì‚° ì™„ë£Œ - total: {total_loss.item()}")
        
        losses = {
            'loss': total_loss,
            'gate_loss': gate_loss,
            'position_loss': position_loss,
            'parameter_loss': parameter_loss,
        }
        
        debug_print(f" [LOSS_DEBUG] ì†ì‹¤ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ")
        
        # ì •í™•ë„ ë° ë¶„ë¥˜ ë©”íŠ¸ë¦­ ì¶”ê°€ (í•˜ìœ„ í˜¸í™˜ì„±)
        if hasattr(self, '_gate_accuracy'):
            losses['gate_accuracy'] = self._gate_accuracy
        if hasattr(self, '_gate_precision'):
            losses['gate_precision'] = self._gate_precision
        if hasattr(self, '_gate_recall'):
            losses['gate_recall'] = self._gate_recall
        if hasattr(self, '_gate_f1'):
            losses['gate_f1'] = self._gate_f1
        
        debug_print(f" [LOSS_DEBUG] ì†ì‹¤ ê³„ì‚° ì™„ì „ ì¢…ë£Œ - ë°˜í™˜ ì¤€ë¹„")
        return losses
    
    def _debug_first_circuit_predictions(self, predictions: Dict, targets: Dict, mask: torch.Tensor, num_gates: Optional[List[int]] = None) -> None:
        """ì²« ë²ˆì§¸ íšŒë¡œì˜ ì˜ˆì¸¡ vs ì •ë‹µ ë¹„êµ ë””ë²„ê·¸"""
        try:
            print(f" [DEBUG_ENTRY] num_gates: {num_gates}")
            print(f" [DEBUG_ENTRY] mask.shape: {mask.shape}")
            print(f" [DEBUG_ENTRY] predictions keys: {list(predictions.keys())}")
            print(f" [DEBUG_ENTRY] targets keys: {list(targets.keys())}")
            
            if num_gates is None or len(num_gates) == 0:
                print(" [DEBUG_EXIT] num_gatesê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìŒ")
                return
            
            batch_size, seq_len = mask.shape
            first_circuit_gates = num_gates[0]
            print(f" [DEBUG_INFO] first_circuit_gates: {first_circuit_gates}")
            
            # ì²« ë²ˆì§¸ íšŒë¡œì˜ ìœ íš¨í•œ ê²Œì´íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
            first_circuit_mask = mask[0]  # [seq_len]
            valid_positions = torch.where(first_circuit_mask)[0][:first_circuit_gates]  # ì²« Nê°œ ìœ„ì¹˜ë§Œ
            print(f" [DEBUG_INFO] valid_positions: {valid_positions}")
            
            if len(valid_positions) == 0:
                print(" [DEBUG_EXIT] valid_positionsê°€ ë¹„ì–´ìˆìŒ")
                return
            
            print(f"\nğŸ” [DEBUG] ì²« ë²ˆì§¸ íšŒë¡œ ì˜ˆì¸¡ ë¶„ì„ (ê²Œì´íŠ¸ ìˆ˜: {first_circuit_gates})")
            print("=" * 80)
        
            # Gate predictions vs targets
            if 'gate_logits' in predictions and 'gate_targets' in targets:
                gate_logits = predictions['gate_logits'][0]  # [seq_len, 20]
                gate_preds = torch.argmax(gate_logits, dim=-1)  # [seq_len]
                
                # íƒ€ê²Ÿ ì²˜ë¦¬
                gate_targets = targets['gate_targets']
                if gate_targets.dim() == 1:
                    # [batch*seq] í˜•íƒœì¸ ê²½ìš°
                    first_circuit_targets = gate_targets[:first_circuit_gates]
                else:
                    # [batch, seq] í˜•íƒœì¸ ê²½ìš°
                    first_circuit_targets = gate_targets[0, :first_circuit_gates]
                
                print(f"ğŸ¯ Gate ì˜ˆì¸¡ vs ì •ë‹µ (ì²˜ìŒ {min(10, first_circuit_gates)}ê°œ):")
                for i, pos in enumerate(valid_positions[:10]):
                    pred_gate = gate_preds[pos].item()
                    true_gate = first_circuit_targets[i].item() if i < len(first_circuit_targets) else -1
                    match = "âœ…" if pred_gate == true_gate else "âŒ"
                    print(f"   ìœ„ì¹˜ {pos:2d}: ì˜ˆì¸¡={pred_gate:2d}, ì •ë‹µ={true_gate:2d} {match}")
            
            # Position predictions vs targets  
            if 'position_preds' in predictions and 'position_targets' in targets:
                position_preds = predictions['position_preds'][0]  # [seq_len, 32, 2]
                position_logits = position_preds[:, 0, :]  # [seq_len, 2] - ì²« ë²ˆì§¸ íë¹—ë§Œ
                
                position_targets = targets['position_targets']
                if position_targets.dim() == 2 and position_targets.shape[1] == 2:
                    # [N, 2] í˜•íƒœ
                    first_circuit_pos_targets = position_targets[:first_circuit_gates]
                else:
                    print("   Position targets í˜•íƒœë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ")
                    first_circuit_pos_targets = None
                
                if first_circuit_pos_targets is not None:
                    print(f"ğŸ¯ Position ì˜ˆì¸¡ vs ì •ë‹µ (ì²˜ìŒ {min(5, first_circuit_gates)}ê°œ):")
                    for i, pos in enumerate(valid_positions[:5]):
                        pred_pos = position_logits[pos]  # [2]
                        true_pos = first_circuit_pos_targets[i] if i < len(first_circuit_pos_targets) else torch.tensor([-1, -1])
                        print(f"   ìœ„ì¹˜ {pos:2d}: ì˜ˆì¸¡=[{pred_pos[0]:.2f}, {pred_pos[1]:.2f}], ì •ë‹µ=[{true_pos[0]}, {true_pos[1]}]")
            
                print("=" * 80)
        
        except Exception as e:
            print(f" [DEBUG_ERROR] ë””ë²„ê·¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def _compute_gate_loss(self, predictions, targets, valid_mask):
        """ê²Œì´íŠ¸ íƒ€ì… ì†ì‹¤ ê³„ì‚°"""
        # Gate prediction logits
        gate_logits = predictions['gate_logits']  # [batch, seq, num_types]
        batch_size, seq_len, num_gate_types = gate_logits.shape
        
        # ë¡œì§“ ì¬êµ¬ì„± (Reshape logits)
        reshaped_logits = gate_logits.reshape(-1, num_gate_types)  # [batch*seq, num_types]
        
        # ë””ë²„ê¹… ì •ë³´
        debug_print(f" GATE_LOGITS_SHAPE: {gate_logits.shape}")
        debug_print(f" RESHAPED_LOGITS_SHAPE: {reshaped_logits.shape}")
        debug_print(f" VALID_MASK_FOR_GATE: {valid_mask.shape}")
        
        # ë§ˆìŠ¤í¬ê°€ Trueì¸ ìœ„ì¹˜ë§Œ ì„ íƒ (ì§ì ‘ ì¸ë±ì‹±)
        selected_indices = torch.where(valid_mask)[0]
        debug_print(f" SELECTED_INDICES_COUNT: {len(selected_indices)}")
        
        # ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì˜ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        max_idx = reshaped_logits.shape[0] - 1
        valid_indices = selected_indices[selected_indices <= max_idx]
        
        if len(valid_indices) < len(selected_indices):
            debug_print(f"âš ï¸ ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼! {len(selected_indices) - len(valid_indices)}ê°œ ì¸ë±ìŠ¤ ì œì™¸ë¨")
        
        # ë§ˆìŠ¤í¬ ì ìš© - ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        gate_logits = reshaped_logits[valid_indices]
        debug_print(f" SELECTED_LOGITS_SHAPE: {gate_logits.shape}")
        
        # íƒ€ê²Ÿ ì¸ë±ì‹± - ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        gate_targets = targets['gate_targets'].reshape(-1)
        
        # ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì˜ íƒ€ê²Ÿë§Œ ì‚¬ìš©
        if len(gate_targets) > len(valid_indices):
            gate_targets = gate_targets[valid_indices]
        else:
            # íƒ€ê²Ÿ ë°°ì—´ì´ ë” ì‘ì€ ê²½ìš°
            max_target_idx = min(len(gate_targets)-1, max(valid_indices))
            usable_indices = valid_indices[valid_indices <= max_target_idx]
            gate_logits = reshaped_logits[usable_indices]
            gate_targets = gate_targets[usable_indices]
            debug_print(f"âš ï¸ íƒ€ê²Ÿ í¬ê¸° ì œí•œìœ¼ë¡œ {len(valid_indices) - len(usable_indices)}ê°œ ì¸ë±ìŠ¤ ì¶”ê°€ ì œì™¸")
        
        debug_print(f" FINAL_LOGITS_SHAPE: {gate_logits.shape}, FINAL_TARGETS_SHAPE: {gate_targets.shape}")
        
        # í…ì„œ íƒ€ì…ì„ Longìœ¼ë¡œ ë³€í™˜ (cross_entropyëŠ” Long íƒ€ì…ì„ ìš”êµ¬í•¨)
        if gate_targets.dtype == torch.bool:
            gate_targets = gate_targets.long()
        elif gate_targets.dtype != torch.long:
            gate_targets = gate_targets.to(torch.long)
        
        # íŒ¨ë”© íƒ€ê²Ÿ(-1) í•„í„°ë§
        valid_target_mask = gate_targets >= 0
        if valid_target_mask.sum() == 0:
            return torch.tensor(0.0, device=gate_logits.device)
        
        final_logits = gate_logits[valid_target_mask]
        final_targets = gate_targets[valid_target_mask]
        
        # ì •í™•ë„ ê³„ì‚° ë° ì €ì¥
        gate_predictions = torch.argmax(final_logits, dim=-1)
        self._gate_accuracy = (gate_predictions == final_targets).float().mean()
        
        # F1, Precision, Recall ê³„ì‚° (ë‹¤ì¤‘ í´ë˜ìŠ¤)
        self._compute_classification_metrics(gate_predictions, final_targets)
        
        return self.focal_loss(final_logits, final_targets)
    
    def _compute_classification_metrics(self, predictions: torch.Tensor, targets: torch.Tensor):
        """F1, Precision, Recall ê³„ì‚° (ë§¤í¬ë¡œ í‰ê· )"""
        predictions_np = predictions.cpu().numpy()
        targets_np = targets.cpu().numpy()
        
        # ê³ ìœ  í´ë˜ìŠ¤ë“¤
        unique_classes = np.unique(np.concatenate([predictions_np, targets_np]))
        
        if len(unique_classes) <= 1:
            # ë‹¨ì¼ í´ë˜ìŠ¤ì¸ ê²½ìš°
            self._gate_precision = 1.0
            self._gate_recall = 1.0
            self._gate_f1 = 1.0
            return
        
        # í´ë˜ìŠ¤ë³„ precision, recall ê³„ì‚°
        precisions = []
        recalls = []
        f1s = []
        
        for cls in unique_classes:
            # True Positive, False Positive, False Negative
            tp = np.sum((predictions_np == cls) & (targets_np == cls))
            fp = np.sum((predictions_np == cls) & (targets_np != cls))
            fn = np.sum((predictions_np != cls) & (targets_np == cls))
            
            # Precision, Recall ê³„ì‚°
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            precisions.append(precision)
            recalls.append(recall)
            f1s.append(f1)
        
        # ë§¤í¬ë¡œ í‰ê· 
        self._gate_precision = np.mean(precisions)
        self._gate_recall = np.mean(recalls)
        self._gate_f1 = np.mean(f1s)
    
    def _compute_position_loss(self, predictions: Dict, targets: Dict, valid_mask: torch.Tensor,
                               num_qubits: Optional[List[int]] = None, num_gates: Optional[List[int]] = None) -> torch.Tensor:
        position_preds = predictions['position_preds']  # [batch, seq_len, max_qubits, 2]
        batch_size, seq_len, max_qubits, pos_dim = position_preds.shape
        
        debug_print(f"ğŸ”position_preds shape: {position_preds.shape}")
        
        # ì²« ë²ˆì§¸ íë¹— ìœ„ì¹˜ë§Œ ì˜ˆì¸¡ (ê°„ë‹¨í™”)
        position_logits = position_preds[:, :, 0, :]  # [batch, seq_len, 2]
        position_logits_flat = position_logits.reshape(-1, 2)  # [batch*seq_len, 2]
        
        # valid_maskë¡œ ì‹¤ì œ ê²Œì´íŠ¸ ìœ„ì¹˜ë§Œ ì„ íƒ
        valid_position_logits = position_logits_flat[valid_mask]  # [num_valid_gates, 2]
        debug_print(f"  valid_position_logits shape: {valid_position_logits.shape}")
        
        # position_targets ê°€ì ¸ì˜¤ê¸° (qubit_targetsë¡œ ë§¤í•‘)
        if 'qubit_targets' not in targets:
            debug_print(f"  qubit_targets í‚¤ê°€ ì—†ìŒ!")
            return torch.tensor(0.0, device=position_preds.device)
        
        position_targets = targets['qubit_targets']  # [batch, seq_len, 2]
        debug_print(f"ğŸ”position_targets shape: {position_targets.shape}")
        debug_print(f"ğŸ”position_targets sample: {position_targets[:2]}")
        
        # position_targetsë¥¼ flatí•˜ê²Œ ë³€í™˜
        position_targets_flat = position_targets.reshape(-1, 2)  # [batch*seq_len, 2]
        debug_print(f"ğŸ”position_targets_flat shape: {position_targets_flat.shape}")
        
        # valid_maskë¡œ í•„í„°ë§
        valid_position_targets = position_targets_flat[valid_mask]
        debug_print(f"ğŸ”valid_position_targets sample: {valid_position_targets[:5]}")
        
        # íŒ¨ë”©ëœ íƒ€ê²Ÿ(-1) ì œê±°
        non_padding_mask = (valid_position_targets[:, 0] >= 0) & (valid_position_targets[:, 1] >= 0)
        debug_print(f"ğŸ”non_padding_mask sum: {non_padding_mask.sum().item()}")
        
        if non_padding_mask.sum() == 0:
            debug_print(f"ğŸ”ëª¨ë“  position targetsì´ íŒ¨ë”©(-1)ì„!")
            return torch.tensor(0.0, device=position_preds.device)
        
        final_preds = valid_position_logits[non_padding_mask]
        final_targets = valid_position_targets[non_padding_mask].float()
        
        # ğŸš¨ ê°•ë ¥í•œ íë¹— ë²”ìœ„ í˜ë„í‹° ì ìš©
        position_loss = F.mse_loss(final_preds, final_targets, reduction='mean')
        
        # íë¹— ë²”ìœ„ ìœ„ë°˜ í˜ë„í‹° ê³„ì‚°
        if num_qubits is not None:
            penalty_loss = self._compute_qubit_range_penalty(
                final_preds, final_targets, num_qubits, valid_mask, non_padding_mask
            )
            # í˜ë„í‹°ë¥¼ ê¸°ë³¸ ì†ì‹¤ì— ì¶”ê°€ (ê°•ë ¥í•œ ê°€ì¤‘ì¹˜ ì ìš©)
            position_loss = position_loss + 10.0 * penalty_loss
        
        return position_loss
    
    def _compute_qubit_range_penalty(self, predictions: torch.Tensor, targets: torch.Tensor, 
                                   num_qubits: List[int], valid_mask: torch.Tensor, 
                                   non_padding_mask: torch.Tensor) -> torch.Tensor:
        """
        íë¹— ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì˜ˆì¸¡ì— ëŒ€í•œ ê°•ë ¥í•œ í˜ë„í‹° ê³„ì‚°
        
        Args:
            predictions: ì˜ˆì¸¡ëœ íë¹— ìœ„ì¹˜ [num_valid, 2]
            targets: ì‹¤ì œ íë¹— ìœ„ì¹˜ [num_valid, 2]  
            num_qubits: ê° íšŒë¡œì˜ íë¹— ìˆ˜ [batch_size]
            valid_mask: ìœ íš¨í•œ ìœ„ì¹˜ ë§ˆìŠ¤í¬
            non_padding_mask: íŒ¨ë”©ì´ ì•„ë‹Œ ìœ„ì¹˜ ë§ˆìŠ¤í¬
        
        Returns:
            í˜ë„í‹° ì†ì‹¤ (íë¹— ë²”ìœ„ ìœ„ë°˜ì‹œ í° ê°’)
        """
        device = predictions.device
        penalty_loss = torch.tensor(0.0, device=device)
        
        # ë°°ì¹˜ë³„ë¡œ íë¹— ë²”ìœ„ í™•ì¸
        batch_size = len(num_qubits)
        
        # valid_maskì™€ non_padding_maskë¥¼ í†µí•´ ë°°ì¹˜ ì¸ë±ìŠ¤ ë³µì›
        batch_indices = []
        current_idx = 0
        
        for batch_idx in range(batch_size):
            # ì´ ë°°ì¹˜ì˜ ìœ íš¨í•œ ìœ„ì¹˜ ìˆ˜ ê³„ì‚° (ê·¼ì‚¬ì¹˜)
            batch_valid_count = valid_mask.sum().item() // batch_size
            
            for _ in range(batch_valid_count):
                if current_idx < len(predictions):
                    batch_indices.append(batch_idx)
                    current_idx += 1
        
        # ì˜ˆì¸¡ê°’ì´ íë¹— ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ”ì§€ í™•ì¸
        total_violations = 0
        total_penalty = 0.0
        
        for i, pred in enumerate(predictions):
            if i < len(batch_indices):
                batch_idx = batch_indices[i]
                max_qubit = num_qubits[batch_idx] - 1  # 0-indexed
                
                # ë‘ íë¹— ìœ„ì¹˜ ëª¨ë‘ í™•ì¸
                qubit1, qubit2 = pred[0], pred[1]
                
                # ë²”ìœ„ ìœ„ë°˜ ê²€ì‚¬
                violation_penalty = 0.0
                
                # íë¹—1 ë²”ìœ„ ìœ„ë°˜
                if qubit1 < 0 or qubit1 > max_qubit:
                    violation_penalty += torch.abs(qubit1 - torch.clamp(qubit1, 0, max_qubit))
                    total_violations += 1
                
                # íë¹—2 ë²”ìœ„ ìœ„ë°˜  
                if qubit2 < 0 or qubit2 > max_qubit:
                    violation_penalty += torch.abs(qubit2 - torch.clamp(qubit2, 0, max_qubit))
                    total_violations += 1
                
                total_penalty += violation_penalty
        
        # ìœ„ë°˜ì´ ìˆìœ¼ë©´ ê°•ë ¥í•œ í˜ë„í‹° ì ìš©
        if total_violations > 0:
            penalty_loss = torch.tensor(total_penalty, device=device)
            debug_print(f"ğŸš¨ íë¹— ë²”ìœ„ ìœ„ë°˜ ê°ì§€: {total_violations}ê°œ ìœ„ë°˜, í˜ë„í‹°: {penalty_loss.item():.4f}")
        
        return penalty_loss
    
    def _compute_parameter_loss(self, predictions: Dict, targets: Dict, valid_mask: torch.Tensor) -> torch.Tensor:
        # íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ ì •ë³´ ì°¨ì› ë””ë²„ê¹…
        param_preds = predictions['parameter_preds']
        debug_print(f" PARAMETER_PREDS_SHAPE: {param_preds.shape}")
        
        # ë¦¬ì„œì´í•‘
        reshaped_preds = param_preds.reshape(-1)
        debug_print(f" RESHAPED_PARAM_PREDS_SHAPE: {reshaped_preds.shape}")
        
        # ì•ˆì „í•˜ê²Œ ë§ˆìŠ¤í¬ ì ìš© (ì°¨ì›ì´ ë§ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ)
        if len(reshaped_preds) != len(valid_mask):
            debug_print(f"âš ï¸ íŒŒë¼ë¯¸í„° ì°¨ì› ë¶ˆì¼ì¹˜ ë°œìƒ! ê°€ì¥ ì‘ì€ ì°¨ì›ìœ¼ë¡œ ì˜ë¼ëƒ„")
            min_len = min(len(reshaped_preds), len(valid_mask))
            valid_mask = valid_mask[:min_len]
            reshaped_preds = reshaped_preds[:min_len]
        
        # ë§ˆìŠ¤í¬ ì ìš©
        param_preds = reshaped_preds[valid_mask]
        
        # parameter_targets í™•ì¸
        if 'parameter_targets' not in targets:
            return torch.tensor(0.0, device=param_preds.device)
        
        # parameter_targetsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ì„œë¡œ ë³€í™˜
        if isinstance(targets['parameter_targets'], list):
            if len(targets['parameter_targets']) == 0:
                return torch.tensor(0.0, device=param_preds.device)
            parameter_targets_tensor = torch.tensor(targets['parameter_targets'], device=param_preds.device)
        else:
            parameter_targets_tensor = targets['parameter_targets']
        
        # íƒ€ê²Ÿ ì¬ìƒì„±
        reshaped_targets = parameter_targets_tensor.reshape(-1)
        if len(reshaped_targets) > len(valid_mask):
            reshaped_targets = reshaped_targets[:len(valid_mask)]
        param_targets = reshaped_targets[valid_mask]
        
        # NaN ì²˜ë¦¬
        non_nan_mask = ~torch.isnan(param_targets)
        if non_nan_mask.sum() > 0:
            return self.mse_loss(param_preds[non_nan_mask], param_targets[non_nan_mask])
        return torch.tensor(0.0, device=param_preds.device)
    
    def _apply_dynamic_qubit_masking(self, predictions: Dict, num_qubits: List[int], mask: torch.Tensor) -> Dict:
        """ğŸš€ ì—„ê²©í•œ ë™ì  íë¹— ë§ˆìŠ¤í‚¹: ê° íšŒë¡œì˜ ì •í™•í•œ íë¹— ìˆ˜ì— ë§ê²Œ ì˜ˆì¸¡ì„ ë§ˆìŠ¤í‚¹"""
        if 'position_preds' not in predictions:
            raise ValueError("âŒ CRITICAL ERROR: position_predsê°€ ì˜ˆì¸¡ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        if num_qubits is None:
            raise ValueError("âŒ CRITICAL ERROR: num_qubits ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì˜ˆì¸¡ ë³µì‚¬ (ì›ë³¸ ìˆ˜ì • ë°©ì§€)
        masked_predictions = predictions.copy()
        position_preds = predictions['position_preds'].clone()  # [batch, num_actions, max_qubits, 2]
        
        batch_size, num_actions, max_qubits, qubit_dims = position_preds.shape
        
        # ğŸš€ ë°°ì¹˜ í¬ê¸° ê²€ì¦
        if len(num_qubits) != batch_size:
            raise ValueError(
                f"âŒ CRITICAL ERROR: ë°°ì¹˜ í¬ê¸° ë¶ˆì¼ì¹˜!\n"
                f"   position_preds ë°°ì¹˜ í¬ê¸°: {batch_size}\n"
                f"   num_qubits ê¸¸ì´: {len(num_qubits)}"
            )
        
        # ğŸš€ ê° ë°°ì¹˜ë³„ë¡œ ì •í™•í•œ ë™ì  ë§ˆìŠ¤í‚¹ ì ìš©
        for batch_idx in range(batch_size):
            circuit_qubits = num_qubits[batch_idx]
            
            # ğŸš€ íë¹— ìˆ˜ ê²€ì¦ ë° ìë™ ì¡°ì •
            if circuit_qubits <= 0:
                raise ValueError(f"âŒ CRITICAL ERROR: íšŒë¡œ {batch_idx}ì˜ íë¹— ìˆ˜ê°€ 0 ì´í•˜ì…ë‹ˆë‹¤: {circuit_qubits}")
            
            if circuit_qubits > max_qubits:
                debug_print(f"âš ï¸ íšŒë¡œ {batch_idx}: íë¹— ìˆ˜ ì´ˆê³¼ ê°ì§€ - ìë™ ì¡°ì •")
                debug_print(f"   ì›ë³¸ íë¹— ìˆ˜: {circuit_qubits}")
                debug_print(f"   ëª¨ë¸ ìµœëŒ€ íë¹—: {max_qubits}")
                debug_print(f"   â†’ {max_qubits}ê°œë¡œ ì œí•œ")
                circuit_qubits = max_qubits
            
            # âœ… ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ë¥¼ -infë¡œ ë§ˆìŠ¤í‚¹ (softmaxì—ì„œ í™•ë¥  0ì´ ë¨)
            for action_idx in range(num_actions):
                for qubit_dim in range(qubit_dims):  # qubit1, qubit2
                    # circuit_qubits ì´ìƒì˜ ì¸ë±ìŠ¤ëŠ” -infë¡œ ë§ˆìŠ¤í‚¹
                    position_preds[batch_idx, action_idx, circuit_qubits:, qubit_dim] = float('-inf')
        
        masked_predictions['position_preds'] = position_preds
        return masked_predictions
    
    def _empty_loss_dict(self, device: torch.device) -> Dict:
        return {
            'total_loss': torch.tensor(0.0, device=device),
            'gate_loss': torch.tensor(0.0, device=device),
            'position_loss': torch.tensor(0.0, device=device),
            'parameter_loss': torch.tensor(0.0, device=device),
            'gate_accuracy': torch.tensor(0.0, device=device)
        }

    def forward(self, predictions, targets, action_prediction_mask, eos_mask=None, num_qubits=None):
        """ í´ë¦°í•œ ì†ì‹¤ ê³„ì‚° (ë³µì¡í•œ ë¡œì§ ì œê±°)"""
        device = action_prediction_mask.device
        
        # ìƒˆë¡œìš´ íƒ€ê²Ÿ êµ¬ì¡° ì²˜ë¦¬
        if 'action_targets' in targets and targets['action_targets']:
            action_targets = targets['action_targets']
            
            # ActionLossComputer ì‚¬ìš©
            loss_computer = ActionLossComputer()
            combined_mask = action_prediction_mask & eos_mask if eos_mask is not None else action_prediction_mask
            
            return loss_computer.compute(predictions, action_targets, combined_mask, num_qubits=num_qubits)
        
        # ğŸ”¥ NEW: legacy íƒ€ê²Ÿ êµ¬ì¡°ë¥¼ ActionLossComputerë¡œ ë³€í™˜
        if 'target_actions' in targets:
            # legacy íƒ€ê²Ÿì„ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë³€í™˜
            action_targets = {
                'gate_targets': targets['target_actions'],
                'position_targets': targets.get('target_qubits'),
                'parameter_targets': targets.get('target_params')
            }
            
            # ActionLossComputer ì‚¬ìš©
            loss_computer = ActionLossComputer()
            combined_mask = action_prediction_mask & eos_mask if eos_mask is not None else action_prediction_mask
            
            return loss_computer.compute(predictions, action_targets, combined_mask, num_qubits=num_qubits)
        
        # FALLBACK: ê¸°ì¡´ ë³µì¡í•œ ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„±)
        return self._legacy_loss_computation(predictions, targets, action_prediction_mask, eos_mask, num_qubits)
    
    def _legacy_loss_computation(self, predictions, targets, action_prediction_mask, eos_mask, num_qubits):
        """ê¸°ì¡´ ë³µì¡í•œ ì†ì‹¤ ê³„ì‚° ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        device = action_prediction_mask.device
        total_loss = torch.tensor(0.0, device=device)
        loss_dict = {}
        
        # EOS í† í° ì´í›„ ìœ„ì¹˜ ë§ˆìŠ¤í‚¹
        eos_mask = self.create_eos_mask(target_gates)
        
        # ë””ë²„ê¹…: ë§ˆìŠ¤í¬ í¬ê¸° í™•ì¸
        if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
            print(f" MASK_DEBUG: action_prediction_mask.shape = {action_prediction_mask.shape}")
            print(f" MASK_DEBUG: eos_mask.shape = {eos_mask.shape}")
            print(f" MASK_DEBUG: target_gates.shape = {target_gates.shape if target_gates is not None else 'None'}")
        
        # ì•¡ì…˜ ì˜ˆì¸¡ ë§ˆìŠ¤í¬ì™€ EOS ë§ˆìŠ¤í¬ ê²°í•© (í¬ê¸° ë¶ˆì¼ì¹˜ ì‹œ ì—ëŸ¬ ë°œìƒ)
        combined_mask = action_prediction_mask & eos_mask
        
        # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì—ì„œë§Œ ì†ì‹¤ ê³„ì‚°
        mask_sum = combined_mask.sum()
        
        if mask_sum == 0:
            if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
                print(" WARNING: No valid predictions to compute loss!")
            return {
                'total_loss': total_loss,
                'gate_loss': torch.tensor(0.0, device=device),
                'qubit_loss': torch.tensor(0.0, device=device),
                'param_loss': torch.tensor(0.0, device=device),
                'gate_accuracy': torch.tensor(0.0, device=device),
                'num_predictions': torch.tensor(0, device=device)
            }
        
        masked_gate_logits = gate_type_logits[combined_mask]
        masked_gate_targets = target_gates[combined_mask]
        
        # ëª¨ë¸ ì˜ˆì¸¡ ë””ë²„ê·¸ (ì„ íƒì  ì¶œë ¥)
        if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
            # íŒ¨ë”© í† í°(0ë²ˆ) ì œì™¸í•˜ê³  ì‹¤ì œ ê²Œì´íŠ¸ë§Œ ë¶„ì„
            non_pad_mask = masked_gate_targets != 0
            non_pad_targets = masked_gate_targets[non_pad_mask]
            
            if len(non_pad_targets) > 0:
                unique_targets = torch.unique(non_pad_targets)
                target_dist = torch.bincount(non_pad_targets)
                print(f" REAL_TARGETS: unique={unique_targets.tolist()}, count={len(unique_targets)}")
                print(f" REAL_DIST: {target_dist.tolist()}")
                print(f" PAD_RATIO: {(masked_gate_targets == 0).sum().item()}/{len(masked_gate_targets)} ({(masked_gate_targets == 0).float().mean():.2%})")
            else:
                print(" No non-padding targets found!")
            
            # íë¹— ìœ„ì¹˜ íƒ€ê²Ÿ ë¶„ì„
            if target_qubits:
                for i, qubit_targets in enumerate(target_qubits):
                    if qubit_targets is not None:
                        masked_qubit_targets = qubit_targets[combined_mask]
                        valid_qubits = masked_qubit_targets[masked_qubit_targets >= 0]  # -1 ì œì™¸
                        if len(valid_qubits) > 0:
                            unique_qubits = torch.unique(valid_qubits)
                            print(f" QUBIT_{i}: unique={unique_qubits.tolist()}, count={len(unique_qubits)}")
            
            # íŒŒë¼ë¯¸í„° íƒ€ê²Ÿ ë¶„ì„
            if target_params:
                for i, param_targets in enumerate(target_params):
                    if param_targets is not None:
                        masked_param_targets = param_targets[combined_mask]
                        valid_params = masked_param_targets[~torch.isnan(masked_param_targets)]  # NaN ì œì™¸
                        if len(valid_params) > 0:
                            param_range = (valid_params.min().item(), valid_params.max().item())
                            print(f"ğŸ¯ PARAM_{i}: range={param_range}, count={len(valid_params)}")
        
        gate_loss = self.cross_entropy(masked_gate_logits, masked_gate_targets)
        total_loss = total_loss + self.gate_weight * gate_loss
        loss_dict['gate_loss'] = gate_loss
        
        with torch.no_grad():
            gate_predictions = torch.argmax(masked_gate_logits, dim=-1)
            gate_accuracy = (gate_predictions == masked_gate_targets).float().mean()
            
            # ëª¨ë¸ ì˜ˆì¸¡ ë””ë²„ê·¸ (ì„ íƒì  ì¶œë ¥)
            if DebugMode.is_active(DebugMode.MODEL_PREDICTION):
                unique_preds = torch.unique(gate_predictions)
                print(f"ğŸ¤– PREDICTIONS: unique={unique_preds.tolist()}, count={len(unique_preds)}")
                print(f"ğŸ“ˆ ACCURACY: gate={gate_accuracy:.4f}")
                
            loss_dict['gate_accuracy'] = gate_accuracy
        
        # 2. íë¹— ìœ„ì¹˜ ì†ì‹¤ (ìš”êµ¬ì‚¬í•­ 1: í†µí•© í…ì„œë¡œ ì²˜ë¦¬)
        if target_qubits is not None:
            # ìƒˆë¡œìš´ í†µí•© í…ì„œ êµ¬ì¡° ì²˜ë¦¬: [batch, num_gates, max_qubits_per_gate]
            if isinstance(target_qubits, torch.Tensor) and target_qubits.dim() == 3:
                # í†µí•© íë¹— ì˜ˆì¸¡: [num_actions, max_qubits_per_gate, max_qubits + 1]
                masked_qubit_logits = qubit_position_logits[combined_mask]
                
                # í†µí•© íë¹— íƒ€ê²Ÿ: [num_actions, max_qubits_per_gate]
                masked_qubit_targets = target_qubits[combined_mask]
                
                # ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì ìš©
                if num_qubits is not None:
                    num_actions = masked_qubit_logits.shape[0]
                    max_qubits_per_gate = masked_qubit_logits.shape[1]
                    
                    for action_idx in range(num_actions):
                        # ê° íšŒë¡œì˜ ì‹¤ì œ íë¹— ìˆ˜ë¥¼ ë„˜ëŠ” ì˜ˆì¸¡ì€ ë§ˆìŠ¤í‚¹
                        circuit_idx = action_idx  # ë°°ì¹˜ ë‚´ íšŒë¡œ ì¸ë±ìŠ¤ (ë‹¨ìˆœí™”)
                        if circuit_idx < len(num_qubits):
                            max_valid_qubit = num_qubits[circuit_idx].item()
                            # ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ëŠ” ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
                            masked_qubit_logits[action_idx, qubit_pos, max_valid_qubit+1:] = -1e9
                
                # ìœ íš¨í•œ íƒ€ê²Ÿì´ ìˆëŠ” ìœ„ì¹˜ë§Œ ì†ì‹¤ ê³„ì‚°
                valid_mask = masked_qubit_targets != self.ignore_index
                if valid_mask.sum() > 0:
                    # Flatten for cross entropy: [num_valid_predictions, max_qubits + 1]
                    valid_logits = masked_qubit_logits[valid_mask]  # [num_valid, max_qubits + 1]
                    valid_targets = masked_qubit_targets[valid_mask]  # [num_valid]
                    
                    qubit_loss = self.cross_entropy(valid_logits, valid_targets)
                    total_loss = total_loss + self.qubit_weight * qubit_loss
                    loss_dict['qubit_loss'] = qubit_loss
                else:
                    loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
            
            # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
            elif isinstance(target_qubits, list) and len(target_qubits) > 0:
                # í†µí•© íë¹— ì˜ˆì¸¡: [num_actions, max_qubits_per_gate, max_qubits + 1]
                masked_qubit_logits = qubit_position_logits[combined_mask]
                
                # íƒ€ê²Ÿ íë¹—ë“¤ì„ í†µí•© í…ì„œë¡œ ë³€í™˜: [num_actions, max_qubits_per_gate]
                num_actions = masked_qubit_logits.shape[0]
                max_qubits_per_gate = masked_qubit_logits.shape[1]
                
                # í†µí•© íƒ€ê²Ÿ í…ì„œ ìƒì„±
                unified_qubit_targets = torch.full(
                    (num_actions, max_qubits_per_gate), 
                    self.ignore_index, 
                    dtype=torch.long, 
                    device=device
                )
                
                # ê° íë¹— ìœ„ì¹˜ë³„ íƒ€ê²Ÿì„ í†µí•© í…ì„œì— ë³µì‚¬
                for qubit_idx, qubit_targets in enumerate(target_qubits):
                    if qubit_targets is not None and qubit_idx < max_qubits_per_gate:
                        masked_targets = qubit_targets[combined_mask]
                        unified_qubit_targets[:, qubit_idx] = masked_targets
                
                # ë™ì  íë¹— ë§ˆìŠ¤í‚¹ ì ìš©
                if num_qubits is not None:
                    for action_idx in range(num_actions):
                        for qubit_pos in range(max_qubits_per_gate):
                            # ê° íšŒë¡œì˜ ì‹¤ì œ íë¹— ìˆ˜ë¥¼ ë„˜ëŠ” ì˜ˆì¸¡ì€ ë§ˆìŠ¤í‚¹
                            circuit_idx = action_idx  # ë°°ì¹˜ ë‚´ íšŒë¡œ ì¸ë±ìŠ¤ (ë‹¨ìˆœí™”)
                            if circuit_idx < len(num_qubits):
                                max_valid_qubit = num_qubits[circuit_idx].item()
                                # ìœ íš¨í•˜ì§€ ì•Šì€ íë¹— ì¸ë±ìŠ¤ëŠ” ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
                                masked_qubit_logits[action_idx, qubit_pos, max_valid_qubit+1:] = -1e9
                
                # ìœ íš¨í•œ íƒ€ê²Ÿì´ ìˆëŠ” ìœ„ì¹˜ë§Œ ì†ì‹¤ ê³„ì‚°
                valid_mask = unified_qubit_targets != self.ignore_index
                if valid_mask.sum() > 0:
                    # Flatten for cross entropy: [num_valid_predictions, max_qubits + 1]
                    valid_logits = masked_qubit_logits[valid_mask]  # [num_valid, max_qubits + 1]
                    valid_targets = unified_qubit_targets[valid_mask]  # [num_valid]
                    
                    qubit_loss = self.cross_entropy(valid_logits, valid_targets)
                    total_loss = total_loss + self.qubit_weight * qubit_loss
                    loss_dict['qubit_loss'] = qubit_loss
                else:
                    loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
            else:
                loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
        else:
            loss_dict['qubit_loss'] = torch.tensor(0.0, device=device)
        
        # 3. íŒŒë¼ë¯¸í„° ì†ì‹¤ (ìš”êµ¬ì‚¬í•­ 2: í†µí•© í…ì„œë¡œ ì²˜ë¦¬)
        if target_params is not None:
            # ìƒˆë¡œìš´ í†µí•© í…ì„œ êµ¬ì¡° ì²˜ë¦¬: [batch, num_gates, max_params_per_gate]
            if isinstance(target_params, torch.Tensor) and target_params.dim() == 3:
                # í†µí•© íŒŒë¼ë¯¸í„° ì˜ˆì¸¡: [num_actions, max_params_per_gate]
                masked_param_preds = parameter_predictions[combined_mask]
                
                # í†µí•© íŒŒë¼ë¯¸í„° íƒ€ê²Ÿ: [num_actions, max_params_per_gate]
                masked_param_targets = target_params[combined_mask]
                
                # ìœ íš¨í•œ íŒŒë¼ë¯¸í„° íƒ€ê²Ÿë§Œ ì†ì‹¤ ê³„ì‚° (NaN ì œì™¸)
                valid_mask = ~torch.isnan(masked_param_targets)
                non_padding_mask = position_targets != -1
        if non_padding_mask.sum() > 0:
            # ğŸš¨ ê°œë³„ íšŒë¡œ íë¹— ìˆ˜ ê¸°ë°˜ ë™ì  ì •ê·œí™”
            if num_qubits is not None and len(num_qubits) > 0:
                # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì˜ íë¹— ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”
                batch_size = len(num_qubits)
                seq_len = position_targets.size(0) // batch_size
                # í†µí•© íŒŒë¼ë¯¸í„° ì˜ˆì¸¡: [num_actions, max_params_per_gate]
                masked_param_preds = parameter_predictions[combined_mask]
                
                # íƒ€ê²Ÿ íŒŒë¼ë¯¸í„°ë“¤ì„ í†µí•© í…ì„œë¡œ ë³€í™˜: [num_actions, max_params_per_gate]
                num_actions = masked_param_preds.shape[0]
                max_params_per_gate = masked_param_preds.shape[1]
                
                # í†µí•© íƒ€ê²Ÿ í…ì„œ ìƒì„± (NaNìœ¼ë¡œ ì´ˆê¸°í™”)
                unified_param_targets = torch.full(
                    (num_actions, max_params_per_gate), 
                    float('nan'), 
                    dtype=torch.float32, 
                    device=device
                )
                
                # ê° íŒŒë¼ë¯¸í„°ë³„ íƒ€ê²Ÿì„ í†µí•© í…ì„œì— ë³µì‚¬
                for param_idx, param_targets in enumerate(target_params):
                    if param_targets is not None and param_idx < max_params_per_gate:
                        masked_targets = param_targets[combined_mask]
                        unified_param_targets[:, param_idx] = masked_targets
                
                # ìœ íš¨í•œ íŒŒë¼ë¯¸í„° íƒ€ê²Ÿë§Œ ì†ì‹¤ ê³„ì‚° (NaN ì œì™¸)
                valid_mask = ~torch.isnan(unified_param_targets)
                if valid_mask.sum() > 0:
                    valid_preds = masked_param_preds[valid_mask]
                    valid_targets = unified_param_targets[valid_mask]
                    
                    param_loss = self.mse_loss(valid_preds, valid_targets)
                    total_loss = total_loss + self.param_weight * param_loss
                    loss_dict['param_loss'] = param_loss
                else:
                    loss_dict['param_loss'] = torch.tensor(0.0, device=device)
            else:
                loss_dict['param_loss'] = torch.tensor(0.0, device=device)
        else:
            loss_dict['param_loss'] = torch.tensor(0.0, device=device)
        
        # ğŸ”¥ CRITICAL: gate_accuracyê°€ í•­ìƒ í¬í•¨ë˜ë„ë¡ ë³´ì¥
        if 'gate_accuracy' not in loss_dict:
            loss_dict['gate_accuracy'] = torch.tensor(0.0, device=device)
        
        loss_dict.update({
            'total_loss': total_loss,
            'num_predictions': torch.tensor(combined_mask.sum().item(), device=device)
        })
        
        return loss_dict


# ëª¨ë¸ íŒ©í† ë¦¬ í•¨ìˆ˜
def create_decision_transformer(
    config = None,
    d_model: int = 512,
    n_layers: int = 6,
    n_heads: int = 8,
    n_gate_types: int = 20,
    dropout: float = 0.1,
    property_prediction_model: Optional[PropertyPredictionTransformer] = None
) -> DecisionTransformer:
    """Decision Transformer ëª¨ë¸ ìƒì„±"""
    
    # config ê°ì²´ê°€ ì œê³µëœ ê²½ìš° ì‚¬ìš©
    if config is not None:
        d_model = getattr(config, 'd_model', d_model)
        n_layers = getattr(config, 'n_layers', n_layers)
        n_heads = getattr(config, 'n_heads', n_heads)
        dropout = getattr(config, 'dropout', dropout)
        attention_mode = getattr(config, 'attention_mode', 'standard')
    
    d_ff = d_model * 4  # í‘œì¤€ ë¹„ìœ¨
    
    return DecisionTransformer(
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        d_ff=d_ff,
        n_gate_types=n_gate_types,
        dropout=dropout,
        property_prediction_model=property_prediction_model
    )


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ìƒì„±
    model = create_decision_transformer(
        d_model=256,
        n_layers=4,
        n_heads=8,
        n_gate_types=20  # ğŸ”§ FIXED: í†µì¼ëœ ê²Œì´íŠ¸ íƒ€ì… ìˆ˜
    )
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    batch_size, seq_len, d_model = 2, 10, 256
    
    input_sequence = torch.randn(batch_size, seq_len, d_model)
    attention_mask = torch.tril(torch.ones(batch_size, seq_len, seq_len, dtype=torch.bool))
    action_prediction_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)
    action_prediction_mask[:, 1::3] = True  # ì•¡ì…˜ ìœ„ì¹˜
    
    # ìˆœì „íŒŒ
    outputs = model(input_sequence, attention_mask, action_prediction_mask)
    
    debug_print(f"Gate logits shape: {outputs['gate_logits'].shape}")
    debug_print(f"Position preds shape: {outputs['position_preds'].shape}")
    debug_print(f"Parameter preds shape: {outputs['parameter_preds'].shape}")
    debug_print(f"Hidden states shape: {outputs['hidden_states'].shape}")
    
    # ğŸ¯ ì†ì‹¤ ê³„ì‚° í…ŒìŠ¤íŠ¸ (ìƒˆë¡œìš´ êµ¬ì¡°)
    # ë”ë¯¸ íƒ€ê²Ÿ ë°ì´í„° ìƒì„±
    targets = {
        'gate_targets': torch.randint(0, 16, (batch_size, seq_len)),
        'position_targets': torch.randn(batch_size, seq_len, 3),
        'parameter_targets': torch.randn(batch_size, seq_len)
    }
    
    # ëª¨ë¸ì˜ compute_loss ë©”ì„œë“œ ì‚¬ìš©
    loss_outputs = model.compute_loss(
        predictions=outputs,
        targets=targets,
        action_prediction_mask=action_prediction_mask
    )
    
    print(f"Total Loss: {loss_outputs['total_loss'].item():.4f}")
    print(f"Gate Accuracy: {loss_outputs['gate_accuracy'].item():.4f}")
    print(f"Gate Loss: {loss_outputs['gate_loss'].item():.4f}")
    print(f"Position Loss: {loss_outputs['position_loss'].item():.4f}")
    print(f"Parameter Loss: {loss_outputs['parameter_loss'].item():.4f}")
