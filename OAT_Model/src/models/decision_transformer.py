"""
Decision Transformer Model
ê°„ë‹¨í•˜ê³  í™•ì¥ì„± ë†’ì€ Decision Transformer êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple, List
import math
import os
import sys
from pathlib import Path

# ê³µí†µ ë””ë²„ê·¸ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
from utils.debug_utils import debug_print, debug_tensor_info

# ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from models.modular_attention import ModularAttention, AttentionMode, create_modular_attention

# ğŸ† NEW: ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "quantumcommon"))
from gates import QuantumGateRegistry

# ğŸ—‘ï¸ REMOVED: Legacy MultiHeadAttention class - now using ModularAttention system


class TransformerBlock(nn.Module):
    """íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ (ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì§€ì›)"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, 
                 attention_mode: str = "standard"):
        super().__init__()
        
        # ğŸ† NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ ì‚¬ìš©
        self.attention = create_modular_attention(d_model, n_heads, dropout, attention_mode)
        self.attention_mode = attention_mode
        
        # ë¬¸ì œ 6 í•´ê²°: í”¼ë“œí¬ì›Œë“œ ì •ê·œí™” ìµœì í™” (ê³¼ë„í•œ ì •ê·œí™” ì œê±°)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),  # ì¤‘ê°„ dropoutë§Œ ìœ ì§€
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # Pre-norm êµ¬ì¡° (ì•ˆì •ì  epsilon)
        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        # ë¬¸ì œ 4 í•´ê²°: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° ë³µì›
        self.scale = nn.Parameter(torch.ones(1) * 0.5)  # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor, 
                grid_structure: Optional[Dict] = None, edges: Optional[List[Dict]] = None) -> torch.Tensor:
        debug_print(f"  TransformerBlock input - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        debug_print(f"  Using attention mode: {self.attention_mode}")
        
        # ğŸ† NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ (ê³ ê¸‰ ëª¨ë“œìš© ì¶”ê°€ ì¸ì ì§€ì›)
        norm_x = self.norm1(x)
        debug_print(f"    After norm1 - NaN: {torch.isnan(norm_x).any()}, min/max: {norm_x.min().item():.4f}/{norm_x.max().item():.4f}")
        
        attn_out = self.attention(norm_x, mask, grid_structure, edges)
        debug_print(f"    After {self.attention_mode} attention - NaN: {torch.isnan(attn_out).any()}, min/max: {attn_out.min().item():.4f}/{attn_out.max().item():.4f}")
        
        dropout_attn = self.dropout1(attn_out)
        debug_print(f"    After dropout1 - NaN: {torch.isnan(dropout_attn).any()}, min/max: {dropout_attn.min().item():.4f}/{dropout_attn.max().item():.4f}")
        
        scaled_attn = self.scale * dropout_attn
        debug_print(f"    After scale*dropout1 - NaN: {torch.isnan(scaled_attn).any()}, scale: {self.scale.item():.4f}")
        
        x = x + scaled_attn
        debug_print(f"    After residual1 - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        
        # Pre-norm + í”¼ë“œí¬ì›Œë“œ + ìŠ¤ì¼€ì¼ë§ëœ ì”ì°¨ ì—°ê²°
        norm_x = self.norm2(x)
        debug_print(f"    After norm2 - NaN: {torch.isnan(norm_x).any()}, min/max: {norm_x.min().item():.4f}/{norm_x.max().item():.4f}")
        
        ff_out = self.feed_forward(norm_x)
        debug_print(f"    After feedforward - NaN: {torch.isnan(ff_out).any()}, min/max: {ff_out.min().item():.4f}/{ff_out.max().item():.4f}")
        
        dropout_ff = self.dropout2(ff_out)
        debug_print(f"    After dropout2 - NaN: {torch.isnan(dropout_ff).any()}, min/max: {dropout_ff.min().item():.4f}/{dropout_ff.max().item():.4f}")
        
        scaled_ff = self.scale * dropout_ff
        debug_print(f"    After scale*dropout2 - NaN: {torch.isnan(scaled_ff).any()}")
        
        x = x + scaled_ff
        debug_print(f"    TransformerBlock output - NaN: {torch.isnan(x).any()}, min/max: {x.min().item():.4f}/{x.max().item():.4f}")
        
        return x
    
    def set_attention_mode(self, mode: str):
        """ì–´í…ì…˜ ëª¨ë“œ ë³€ê²½"""
        self.attention.set_mode(AttentionMode(mode.lower()))
        self.attention_mode = mode
        debug_print(f"TransformerBlock attention mode changed to: {mode}")


class DecisionTransformer(nn.Module):
    """Decision Transformer ëª¨ë¸"""
    
    def __init__(
        self,
        d_model: int = 512,
        n_layers: int = 6,
        n_heads: int = 8,
        d_ff: int = 2048,
        n_gate_types: int = None,  # ğŸ† NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •
        dropout: float = 0.1,
        attention_mode: str = "standard"  # ğŸ† NEW: ì–´í…ì…˜ ëª¨ë“œ ì„ íƒ
    ):
        super().__init__()
        
        self.d_model = d_model
        
        # ğŸ† NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ gate ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        if n_gate_types is None:
            self.n_gate_types = QuantumGateRegistry.get_singleton_gate_count()
            debug_print(f"ğŸ† DecisionTransformer: Using gate vocab singleton, n_gate_types = {self.n_gate_types}")
        else:
            self.n_gate_types = n_gate_types
            debug_print(f"âš ï¸ DecisionTransformer: Using manual n_gate_types = {self.n_gate_types}")
        
        self.attention_mode = attention_mode  # ğŸ† NEW: ì–´í…ì…˜ ëª¨ë“œ ì €ì¥
        
        # ğŸ† NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout, attention_mode)
            for _ in range(n_layers)
        ])
        
        # ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ (ê²Œì´íŠ¸ íƒ€ì… ì˜ˆì¸¡)
        self.action_head = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Linear(d_model, n_gate_types)
        )
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ë§¤ìš° ë³´ìˆ˜ì )
        self.apply(self._conservative_init_weights)
        
    def _conservative_init_weights(self, module):
        """ë¬¸ì œ 5 í•´ê²°: ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì•ˆì •ì„± + í•™ìŠµ ëŠ¥ë ¥)"""
        if isinstance(module, nn.Linear):
            # ë¬¸ì œ 5 í•´ê²°: gain 0.1 â†’ 0.5ë¡œ ì¦ê°€ (ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë°©ì§€)
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            # ì„ë² ë”© ì´ˆê¸°í™”ë„ ì•½ê°„ ì¦ê°€
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(
        self, 
        input_sequence: torch.Tensor,
        attention_mask: torch.Tensor,
        action_prediction_mask: torch.Tensor,
        grid_structure: Optional[Dict] = None,  # ğŸ† NEW: ê³ ê¸‰ ì–´í…ì…˜ìš©
        edges: Optional[List[Dict]] = None       # ğŸ† NEW: ê³ ê¸‰ ì–´í…ì…˜ìš©
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            input_sequence: [batch, seq_len, d_model]
            attention_mask: [batch, seq_len, seq_len] 
            action_prediction_mask: [batch, seq_len]
        
        Returns:
            Dict with predictions and logits
        """
        # ë””ë²„ê·¸: ì…ë ¥ í…Œì„œ ì²´í¬
        debug_print(f"Debug: input_sequence shape: {input_sequence.shape}")
        debug_print(f"Debug: attention_mask shape: {attention_mask.shape}")
        debug_print(f"Debug: action_prediction_mask shape: {action_prediction_mask.shape}")
        
        # ë””ë²„ê·¸: NaN ì²´í¬
        debug_print(f"Debug: input_sequence contains NaN: {torch.isnan(input_sequence).any()}")
        debug_print(f"Debug: input_sequence contains Inf: {torch.isinf(input_sequence).any()}")
        debug_print(f"Debug: input_sequence min/max: {input_sequence.min().item():.4f}/{input_sequence.max().item():.4f}")
        
        # ì°¨ì› ì¡°ì •: [batch, 1, seq_len, ...] -> [batch, seq_len, ...]
        input_sequence = input_sequence.squeeze(1)  # [batch, seq_len, d_model]
        attention_mask = attention_mask.squeeze(1)  # [batch, seq_len, seq_len]
        action_prediction_mask = action_prediction_mask.squeeze(1)  # [batch, seq_len]
        
        debug_print(f"Debug: squeezed input_sequence: {input_sequence.shape}")
        debug_print(f"Debug: squeezed attention_mask: {attention_mask.shape}")
        debug_print(f"Debug: squeezed action_prediction_mask: {action_prediction_mask.shape}")
        
        batch_size, seq_len, d_model = input_sequence.shape
        
        # ì…ë ¥ ë“œë¡­ì•„ì›ƒ
        x = self.dropout(input_sequence)
        debug_print(f"Debug: After dropout - contains NaN: {torch.isnan(x).any()}")
        
        # ğŸ† NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ í†µê³¼
        for i, transformer_block in enumerate(self.transformer_blocks):
            x = transformer_block(x, attention_mask, grid_structure, edges)
            debug_print(f"Debug: After transformer block {i} ({self.attention_mode}) - contains NaN: {torch.isnan(x).any()}")
            if torch.isnan(x).any():
                debug_print(f"Debug: NaN detected at transformer block {i}!")
                break
        
        # ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ
        debug_print(f"Debug: Before action_head - contains NaN: {torch.isnan(x).any()}")
        action_logits = self.action_head(x)  # [batch, seq_len, n_gate_types]
        debug_print(f"Debug: After action_head - contains NaN: {torch.isnan(action_logits).any()}")
        
        # ì•¡ì…˜ ìœ„ì¹˜ì—ì„œë§Œ ë¡œì§“ ì¶”ì¶œ
        action_predictions = torch.zeros_like(action_logits)
        action_predictions[action_prediction_mask] = action_logits[action_prediction_mask]
        
        return {
            'action_logits': action_logits,
            'action_predictions': action_predictions,
            'hidden_states': x
        }
    
    def set_attention_mode(self, mode: str):
        """ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ ì–´í…ì…˜ ëª¨ë“œ ë³€ê²½"""
        self.attention_mode = mode
        for block in self.transformer_blocks:
            block.set_attention_mode(mode)
        debug_print(f"DecisionTransformer attention mode changed to: {mode}")
    
    def get_attention_mode(self) -> str:
        """í˜„ì¬ ì–´í…ì…˜ ëª¨ë“œ ë°˜í™˜"""
        return self.attention_mode
    
    def compare_attention_modes(self, input_sequence: torch.Tensor, attention_mask: torch.Tensor, 
                              action_prediction_mask: torch.Tensor) -> Dict[str, Dict[str, torch.Tensor]]:
        """ì–´í…ì…˜ ëª¨ë“œë³„ ê²°ê³¼ ë¹„êµ"""
        original_mode = self.attention_mode
        results = {}
        
        for mode in ["standard", "advanced", "hybrid"]:
            self.set_attention_mode(mode)
            with torch.no_grad():
                output = self.forward(input_sequence, attention_mask, action_prediction_mask)
                results[mode] = {
                    'action_logits': output['action_logits'].clone(),
                    'action_predictions': output['action_predictions'].clone()
                }
        
        # ì›ë˜ ëª¨ë“œë¡œ ë³µêµ¬
        self.set_attention_mode(original_mode)
        return results
    
    def predict_next_action(
        self,
        input_sequence: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """ë‹¤ìŒ ì•¡ì…˜ ì˜ˆì¸¡ (ì¶”ë¡ ìš©)"""
        with torch.no_grad():
            outputs = self.forward(
                input_sequence, 
                attention_mask,
                torch.ones(input_sequence.shape[:2], dtype=torch.bool, device=input_sequence.device)
            )
            
            # ë§ˆì§€ë§‰ ìœ„ì¹˜ì˜ ì˜ˆì¸¡ ë°˜í™˜
            last_logits = outputs['action_logits'][:, -1, :]  # [batch, n_gate_types]
            return F.softmax(last_logits, dim=-1)


class DecisionTransformerLoss(nn.Module):
    """Decision Transformer ì†ì‹¤ í•¨ìˆ˜"""
    
    def __init__(self, ignore_index: int = -100):
        super().__init__()
        self.ignore_index = ignore_index
        self.cross_entropy = nn.CrossEntropyLoss(ignore_index=ignore_index)
    
    def forward(
        self,
        action_logits: torch.Tensor,
        target_actions: torch.Tensor,
        action_prediction_mask: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            action_logits: [batch, seq_len, n_gate_types]
            target_actions: [batch, seq_len] - ì •ë‹µ ì•¡ì…˜ ì¸ë±ìŠ¤
            action_prediction_mask: [batch, seq_len] - ì•¡ì…˜ ì˜ˆì¸¡ ìœ„ì¹˜
        """
        
        # ë””ë²„ê·¸: í˜•íƒœ í™•ì¸
        debug_print(f"Debug: action_logits shape: {action_logits.shape}")
        debug_print(f"Debug: target_actions shape: {target_actions.shape}")
        debug_print(f"Debug: action_prediction_mask shape: {action_prediction_mask.shape}")
        
        # ë””ë²„ê·¸: ì‹¤ì œ ê°’ë“¤ í™•ì¸
        debug_print(f"Debug: action_prediction_mask sum: {action_prediction_mask.sum().item()}")
        debug_print(f"Debug: target_actions unique values: {torch.unique(target_actions)}")
        debug_print(f"Debug: action_logits contains NaN: {torch.isnan(action_logits).any()}")
        debug_print(f"Debug: target_actions contains invalid: {(target_actions < 0).any() or (target_actions >= 20).any()}")
        
        # ì•¡ì…˜ ìœ„ì¹˜ì—ì„œë§Œ ì†ì‹¤ ê³„ì‚°
        masked_logits = action_logits[action_prediction_mask]  # [n_actions, n_gate_types]
        masked_targets = target_actions[action_prediction_mask]  # [n_actions]
        
        debug_print(f"Debug: masked_logits shape: {masked_logits.shape}")
        debug_print(f"Debug: masked_targets shape: {masked_targets.shape}")
        debug_print(f"Debug: masked_targets values: {masked_targets}")
        debug_print(f"Debug: masked_logits contains NaN: {torch.isnan(masked_logits).any()}")
        debug_print(f"Debug: masked_logits min/max: {masked_logits.min().item():.4f}/{masked_logits.max().item():.4f}")
        
        if masked_logits.numel() == 0:
            # ì˜ˆì¸¡í•  ì•¡ì…˜ì´ ì—†ëŠ” ê²½ìš°
            return {
                'loss': torch.tensor(0.0, device=action_logits.device, requires_grad=True),
                'accuracy': torch.tensor(0.0, device=action_logits.device)
            }
        
        # í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
        loss = self.cross_entropy(masked_logits, masked_targets)
        
        # ì •í™•ë„ ê³„ì‚°
        with torch.no_grad():
            predictions = torch.argmax(masked_logits, dim=-1)
            accuracy = (predictions == masked_targets).float().mean()
        
        return {
            'loss': loss,
            'accuracy': accuracy
        }


# ëª¨ë¸ íŒ©í† ë¦¬ í•¨ìˆ˜
def create_decision_transformer(
    d_model: int = 512,
    n_layers: int = 6,
    n_heads: int = 8,
    n_gate_types: int = 20,
    dropout: float = 0.1
) -> DecisionTransformer:
    """Decision Transformer ëª¨ë¸ ìƒì„±"""
    
    d_ff = d_model * 4  # í‘œì¤€ ë¹„ìœ¨
    
    return DecisionTransformer(
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        d_ff=d_ff,
        n_gate_types=n_gate_types,
        dropout=dropout
    )


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ìƒì„±
    model = create_decision_transformer(
        d_model=256,
        n_layers=4,
        n_heads=8,
        n_gate_types=20  # ğŸ”§ FIXED: í†µì¼ëœ ê²Œì´íŠ¸ íƒ€ì… ìˆ˜
    )
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    batch_size, seq_len, d_model = 2, 10, 256
    
    input_sequence = torch.randn(batch_size, seq_len, d_model)
    attention_mask = torch.tril(torch.ones(batch_size, seq_len, seq_len, dtype=torch.bool))
    action_prediction_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)
    action_prediction_mask[:, 1::3] = True  # ì•¡ì…˜ ìœ„ì¹˜
    
    # ìˆœì „íŒŒ
    outputs = model(input_sequence, attention_mask, action_prediction_mask)
    
    debug_print(f"Action logits shape: {outputs['action_logits'].shape}")
    debug_print(f"Hidden states shape: {outputs['hidden_states'].shape}")
    
    # ì†ì‹¤ ê³„ì‚° í…ŒìŠ¤íŠ¸
    loss_fn = DecisionTransformerLoss()
    target_actions = torch.randint(0, 16, (batch_size, seq_len))
    
    loss_outputs = loss_fn(
        outputs['action_logits'],
        target_actions,
        action_prediction_mask
    )
    
    print(f"Loss: {loss_outputs['loss'].item():.4f}")
    print(f"Accuracy: {loss_outputs['accuracy'].item():.4f}")
