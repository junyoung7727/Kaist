"""RTG (Return-to-Go) Calculator
Property ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Decision Transformerìš© RTG ê³„ì‚°

í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ì›Œë“œ ì„¤ê³„:
- í° ì°¨ì´(>60%): ì„ í˜• ë¦¬ì›Œë“œë¡œ ë¹ ë¥¸ í•™ìŠµ ì‹ í˜¸
- ì •ë°€í•œ ì°¨ì´(<60%): ê°€ìš°ì‹œì•ˆ ë¦¬ì›Œë“œë¡œ ì„¸ë°€í•œ ì¡°ì •
- ì ì‘ì  ê°€ì¤‘ì¹˜: ì†ì„±ë³„ ì¤‘ìš”ë„ ë°˜ì˜ (fidelity > expressibility > entanglement)
- ì—°ì†ì  ì „í™˜: 60% ê²½ê³„ì—ì„œ ë¶€ë“œëŸ¬ìš´ ì „í™˜
- ë²”ìœ„: [0, 1], ì°¨ì´ í¬ê¸°ì— ë”°ë¥¸ ìµœì  í•¨ìˆ˜ ì„ íƒ

RTG íŠ¹ì„±:
- ì‹œí€€ìŠ¤ ëì—ì„œ 0ìœ¼ë¡œ ìˆ˜ë ´ (Decision Transformer í‘œì¤€)
- ì¸í¼ëŸ°ìŠ¤ ì‹œ ëª…ì‹œì  ì„±ëŠ¥ ë ˆë²¨ ì¡°ê±´ ì œê³µ ê°€ëŠ¥
- RTG[t] = Î£(k=t to T) Î³^(k-t) * r[k], RTG[T] = 0
"""

import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
import json
import math
from pathlib import Path

class RTGCalculator:
    """Property ëª¨ë¸ ê¸°ë°˜ RTG ê³„ì‚°ê¸°"""
    
    def __init__(self, property_model, property_config, device='cpu'):
        """
        Args:
            property_model: ì‚¬ì „ í›ˆë ¨ëœ Property Prediction ëª¨ë¸
            property_config: Property ëª¨ë¸ ì„¤ì •
            device: ê³„ì‚° ë””ë°”ì´ìŠ¤
        """
        self.property_model = property_model
        self.property_config = property_config
        self.device = device
        
        # Property ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.property_model.eval()
        self.property_model.to(device)
        
    def calculate_sequence_properties(self, state_sequence: torch.Tensor, 
                                    attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ê° ìŠ¤í…ì˜ ì†ì„±ê°’ ê³„ì‚°
        
        Args:
            state_sequence: [batch_size, seq_len, d_model] ìƒíƒœ ì‹œí€€ìŠ¤
            attention_mask: [batch_size, seq_len] ì–´í…ì…˜ ë§ˆìŠ¤í¬
            
        Returns:
            step_properties: ê° ìŠ¤í…ë³„ ì†ì„±ê°’ ë”•ì…”ë„ˆë¦¬
        """
        batch_size, seq_len, d_model = state_sequence.shape
        
        # ê° ìŠ¤í…ë³„ ì†ì„±ê°’ ì €ì¥
        step_properties = {
            'entanglement': torch.zeros(batch_size, seq_len, device=self.device),
            'fidelity': torch.zeros(batch_size, seq_len, device=self.device),
            'expressibility': torch.zeros(batch_size, seq_len, device=self.device)
        }
        
        with torch.no_grad():
            for batch_idx in range(batch_size):
                for step_idx in range(seq_len):
                    # ì–´í…ì…˜ ë§ˆìŠ¤í¬ í™•ì¸
                    if not attention_mask[batch_idx, step_idx]:
                        continue
                    
                    # í˜„ì¬ ìŠ¤í…ê¹Œì§€ì˜ ëˆ„ì  ì‹œí€€ìŠ¤ ì‚¬ìš©
                    current_seq = state_sequence[batch_idx, :step_idx+1, :].unsqueeze(0)  # [1, step+1, d_model]
                    current_mask = attention_mask[batch_idx, :step_idx+1].unsqueeze(0)   # [1, step+1]
                    
                    # Property ëª¨ë¸ë¡œ ì†ì„± ì˜ˆì¸¡
                    predictions = self.property_model.predict(
                        input_sequence=current_seq,
                        attention_mask=current_mask,
                        return_hidden=False
                    )
                    
                    # ì˜ˆì¸¡ëœ ì†ì„±ê°’ ì €ì¥
                    for prop_name in ['entanglement', 'fidelity', 'expressibility']:
                        if prop_name in predictions:
                            step_properties[prop_name][batch_idx, step_idx] = predictions[prop_name].item()
        
        return step_properties
    
    def calculate_property_distance(self, predicted_properties: Dict[str, torch.Tensor],
                                  target_properties: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        ì˜ˆì¸¡ëœ ì†ì„±ê°’ê³¼ ì •ë‹µ ë ˆì´ë¸” ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
        
        Args:
            predicted_properties: ì˜ˆì¸¡ëœ ì†ì„±ê°’ë“¤
            target_properties: ì •ë‹µ ì†ì„±ê°’ë“¤
            
        Returns:
            distances: [batch_size, seq_len] ê° ìŠ¤í…ë³„ ê±°ë¦¬
        """
        batch_size, seq_len = predicted_properties['entanglement'].shape
        distances = torch.zeros(batch_size, seq_len, device=self.device)
        
        for batch_idx in range(batch_size):
            for step_idx in range(seq_len):
                step_distance = 0.0
                valid_properties = 0
                
                for prop_name in ['entanglement', 'fidelity', 'expressibility']:
                    if prop_name in predicted_properties and prop_name in target_properties:
                        pred_val = predicted_properties[prop_name][batch_idx, step_idx]
                        target_val = target_properties[prop_name][batch_idx, step_idx]
                        
                        # L2 ê±°ë¦¬ ê³„ì‚°
                        step_distance += (pred_val - target_val) ** 2
                        valid_properties += 1
                
                if valid_properties > 0:
                    # RMSEë¡œ ì •ê·œí™”
                    distances[batch_idx, step_idx] = torch.sqrt(step_distance / valid_properties)
        
        return distances
    
    def calculate_step_rewards(self, predicted_properties: Dict[str, torch.Tensor],
                             target_properties: Dict[str, torch.Tensor],
                             attention_mask: torch.Tensor) -> torch.Tensor:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ì›Œë“œ í•¨ìˆ˜: ì°¨ì´ í¬ê¸°ì— ë”°ë¥¸ ì ì‘ì  ê³„ì‚°
        
        í•µì‹¬ ì„¤ê³„:
        1. í° ì°¨ì´(>60%): ì„ í˜• í•¨ìˆ˜ë¡œ ë¹ ë¥¸ í•™ìŠµ ì‹ í˜¸ ì œê³µ
        2. ì •ë°€í•œ ì°¨ì´(<60%): ê°€ìš°ì‹œì•ˆ í•¨ìˆ˜ë¡œ ì„¸ë°€í•œ ì¡°ì •
        3. ë¶€ë“œëŸ¬ìš´ ì „í™˜: 60% ê²½ê³„ì—ì„œ ì—°ì†ì„± ë³´ì¥
        4. ì†ì„±ë³„ ê°€ì¤‘ì¹˜: fidelity > expressibility > entanglement
        
        ìˆ˜í•™ì  ì •ì˜:
        - distance > 0.6: r = 1 - distance (ì„ í˜•)
        - distance â‰¤ 0.6: r = exp(-0.5 * distanceÂ² / ÏƒÂ²) (ê°€ìš°ì‹œì•ˆ, Ïƒ=0.25)
        
        Args:
            predicted_properties: ì˜ˆì¸¡ëœ ì†ì„±ê°’ë“¤
            target_properties: ëª©í‘œ ì†ì„±ê°’ë“¤
            attention_mask: [batch_size, seq_len] ì–´í…ì…˜ ë§ˆìŠ¤í¬
            
        Returns:
            step_rewards: [batch_size, seq_len] ê° ìŠ¤í…ì˜ ì¦‰ì‹œ ë¦¬ì›Œë“œ (0~1 ë²”ìœ„)
        """
        batch_size, seq_len = attention_mask.shape
        step_rewards = torch.zeros(batch_size, seq_len, device=self.device)
        
        for batch_idx in range(batch_size):
            for step_idx in range(seq_len):
                if not attention_mask[batch_idx, step_idx]:
                    continue
                
                property_rewards = []
                
                # í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ì›Œë“œ: ì°¨ì´ í¬ê¸°ì— ë”°ë¥¸ ì ì‘ì  ê³„ì‚°
                property_weights = {
                    'fidelity': 0.4,        # ê°€ì¥ ì¤‘ìš”: íšŒë¡œ ì •í™•ë„
                    'expressibility': 0.3,  # ì¤‘ê°„: íƒìƒ‰ ëŠ¥ë ¥
                    'entanglement': 0.3     # ë³´ì¡°: ì–‘ì íŠ¹ì„±
                }
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì „í™˜ ì„ê³„ê°’
                hybrid_threshold = 0.6  # 60% ì°¨ì´ ê¸°ì¤€
                
                for prop_name in ['entanglement', 'fidelity', 'expressibility']:
                    if prop_name in predicted_properties and prop_name in target_properties:
                        pred_val = predicted_properties[prop_name][batch_idx, step_idx]
                        target_val = target_properties[prop_name][batch_idx, step_idx]
                        
                        distance = torch.abs(pred_val - target_val)
                        weight = property_weights[prop_name]
                        
                        if distance > hybrid_threshold:
                            # í° ì°¨ì´(>60%): ì„ í˜• ë¦¬ì›Œë“œë¡œ ë¹ ë¥¸ í•™ìŠµ
                            linear_reward = 1.0 - torch.clamp(distance, 0.0, 1.0)
                            reward = linear_reward
                        else:
                            # ì •ë°€í•œ ì°¨ì´(â‰¤60%): ê°€ìš°ì‹œì•ˆ ë¦¬ì›Œë“œë¡œ ì„¸ë°€í•œ ì¡°ì •
                            sigma = 0.25  # ê°€ìš°ì‹œì•ˆ í‘œì¤€í¸ì°¨
                            gaussian_reward = torch.exp(-0.5 * (distance ** 2) / (sigma ** 2))
                            
                            # 60% ì§€ì ì—ì„œ ì—°ì†ì„± ë³´ì¥ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§
                            # ì„ í˜• í•¨ìˆ˜ì˜ 60% ì§€ì  ê°’: 1 - 0.6 = 0.4
                            # ê°€ìš°ì‹œì•ˆ í•¨ìˆ˜ì˜ 60% ì§€ì  ê°’: exp(-0.5 * 0.6Â² / 0.25Â²)
                            gaussian_at_threshold = torch.exp(-0.5 * (hybrid_threshold ** 2) / (sigma ** 2))
                            linear_at_threshold = 1.0 - hybrid_threshold
                            
                            # ì—°ì†ì„±ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§
                            scale_factor = linear_at_threshold / gaussian_at_threshold
                            reward = gaussian_reward * scale_factor
                        
                        # ê°€ì¤‘ ë¦¬ì›Œë“œ ì ìš©
                        weighted_reward = weight * reward
                        property_rewards.append(weighted_reward)
                
                # ê°€ì¤‘ í•©ê³„ (ì´ë¯¸ ê°€ì¤‘ì¹˜ ì ìš©ë¨)
                if property_rewards:
                    # ê°€ì¤‘ í•©ê³„ (ì´í•© = 1.0 ë³´ì¥)
                    total_reward = torch.stack(property_rewards).sum()
                    step_rewards[batch_idx, step_idx] = torch.clamp(total_reward, 0.0, 1.0)
                else:
                    # ê¸°ë³¸ê°’
                    step_rewards[batch_idx, step_idx] = 0.0
        
        return step_rewards
    
    def calculate_rtg_rewards(self, predicted_properties: Dict[str, torch.Tensor],
                            target_properties: Dict[str, torch.Tensor],
                            attention_mask: torch.Tensor,
                            gamma: float = 0.99,
                            normalize_rtg: bool = True) -> torch.Tensor:
        """
        í‘œì¤€ Decision Transformer RTG (Return-to-Go) ê³„ì‚°
        RTG[t] = Î£(k=t to T) Î³^(k-t) * r[k]
        
        ì¤‘ìš”: RTGëŠ” ì‹œí€€ìŠ¤ ëì—ì„œ 0ìœ¼ë¡œ ìˆ˜ë ´í•´ì•¼ í•¨ (Decision Transformer í‘œì¤€)
        ì´ë¥¼ í†µí•´ ì¸í¼ëŸ°ìŠ¤ ì‹œ ëª…ì‹œì  ì„±ëŠ¥ ë ˆë²¨ ì¡°ê±´ ì œê³µ ê°€ëŠ¥
        
        Args:
            predicted_properties: ì˜ˆì¸¡ëœ ì†ì„±ê°’ë“¤
            target_properties: ëª©í‘œ ì†ì„±ê°’ë“¤  
            attention_mask: [batch_size, seq_len] ì–´í…ì…˜ ë§ˆìŠ¤í¬
            gamma: í• ì¸ íŒ©í„°
            normalize_rtg: RTG ì •ê·œí™” ì—¬ë¶€
            
        Returns:
            rtg_rewards: [batch_size, seq_len] RTG ì‹œí€€ìŠ¤ (ëì—ì„œ 0ìœ¼ë¡œ ìˆ˜ë ´)
        """
        # ê° ìŠ¤í…ì˜ ì¦‰ì‹œ ë¦¬ì›Œë“œ ê³„ì‚° (ì •ê·œí™”ëœ ê±°ë¦¬ ê¸°ë°˜)
        step_rewards = self.calculate_step_rewards(
            predicted_properties, target_properties, attention_mask
        )
        
        batch_size, seq_len = step_rewards.shape
        rtg_rewards = torch.zeros(batch_size, seq_len, device=self.device)
        
        for batch_idx in range(batch_size):
            # ìœ íš¨í•œ ìŠ¤í…ë“¤ ì°¾ê¸°
            valid_steps = attention_mask[batch_idx].nonzero(as_tuple=True)[0]
            
            if len(valid_steps) == 0:
                continue
            
            # ë’¤ì—ì„œë¶€í„° RTG ê³„ì‚° (ë™ì  í”„ë¡œê·¸ë˜ë°)
            # ë§ˆì§€ë§‰ ìŠ¤í…ì—ì„œ RTG = 0 (Decision Transformer í‘œì¤€)
            for i in range(len(valid_steps) - 1, -1, -1):
                step_idx = valid_steps[i]
                
                if i == len(valid_steps) - 1:
                    # ë§ˆì§€ë§‰ ìŠ¤í…: RTG = 0 (í‘œì¤€ Decision Transformer)
                    rtg_rewards[batch_idx, step_idx] = 0.0
                else:
                    # ì´ì „ ìŠ¤í…ë“¤: RTG = r[t] + Î³ * RTG[t+1]
                    current_reward = step_rewards[batch_idx, step_idx]
                    next_step_idx = valid_steps[i + 1]
                    future_rtg = gamma * rtg_rewards[batch_idx, next_step_idx]
                    
                    rtg_rewards[batch_idx, step_idx] = current_reward + future_rtg
        
        # RTG ì •ê·œí™” (ì„ íƒì )
        if normalize_rtg:
            for batch_idx in range(batch_size):
                valid_steps = attention_mask[batch_idx].nonzero(as_tuple=True)[0]
                if len(valid_steps) > 0:
                    # ê° ì‹œí€€ìŠ¤ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”
                    valid_rtg = rtg_rewards[batch_idx, valid_steps]
                    if valid_rtg.max() > 0:
                        # [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
                        rtg_rewards[batch_idx, valid_steps] = valid_rtg / valid_rtg.max()
        
        return rtg_rewards
    
    def precompute_rtg_for_dataset(self, dataset_path: str, 
                                 output_path: str,
                                 batch_size: int = 32) -> None:
        """
        ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ RTG ê°’ì„ ì‚¬ì „ ê³„ì‚°í•˜ì—¬ ì €ì¥
        
        Args:
            dataset_path: ì…ë ¥ ë°ì´í„°ì…‹ ê²½ë¡œ
            output_path: RTG ê³„ì‚° ê²°ê³¼ ì €ì¥ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        print(f"ğŸ”„ RTG ì‚¬ì „ ê³„ì‚° ì‹œì‘: {dataset_path}")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        rtg_results = []
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
        for i in range(0, len(dataset), batch_size):
            batch_data = dataset[i:i+batch_size]
            print(f"ğŸ“Š ë°°ì¹˜ {i//batch_size + 1}/{(len(dataset) + batch_size - 1)//batch_size} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
            batch_states, batch_targets, batch_masks = self._prepare_batch(batch_data)
            
            # ì†ì„±ê°’ ê³„ì‚°
            predicted_properties = self.calculate_sequence_properties(batch_states, batch_masks)
            
            # RTG ë¦¬ì›Œë“œ ê³„ì‚° (ìƒˆë¡œìš´ í‘œì¤€ RL ë°©ì‹)
            rtg_rewards = self.calculate_rtg_rewards(predicted_properties, batch_targets, batch_masks)
            
            # ê±°ë¦¬ ê³„ì‚° (ë””ë²„ê¹…ìš©)
            property_distances = self.calculate_property_distance(predicted_properties, batch_targets)
            
            # ê²°ê³¼ ì €ì¥
            for j, data_item in enumerate(batch_data):
                rtg_sequence = rtg_rewards[j].cpu().numpy().tolist()
                result_item = {
                    **data_item,  # ì›ë³¸ ë°ì´í„° ìœ ì§€
                    'rtg_rewards': rtg_sequence,
                    'predicted_properties': {
                        key: predicted_properties[key][j].cpu().numpy().tolist()
                        for key in predicted_properties
                    },
                    'property_distances': property_distances[j].cpu().numpy().tolist()
                }
                rtg_results.append(result_item)
        
        # ê²°ê³¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(rtg_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… RTG ì‚¬ì „ ê³„ì‚° ì™„ë£Œ: {output_path}")
        print(f"ğŸ“ˆ ì´ {len(rtg_results)}ê°œ ì‹œí€€ìŠ¤ ì²˜ë¦¬ë¨")
    
    def calculate_single_step_rtg(self, state_action: torch.Tensor, 
                                attention_mask: torch.Tensor,
                                target_properties: Dict[str, float],
                                current_step: int, total_steps: int,
                                desired_rtg: float = None,
                                gamma: float = 0.99) -> float:
        """
        ë‹¨ì¼ ìŠ¤í…ì— ëŒ€í•œ RTG ê°’ ê³„ì‚° (ì‹¤ì‹œê°„ ì‚¬ìš©)
        Decision Transformer í‘œì¤€: RTGëŠ” 0ìœ¼ë¡œ ìˆ˜ë ´, ì¸í¼ëŸ°ìŠ¤ ì‹œ ëª…ì‹œì  ì¡°ê±´ ì œê³µ
        
        Args:
            state_action: [1, 1, 2*d_model] ìƒíƒœ-ì•¡ì…˜ í…ì„œ
            attention_mask: [1, 1] ì–´í…ì…˜ ë§ˆìŠ¤í¬
            target_properties: ëª©í‘œ ì†ì„±ê°’ë“¤ {'entanglement': 0.8, 'fidelity': 0.9, ...}
            current_step: í˜„ì¬ ìŠ¤í…
            total_steps: ì „ì²´ ìŠ¤í… ìˆ˜
            desired_rtg: ì¸í¼ëŸ°ìŠ¤ ì‹œ ì›í•˜ëŠ” ì„±ëŠ¥ ë ˆë²¨ (Noneì´ë©´ ìë™ ê³„ì‚°)
            gamma: í• ì¸ íŒ©í„°
            
        Returns:
            rtg_value: RTG ê°’ (0ìœ¼ë¡œ ìˆ˜ë ´)
        """
        with torch.no_grad():
            # ì¸í¼ëŸ°ìŠ¤ ëª¨ë“œ: ëª…ì‹œì  RTG ì¡°ê±´ ì‚¬ìš©
            if desired_rtg is not None:
                # ì„ í˜• ê°ì†Œë¡œ 0ì— ìˆ˜ë ´
                progress = current_step / max(1, total_steps - 1)
                return desired_rtg * (1.0 - progress)
            
            # í›ˆë ¨ ëª¨ë“œ: Property ëª¨ë¸ ê¸°ë°˜ RTG ê³„ì‚°
            predictions = self.property_model.predict(
                input_sequence=state_action,
                attention_mask=attention_mask,
                return_hidden=False
            )
            
            # í˜„ì¬ ìŠ¤í…ì˜ ì¦‰ì‹œ ë¦¬ì›Œë“œ ê³„ì‚° (ì •ê·œí™”ëœ ê±°ë¦¬ ê¸°ë°˜)
            current_reward = 0.0
            valid_properties = 0
            
            for prop_name in ['entanglement', 'fidelity', 'expressibility']:
                if prop_name in predictions and prop_name in target_properties:
                    pred_val = predictions[prop_name].item()
                    target_val = target_properties[prop_name]
                    
                    # ì •ê·œí™”ëœ ê±°ë¦¬ ê¸°ë°˜ ë¦¬ì›Œë“œ: r = 1 - |pred - target|
                    distance = abs(pred_val - target_val)
                    reward = 1.0 - min(distance, 1.0)  # [0, 1] í´ë¨í•‘
                    
                    current_reward += reward
                    valid_properties += 1
            
            if valid_properties > 0:
                current_reward /= valid_properties
            else:
                current_reward = 0.0  # ê¸°ë³¸ê°’
            
            # RTG ê³„ì‚°: ë‚¨ì€ ìŠ¤í…ì— ë”°ë¼ 0ìœ¼ë¡œ ìˆ˜ë ´
            remaining_steps = max(0, total_steps - current_step - 1)
            
            if remaining_steps == 0:
                # ë§ˆì§€ë§‰ ìŠ¤í…: RTG = 0
                return 0.0
            else:
                # ì´ì „ ìŠ¤í…ë“¤: ê¸°í•˜ê¸‰ìˆ˜ì  ê°ì†Œë¡œ 0ì— ìˆ˜ë ´
                if gamma < 1.0:
                    rtg_value = current_reward * (1 - gamma ** remaining_steps) / (1 - gamma)
                else:
                    rtg_value = current_reward * remaining_steps
                
                # ì •ê·œí™”: ìµœëŒ€ ê°€ëŠ¥í•œ RTGë¡œ ë‚˜ëˆ„ê¸°
                max_possible_rtg = remaining_steps if gamma >= 1.0 else 1.0 / (1 - gamma)
                normalized_rtg = rtg_value / max_possible_rtg
                
                return float(normalized_rtg)
    
    def _prepare_batch(self, batch_data: List[Dict]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
        """
        ë°°ì¹˜ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
        
        Args:
            batch_data: ë°°ì¹˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            batch_states: [batch_size, seq_len, d_model]
            batch_targets: ì •ë‹µ ì†ì„±ê°’ë“¤
            batch_masks: [batch_size, seq_len]
        """
        # êµ¬í˜„ í•„ìš”: ë°ì´í„°ì…‹ í˜•ì‹ì— ë§ê²Œ ì¡°ì •
        # í˜„ì¬ëŠ” í”Œë ˆì´ìŠ¤í™€ë”
        batch_size = len(batch_data)
        max_seq_len = max(len(item.get('sequence', [])) for item in batch_data)
        d_model = self.property_config.d_model
        
        batch_states = torch.zeros(batch_size, max_seq_len, d_model, device=self.device)
        batch_masks = torch.zeros(batch_size, max_seq_len, dtype=torch.bool, device=self.device)
        
        batch_targets = {
            'entanglement': torch.zeros(batch_size, max_seq_len, device=self.device),
            'fidelity': torch.zeros(batch_size, max_seq_len, device=self.device),
            'expressibility': torch.zeros(batch_size, max_seq_len, device=self.device)
        }
        
        # TODO: ì‹¤ì œ ë°ì´í„° ë³€í™˜ ë¡œì§ êµ¬í˜„
        
        return batch_states, batch_targets, batch_masks


def create_rtg_calculator(property_checkpoint_path: str, 
                         property_config_path: str,
                         device: str = 'cpu') -> RTGCalculator:
    """
    RTG Calculator ìƒì„± í•¨ìˆ˜
    
    Args:
        property_checkpoint_path: Property ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        property_config_path: Property ëª¨ë¸ ì„¤ì • ê²½ë¡œ
        device: ê³„ì‚° ë””ë°”ì´ìŠ¤
        
    Returns:
        RTGCalculator ì¸ìŠ¤í„´ìŠ¤
    """
    from models.property_prediction_transformer import create_property_prediction_model, PropertyPredictionConfig
    
    # ì„¤ì • ë¡œë“œ
    with open(property_config_path, 'r') as f:
        config_dict = json.load(f)
    
    property_config = PropertyPredictionConfig(**config_dict)
    
    # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    property_model = create_property_prediction_model(property_config)
    checkpoint = torch.load(property_checkpoint_path, map_location=device)
    property_model.load_state_dict(checkpoint['model_state_dict'])
    
    return RTGCalculator(property_model, property_config, device)


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    calculator = create_rtg_calculator(
        property_checkpoint_path="property_prediction_checkpoints/best_model.pt",
        property_config_path="configs/property_config.json",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    # ë°ì´í„°ì…‹ì— ëŒ€í•´ RTG ì‚¬ì „ ê³„ì‚°
    calculator.precompute_rtg_for_dataset(
        dataset_path="raw_data/merged_data.json",
        output_path="processed_data/rtg_dataset.json",
        batch_size=32
    )
