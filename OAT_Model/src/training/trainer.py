"""
Decision Transformer Training Pipeline
ê°„ë‹¨í•˜ê³  í™•ì¥ì„± ë†’ì€ í•™ìŠµ íŒŒì´í”„ë¼ì¸ (ì—í¬í¬ ìºì‹œ ì‹œìŠ¤í…œ í†µí•©)
"""
import gc
import torch
import torch.nn as nn
from torch.cuda.amp import GradScaler, autocast
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import wandb
from typing import Dict, Any, Optional, List
import numpy as np
from pathlib import Path

from .epoch_cache import EpochCache
from typing import Dict, Any
import wandb
from torch.optim import AdamW
import random
import numpy as np
import sys
from typing import Dict, List, Optional, Any, Tuple, List, Tuple, Any, Union, Dict  
import os
from dataclasses import dataclass, asdict
# NEW: ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "quantumcommon"))
from gates import QuantumGateRegistry

import time
# wandb ì„ íƒì ìœ¼ë¡œ ì„í¬íŠ¸
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not installed. Logging will be disabled.")
    
# ê²½ë¡œ ì„¤ì • ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# ì ˆëŒ€ ê²½ë¡œ ì„í¬íŠ¸
try:
    from models.decision_transformer import DecisionTransformer
    from data.embedding_pipeline import EmbeddingPipeline, EmbeddingConfig
    from data.quantum_circuit_dataset import CircuitSpec, CircuitData
except ImportError:
    # ìƒëŒ€ ê²½ë¡œ ì„í¬íŠ¸ ì‹œë„
    from ..models.decision_transformer import DecisionTransformer
    from ..data.embedding_pipeline import EmbeddingPipeline, EmbeddingConfig
    from ..data.quantum_circuit_dataset import CircuitSpec


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì •"""
    # ëª¨ë¸ ì„¤ì •
    d_model: int = 512
    n_layers: int = 6
    n_heads: int = 8
    n_gate_types: int = None  # ğŸ† NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •
    dropout: float = 0.1
    attention_mode: str = "standard"  # "standard", "advanced", "hybrid"
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ gate ìˆ˜ë¥¼ ì‹±ê¸€í†¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        if self.n_gate_types is None:
            self.n_gate_types = QuantumGateRegistry.get_singleton_gate_count()
            print(f"ğŸ† TrainingConfig: Using gate vocab singleton, n_gate_types = {self.n_gate_types}")
    
    # í•™ìŠµ ì„¤ì •
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    batch_size: int = 32
    num_epochs: int = 1
    warmup_steps: int = 1000
    
    # ê²€ì¦ ì„¤ì •
    eval_every: int = 500
    save_every: int = 1000
    
    # ê¸°íƒ€
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42
    
    # ë¡œê¹…
    use_wandb: bool = True
    project_name: str = "quantum-decision-transformer"
    run_name: Optional[str] = None
    
    # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    use_amp: bool = True
    gradient_accumulation_steps: int = 1
    gradient_checkpointing: bool = True
    memory_cleanup_interval: int = 50
    
    # ë°ì´í„°ì…‹ ë¶„í•  ì„¤ì •
    train_ratio: float = 0.7
    val_ratio: float = 0.15
    test_ratio: float = 0.15
    
    # ê¸°íƒ€ ì„¤ì •
    enable_filtering: bool = True
    save_dir: str = "./OAT_Model/checkpoints"


def dict_to_config(config_dict: dict) -> TrainingConfig:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ TrainingConfig í´ë˜ìŠ¤ë¡œ ë³€í™˜"""
    # TrainingConfigì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘
    config = TrainingConfig()
    
    # ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ë¡œ ì—…ë°ì´íŠ¸
    for key, value in config_dict.items():
        if hasattr(config, key):
            setattr(config, key, value)
    
    return config


class QuantumCircuitCollator:
    """ğŸš€ ë°°ì¹˜ ì½œë ˆì´í„° - CircuitDataë¥¼ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (ìºì‹± ìµœì í™”)"""
    
    def __init__(self, embedding_pipeline: EmbeddingPipeline):
        self.embedding_pipeline = embedding_pipeline
        self._batch_count = 0
        self._total_circuits = 0
    
    def __call__(self, batch: List['CircuitData']) -> Dict[str, torch.Tensor]:
        """ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ (ìºì‹± ìµœì í™”)"""
        
        self._batch_count += 1
        self._total_circuits += len(batch)
        
        # CircuitDataì—ì„œ CircuitSpec ì¶”ì¶œ
        circuit_specs = [circuit_data.circuit_spec for circuit_data in batch]
        
        # ì¸¡ì • ê²°ê³¼ ì •ë³´ ì¶”ê°€ (íƒ€ê²Ÿ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‚¬ìš©)
        target_metrics = []
        for circuit_data in batch:
            result = circuit_data.measurement_result
            metrics = {
                'fidelity': result.fidelity,
                'entanglement': result.entanglement,
                'robust_fidelity': result.robust_fidelity or 0.0,
            }
            
            # Expressibility ì •ë³´ ì¶”ê°€
            if result.expressibility:
                expr = result.expressibility
                metrics.update({
                    'expressibility': expr.get('expressibility', 0.0),
                    'kl_divergence': expr.get('kl_divergence', 0.0),
                })
            else:
                metrics.update({
                    'expressibility': 0.0,
                    'kl_divergence': 0.0,
                })
            
            target_metrics.append(metrics)

        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ë°°ì¹˜ ì²˜ë¦¬ (ìºì‹± ìë™ ì ìš©)
        embedded_batch = self.embedding_pipeline.process_batch(circuit_specs)
        
        
        # íƒ€ê²Ÿ ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ê°€
        if embedded_batch:
            embedded_batch['target_metrics'] = target_metrics
        
        if not embedded_batch:
            return {}
        
        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì—ì„œ ì´ë¯¸ í†µí•© ì•¡ì…˜ íƒ€ê²Ÿì´ ìƒì„±ë¨
        # ì¶”ê°€ ì²˜ë¦¬ ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìºì‹± ìµœì í™” ì™„ë£Œ)
        return embedded_batch
    
    
    def get_stats(self) -> Dict[str, int]:
        """ ì½œë ˆì´í„° í†µê³„ ë°˜í™˜"""
        return {
            'total_batches': self._batch_count,
            'total_circuits': self._total_circuits,
            'avg_batch_size': self._total_circuits / max(self._batch_count, 1)
        }
            
    
    # ë³µì¡í•œ ì•¡ì…˜ ìƒì„± ë©”ì„œë“œë“¤ ì œê±°ë¨ - ì´ì œ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì—ì„œ í†µí•© ì²˜ë¦¬ (ìºì‹± ìµœì í™”)


class DecisionTransformerTrainer:
    """Decision Transformer íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, model, train_dataloader, val_dataloader, config: TrainingConfig, embedding_pipeline):
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.config = config
        self.embedding_pipeline = embedding_pipeline
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì—í¬í¬ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.epoch_cache = EpochCache(cache_dir="cache/epochs", max_cache_size_gb=2.0)
        
        # í•™ìŠµ ì§„í–‰ ìƒíƒœ ì¶”ì 
        self.global_step = 0
        
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.model.to(self.device)
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        self.use_amp = self.config.use_amp and self.device.type == 'cuda'
        self.gradient_accumulation_steps = self.config.gradient_accumulation_steps
        self.gradient_checkpointing = self.config.gradient_checkpointing
        
        # Mixed Precision ìŠ¤ì¼€ì¼ëŸ¬
        if self.use_amp:
            self.scaler = GradScaler()
        else:
            # Dummy scaler for code consistency when not using AMP
            self.scaler = None
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        if self.gradient_checkpointing and hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            eps=1e-6  # ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì›œì—… + ì½”ì‚¬ì¸ ì–´ë‹ë§)
        total_steps = len(self.train_dataloader) * self.config.num_epochs
        warmup_steps = total_steps // 10  # ì „ì²´ ìŠ¤í…ì˜ 10%ë¥¼ ì›œì—…ìœ¼ë¡œ ì‚¬ìš©
        
        from torch.optim.lr_scheduler import LinearLR, SequentialLR
        
        # ì›œì—… ìŠ¤ì¼€ì¤„ëŸ¬ (0ì—ì„œ target_lrê¹Œì§€ ì„ í˜• ì¦ê°€)
        warmup_scheduler = LinearLR(
            self.optimizer,
            start_factor=0.1,  # ì‹œì‘ í•™ìŠµë¥  = target_lr * 0.1
            end_factor=1.0,    # ë í•™ìŠµë¥  = target_lr * 1.0
            total_iters=warmup_steps
        )
        
        # ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬ (ì›œì—… í›„ ì ìš©)
        cosine_scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps - warmup_steps,
            eta_min=self.config.learning_rate * 0.01  # ìµœì†Œ í•™ìŠµë¥  = target_lr * 0.01
        )
        
        # ìˆœì°¨ì  ìŠ¤ì¼€ì¤„ëŸ¬ (ì›œì—… â†’ ì½”ì‚¬ì¸)
        self.scheduler = SequentialLR(
            self.optimizer,
            schedulers=[warmup_scheduler, cosine_scheduler],
            milestones=[warmup_steps]
        )
        
        # ğŸ¯ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •: ëª¨ë¸ì˜ compute_loss ë©”ì„œë“œ ì‚¬ìš©
        # ì˜ˆì¸¡ê³¼ ì†ì‹¤ ê³„ì‚°ì„ ë¶„ë¦¬í•˜ì—¬ ê¹”ë”í•œ êµ¬ì¡° ìœ ì§€
        self.loss_fn = self.model.compute_loss
        
        # ë¡œê¹… ì„¤ì •
        self.use_wandb = self.config.use_wandb
        if self.use_wandb and WANDB_AVAILABLE:
            wandb.init(
                project=self.config.project_name,
                name=self.config.run_name,
                config=asdict(self.config)
            )
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.save_dir = Path(self.config.save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ìµœê³  ì„±ëŠ¥ ì¶”ì 
        self.best_val_loss = float('inf')
        self.best_model_path = None
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ ì£¼ê¸°
        self.memory_cleanup_interval = self.config.memory_cleanup_interval

    def train_epoch(self):
        """í•œ ì—í¬í¬ í•™ìŠµ (ìºì‹œ ì‹œìŠ¤í…œ ì ìš©)"""
        self.model.train()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        # í˜„ì¬ ì—í¬í¬ í™•ì¸ (ì „ì—­ ìŠ¤í…ìœ¼ë¡œë¶€í„° ì¶”ì •)
        current_epoch = getattr(self, 'current_epoch', 0)
        
        # ìºì‹œëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        cached_batches = None
        if current_epoch > 0:  # ì²« ë²ˆì§¸ ì—í¬í¬ê°€ ì•„ë‹Œ ê²½ìš°
            cached_batches = self.epoch_cache.load_epoch_data(self.train_dataloader, 0)  # ì²« ë²ˆì§¸ ì—í¬í¬ ë°ì´í„° ì¬ì‚¬ìš©
        
        if cached_batches is not None:
            # ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
            print(f"[CACHE] ìºì‹œëœ ë°ì´í„° ì‚¬ìš© ì¤‘... ({len(cached_batches)} ë°°ì¹˜)")
            pbar = tqdm(cached_batches, desc="Training (Cached)")
            
            for batch_idx, batch in enumerate(pbar):
                # ìºì‹œëœ ë°°ì¹˜ëŠ” ì´ë¯¸ ì²˜ë¦¬ëœ ìƒíƒœì´ë¯€ë¡œ ë°”ë¡œ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©
                loss, accuracy = self._train_single_batch_cached(batch, batch_idx)
                
                total_loss += loss
                total_accuracy += accuracy
                num_batches += 1
                
                # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
                pbar.set_postfix({
                    'loss': f'{loss:.4f}',
                    'acc': f'{accuracy:.4f}',
                    'lr': f'{self.scheduler.get_last_lr()[0]:.2e}'
                })
        else:
            # ì²« ë²ˆì§¸ ì—í¬í¬ ë˜ëŠ” ìºì‹œ ì—†ìŒ - ì •ìƒ ì²˜ë¦¬ ë° ìºì‹œ ì €ì¥
            processed_batches = []
            pbar = tqdm(self.train_dataloader, desc="Training")
            
            # ê° ë°°ì¹˜ ì²˜ë¦¬
            for batch_idx, batch in enumerate(pbar):
                # ìˆœì „íŒŒ
                self.optimizer.zero_grad()
                
                # ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                input_sequence = batch['input_sequence'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                action_prediction_mask = batch['action_prediction_mask'].to(self.device)
                
                # ì–´í…ì…˜ ëª¨ë“œì— ë”°ë¼ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì „ë‹¬
                model_kwargs = {
                    'input_sequence': input_sequence,
                    'attention_mask': attention_mask,
                    'action_prediction_mask': action_prediction_mask
                }
                
                # ê³ ê¸‰ ì–´í…ì…˜ ëª¨ë“œì¸ ê²½ìš° grid_structureì™€ edges ì „ë‹¬
                if hasattr(self.model, 'get_attention_mode') and self.model.get_attention_mode() in ['advanced', 'hybrid']:
                    # ë°°ì¹˜ì—ì„œ grid_structureì™€ edges ì •ë³´ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°) ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    if 'grid_structure' in batch:
                        model_kwargs['grid_structure'] = batch['grid_structure'].to(self.device)
                    if 'edges' in batch:
                        model_kwargs['edges'] = batch['edges'].to(self.device)
                    if 'circuit_constraints' in batch:
                        model_kwargs['circuit_constraints'] = batch['circuit_constraints'].to(self.device)
            
                # ìˆœì „íŒŒ ë° ì†ì‹¤ ê³„ì‚°
                # ê³µí†µ ì „ì²˜ë¦¬ - ëª¨ë“  í…ì„œë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                squeezed_action_mask = action_prediction_mask.squeeze(1)  # ì´ë¯¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ëœ action_prediction_mask ì‚¬ìš©
                
                # íƒ€ê²Ÿ ì•¡ì…˜ ì²˜ë¦¬
                if len(batch['target_actions'].shape) == 3:
                    squeezed_target_actions = batch['target_actions'].squeeze(1).to(self.device)
                else:
                    squeezed_target_actions = batch['target_actions'].to(self.device)
                
                # íƒ€ê²Ÿ íë¹— ë° íŒŒë¼ë¯¸í„° ì²˜ë¦¬
                if 'target_qubits' in batch and torch.is_tensor(batch['target_qubits']):
                    target_qubits = batch['target_qubits'].to(self.device)
                else:
                    target_qubits = batch.get('target_qubits', [])
                    
                if 'target_params' in batch and torch.is_tensor(batch['target_params']):
                    target_params = batch['target_params'].to(self.device)
                else:
                    target_params = batch.get('target_params', [])
                
                # íƒ€ê²Ÿ ë°ì´í„° ì¤€ë¹„
                targets = {
                    'gate_targets': squeezed_target_actions,
                    'qubit_targets': target_qubits,
                    'parameter_targets': target_params
                }
                
                # Mixed Precisionìœ¼ë¡œ forward pass
                # Forward pass and loss calculation with or without AMP
                if self.use_amp:
                    with autocast('cuda', enabled=True):
                        outputs = self.model(**model_kwargs)
                        # ì†ì‹¤ ê³„ì‚°
                        loss_outputs = self.loss_fn(
                            outputs, 
                            targets, 
                            squeezed_action_mask,
                            num_qubits=batch.get('num_qubits', None),
                            num_gates=batch.get('num_gates', None)
                        )
                        loss = loss_outputs['loss']
                        accuracy = loss_outputs.get('gate_accuracy', 0.0)
                    
                    # ìŠ¤ì¼€ì¼ë§ëœ ì†ì‹¤ë¡œ ì—­ì „íŒŒ
                    self.scaler.scale(loss).backward()
                else:
                    outputs = self.model(**model_kwargs)
                    loss_outputs = self.loss_fn(
                        outputs,
                        targets,
                        squeezed_action_mask,
                        num_qubits=batch.get('num_qubits', None),
                        num_gates=batch.get('num_gates', None)
                    )
                    loss = loss_outputs['loss']
                    accuracy = loss_outputs.get('gate_accuracy', 0.0)
                    
                    # Regular backward pass
                    loss.backward()
                
                # Optimizer step handling
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    if self.use_amp:
                        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                        self.scaler.unscale_(self.optimizer)
                        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        
                        # ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í†µí•œ ì—…ë°ì´íŠ¸
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        # Regular gradient clipping and optimizer step
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        self.optimizer.step()
                    
                    self.scheduler.step()
                    self.optimizer.zero_grad()
            
                # í†µê³„ ì—…ë°ì´íŠ¸
                total_loss += loss.item()
                total_accuracy += accuracy
                num_batches += 1
                
                # ì„¸ë¶€ ì†ì‹¤ ë¡œê¹… (WandB)
                if self.config.use_wandb and batch_idx % 10 == 0:  # 10ë°°ì¹˜ë§ˆë‹¤
                    detailed_metrics = {
                        'train/batch_loss': loss.item(),
                        'train/batch_accuracy': accuracy,
                        'train/learning_rate': self.scheduler.get_last_lr()[0],
                        'global_step': self.global_step
                    }
                    
                    # ì„¸ë¶€ ì†ì‹¤ ë¶„í•´ (ê°€ëŠ¥í•œ ê²½ìš°)
                    if 'gate_loss' in loss_outputs:
                        detailed_metrics['train/gate_loss'] = loss_outputs['gate_loss'].item()
                    if 'position_loss' in loss_outputs:
                        detailed_metrics['train/position_loss'] = loss_outputs['position_loss'].item()
                    if 'parameter_loss' in loss_outputs:
                        detailed_metrics['train/parameter_loss'] = loss_outputs['parameter_loss'].item()
                    
                    # F1, Precision, Recall ë©”íŠ¸ë¦­ ì¶”ê°€
                    if 'gate_precision' in loss_outputs:
                        detailed_metrics['train/gate_precision'] = loss_outputs['gate_precision']
                    if 'gate_recall' in loss_outputs:
                        detailed_metrics['train/gate_recall'] = loss_outputs['gate_recall']
                    if 'gate_f1' in loss_outputs:
                        detailed_metrics['train/gate_f1'] = loss_outputs['gate_f1']
                    
                    wandb.log(detailed_metrics)
                
                # ì—í¬í¬ ìºì‹œì— ë°°ì¹˜ ì €ì¥
                processed_batches.append(batch)
                
                # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
                pbar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'acc': f"{accuracy:.4f}",
                    'lr': f"{self.scheduler.get_last_lr()[0]:.2e}"
                })
                
                # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
                if batch_idx % self.memory_cleanup_interval == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    gc.collect()
            
            # ì²« ë²ˆì§¸ ì—í¬í¬ì¸ ê²½ìš°, ìºì‹œ ì €ì¥
            if current_epoch == 0 and processed_batches:
                print(f"\n[CACHE] ì²«ë²ˆì§¸ ì—í¬í¬ ë°ì´í„° ìºì‹œ ì €ì¥ ì¤‘... ({len(processed_batches)} ë°°ì¹˜)")
                self.epoch_cache.save_epoch_data(self.train_dataloader, 0, processed_batches)
        
        # ì—í¬í¬ í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_metrics = {
            'loss': total_loss / max(num_batches, 1),
            'accuracy': total_accuracy / max(num_batches, 1)
        }
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì„¸ë¶€ ë©”íŠ¸ë¦­ ì¶”ê°€ (loss_outputsì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        if 'loss_outputs' in locals():
            if 'gate_loss' in loss_outputs:
                avg_metrics['gate_loss'] = loss_outputs['gate_loss'].item()
            if 'position_loss' in loss_outputs:
                avg_metrics['position_loss'] = loss_outputs['position_loss'].item()
            if 'parameter_loss' in loss_outputs:
                avg_metrics['parameter_loss'] = loss_outputs['parameter_loss'].item()
            if 'gate_precision' in loss_outputs:
                avg_metrics['precision'] = loss_outputs['gate_precision']
            if 'gate_recall' in loss_outputs:
                avg_metrics['recall'] = loss_outputs['gate_recall']
            if 'gate_f1' in loss_outputs:
                avg_metrics['f1'] = loss_outputs['gate_f1']
        
        return avg_metrics
    
    def validate_epoch(self) -> Dict[str, float]:
        """ê²€ì¦ ë‹¨ê³„ - 4ê°€ì§€ ì •í™•ë„ ë©”íŠ¸ë¦­ê³¼ 3ê°€ì§€ ì†ì‹¤ ë©”íŠ¸ë¦­"""
        self.model.eval()
        total_loss = 0.0
        total_accuracy = 0.0
        total_gate_loss = 0.0
        total_position_loss = 0.0
        total_parameter_loss = 0.0
        total_precision = 0.0
        total_recall = 0.0
        total_f1 = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_dataloader, desc="Validation"):
                if not batch:
                    continue
                
                batch = self._move_batch_to_device(batch)
                
                # ëª¨ë¸ ì˜ˆì¸¡
                model_kwargs = {
                    'input_sequence': batch['input_sequence'],
                    'attention_mask': batch['attention_mask'],
                    'action_prediction_mask': batch['action_prediction_mask']
                }
                
                outputs = self.model(**model_kwargs)
                
                # íƒ€ê²Ÿ ì¤€ë¹„ (í›ˆë ¨ê³¼ ë™ì¼í•œ ë°©ì‹)
                squeezed_action_mask = batch['action_prediction_mask']
                squeezed_target_actions = batch['target_actions']
                
                # íƒ€ê²Ÿ íë¹— ë° íŒŒë¼ë¯¸í„° ì²˜ë¦¬ (í›ˆë ¨ê³¼ ë™ì¼í•˜ê²Œ)
                if 'target_qubits' in batch and torch.is_tensor(batch['target_qubits']):
                    target_qubits = batch['target_qubits'].to(self.device)
                else:
                    target_qubits = batch.get('target_qubits', [])
                    
                if 'target_params' in batch and torch.is_tensor(batch['target_params']):
                    target_params = batch['target_params'].to(self.device)
                else:
                    target_params = batch.get('target_params', [])
                
                # ì°¨ì› ìˆ˜ì •
                if len(squeezed_action_mask.shape) == 3 and squeezed_action_mask.shape[1] == 1:
                    squeezed_action_mask = squeezed_action_mask.squeeze(1)
                if len(squeezed_target_actions.shape) == 3 and squeezed_target_actions.shape[1] == 1:
                    squeezed_target_actions = squeezed_target_actions.squeeze(1)
                
                # íƒ€ê²Ÿ êµ¬ì¡° ì¤€ë¹„ (í›ˆë ¨ê³¼ ë™ì¼í•œ êµ¬ì¡°)
                targets_dict = {
                    'gate_targets': squeezed_target_actions,
                    'qubit_targets': target_qubits,  # position_lossì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤
                    'parameter_targets': target_params,
                    'target_actions': squeezed_target_actions,
                    'target_qubits': target_qubits,
                    'target_params': target_params,
                    'action_targets': batch.get('action_targets', {})
                }
                
                # ì†ì‹¤ ê³„ì‚°
                loss_outputs = self.loss_fn(
                    predictions=outputs,
                    targets=targets_dict,
                    action_prediction_mask=squeezed_action_mask,
                    num_qubits=batch.get('num_qubits', None),
                    num_gates=batch.get('num_gates', None)  # ğŸ”§ ê²€ì¦ì—ì„œë„ ê²Œì´íŠ¸ ìˆ˜ ì •ë³´ ì¶”ê°€
                )
                
                # ë©”íŠ¸ë¦­ ëˆ„ì 
                total_loss += loss_outputs['loss'].item()
                total_accuracy += loss_outputs.get('gate_accuracy', 0.0)
                
                # ì„¸ë¶€ ì†ì‹¤ ëˆ„ì 
                if 'gate_loss' in loss_outputs:
                    total_gate_loss += loss_outputs['gate_loss'].item()
                if 'position_loss' in loss_outputs:
                    total_position_loss += loss_outputs['position_loss'].item()
                if 'parameter_loss' in loss_outputs:
                    total_parameter_loss += loss_outputs['parameter_loss'].item()
                
                # ë¶„ë¥˜ ë©”íŠ¸ë¦­ ëˆ„ì 
                total_precision += loss_outputs.get('gate_precision', 0.0)
                total_recall += loss_outputs.get('gate_recall', 0.0)
                total_f1 += loss_outputs.get('gate_f1', 0.0)
                
                num_batches += 1
        
        val_metrics = {
            'val_loss': total_loss / max(num_batches, 1),
            'val_accuracy': total_accuracy / max(num_batches, 1),
            'val_gate_loss': total_gate_loss / max(num_batches, 1),
            'val_position_loss': total_position_loss / max(num_batches, 1),
            'val_parameter_loss': total_parameter_loss / max(num_batches, 1),
            'val_precision': total_precision / max(num_batches, 1),
            'val_recall': total_recall / max(num_batches, 1),
            'val_f1': total_f1 / max(num_batches, 1)
        }
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_metrics['val_loss'] < self.best_val_loss:
            self.best_val_loss = val_metrics['val_loss']
            self.save_checkpoint(is_best=True)
        
        # ë¡œê¹…
        if self.config.use_wandb:
            wandb.log({**{f'val/{k}': v for k, v in val_metrics.items()}, 'global_step': self.global_step})
        
        print(f"Validation - Loss: {val_metrics['val_loss']:.4f}, Accuracy: {val_metrics['val_accuracy']:.4f}")
        
        return val_metrics
    
    def _move_batch_to_device(self, batch):
        """ë°°ì¹˜ ë°ì´í„°ë¥¼ í˜„ì¬ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        result = {}
        for k, v in batch.items():
            if torch.is_tensor(v):
                result[k] = v.to(self.device)
            else:
                result[k] = v
        return result
    
    def train(self):
        """ì „ì²´ í•™ìŠµ ë£¨í”„"""
        print(f"Starting training for {self.config.num_epochs} epochs...")
        print(f"Device: {self.device}")
        print(f"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # í•™ìŠµ ì‹œì‘ ì¤€ë¹„ - ìºì‹œëŠ” ì´ë¯¸ epoch_cache ì´ˆê¸°í™” ì‹œ ì •ë¦¬ë¨
        
        for epoch in range(self.config.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.config.num_epochs}")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch()
            
            # ê²€ì¦
            val_metrics = self.validate_epoch()
            
            # ì—í¬í¬ ë¡œê¹… - 4ê°€ì§€ ì •í™•ë„ ë©”íŠ¸ë¦­ê³¼ 3ê°€ì§€ ì†ì‹¤ ë©”íŠ¸ë¦­
            print(f"\n[STATS] Epoch {epoch + 1}/{self.config.num_epochs} Results:")
            print(f"[TRAIN] Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}")
            if 'gate_loss' in train_metrics:
                print(f"         Gate: {train_metrics.get('gate_loss', 0):.4f}, Pos: {train_metrics.get('position_loss', 0):.4f}, Param: {train_metrics.get('parameter_loss', 0):.4f}")
            if 'precision' in train_metrics:
                print(f"         Prec: {train_metrics.get('precision', 0):.4f}, Rec: {train_metrics.get('recall', 0):.4f}, F1: {train_metrics.get('f1', 0):.4f}")
            
            print(f"[VAL]   Loss: {val_metrics['val_loss']:.4f}, Acc: {val_metrics['val_accuracy']:.4f}")
            print(f"         Gate: {val_metrics['val_gate_loss']:.4f}, Pos: {val_metrics['val_position_loss']:.4f}, Param: {val_metrics['val_parameter_loss']:.4f}")
            print(f"         Prec: {val_metrics['val_precision']:.4f}, Rec: {val_metrics['val_recall']:.4f}, F1: {val_metrics['val_f1']:.4f}")
            
            if self.config.use_wandb:
                wandb_metrics = {
                    'epoch': epoch + 1,
                    'train/epoch_loss': train_metrics['loss'],
                    'train/epoch_accuracy': train_metrics['accuracy'],
                    'val/epoch_loss': val_metrics['val_loss'],
                    'val/epoch_accuracy': val_metrics['val_accuracy'],
                    'val/epoch_gate_loss': val_metrics['val_gate_loss'],
                    'val/epoch_position_loss': val_metrics['val_position_loss'],
                    'val/epoch_parameter_loss': val_metrics['val_parameter_loss'],
                    'val/epoch_precision': val_metrics['val_precision'],
                    'val/epoch_recall': val_metrics['val_recall'],
                    'val/epoch_f1': val_metrics['val_f1']
                }
                
                # í›ˆë ¨ ì„¸ë¶€ ë©”íŠ¸ë¦­ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
                if 'gate_loss' in train_metrics:
                    wandb_metrics.update({
                        'train/epoch_gate_loss': train_metrics['gate_loss'],
                        'train/epoch_position_loss': train_metrics.get('position_loss', 0),
                        'train/epoch_parameter_loss': train_metrics.get('parameter_loss', 0),
                        'train/epoch_precision': train_metrics.get('precision', 0),
                        'train/epoch_recall': train_metrics.get('recall', 0),
                        'train/epoch_f1': train_metrics.get('f1', 0)
                    })
                
                wandb.log(wandb_metrics)
        
        print("Training completed!")
        
        self._debug_last_batch_analysis()
    
    def _debug_last_batch_analysis(self):
        """ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ìƒ˜í”Œ ë¶„ì„ ë””ë²„ê·¸ ì¶œë ¥"""
        # ê²€ì¦ ë°ì´í„°ì—ì„œ ì²« ë²ˆì§¸ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
        if not hasattr(self, 'val_dataloader') or len(self.val_dataloader) == 0:
            print("No validation data available for debug analysis.")
            return
        
        try:
            # ì²« ë²ˆì§¸ ë°°ì¹˜ì™€ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
            batch = next(iter(self.val_dataloader))
            batch_idx = 0
            sample_idx = 0
            
            # ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
            batch = self._move_batch_to_device(batch)
            
            # ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
            model_kwargs = {
            'input_sequence': batch['input_sequence'],
            'attention_mask': batch['attention_mask'],
            'action_prediction_mask': batch['action_prediction_mask']
        }
        
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(**model_kwargs)
            
            print(f"\n ===== ë°°ì¹˜ {batch_idx} ìƒ˜í”Œ {sample_idx} ì‹œí€€ìŠ¤ ë¶„ì„ =====\n")
            
            # ê¸°ë³¸ ì •ë³´
            input_seq = batch['input_sequence'][sample_idx]  # [seq_len, d_model]
            action_mask = batch['action_prediction_mask'][sample_idx]  # [seq_len] or [1, seq_len]
            if len(action_mask.shape) > 1:
                action_mask = action_mask.squeeze(0)
            
            # ì•¡ì…˜ ì˜ˆì¸¡ ìœ„ì¹˜ ì°¾ê¸°
            action_positions = torch.where(action_mask > 0)[0]
            num_actions = len(action_positions)
            
            print(f"ğŸ¯ ê¸°ë³¸ ì •ë³´:")
            print(f"   - ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´: {input_seq.shape[0]}")
            print(f"   - ì•¡ì…˜ ì˜ˆì¸¡ ìœ„ì¹˜ ìˆ˜: {num_actions}")
            print(f"   - ì•¡ì…˜ ìœ„ì¹˜ë“¤: {action_positions.tolist()[:10]}{'...' if num_actions > 10 else ''}")
            
            # ëª¨ë¸ ì˜ˆì¸¡ ì¶”ì¶œ
            gate_logits = outputs['gate_logits'][sample_idx]  # [num_actions, num_gate_types]
            position_preds = outputs['position_preds'][sample_idx]  # [num_actions, max_qubits, num_positions]
            parameter_preds = outputs['parameter_preds'][sample_idx]  # [num_actions]
            
            # ì˜ˆì¸¡ëœ ê²Œì´íŠ¸ íƒ€ì… (ìµœê³  í™•ë¥ )
            predicted_gates = torch.argmax(gate_logits, dim=-1)  # [num_actions]
            
            #  ì •ë‹µ ë ˆì´ë¸” ì¶”ì¶œ (ì•ˆì „í•œ í…ì„œ ì ‘ê·¼)
            target_actions = batch.get('target_actions', torch.tensor([]))
            target_qubits = batch.get('target_qubits', [])
            target_params = batch.get('target_params', [])
            
            # [DEBUG] í…ì„œ ì°¨ì› ë””ë²„ê¹…
            print(f"[DEBUG] íƒ€ê²Ÿ í…ì„œ ì°¨ì› ë¶„ì„:")
            print(f"   - target_actions.shape: {target_actions.shape if hasattr(target_actions, 'shape') else 'N/A'}")
            print(f"   - target_qubits type: {type(target_qubits)}, len: {len(target_qubits) if hasattr(target_qubits, '__len__') else 'N/A'}")
            print(f"   - target_params type: {type(target_params)}, len: {len(target_params) if hasattr(target_params, '__len__') else 'N/A'}")
            
            # ğŸš€ ì•ˆì „í•œ íƒ€ê²Ÿ ê²Œì´íŠ¸ ì¶”ì¶œ
            if hasattr(target_actions, 'shape'):
                if len(target_actions.shape) == 3:  # [32, 1, 164] í˜•íƒœ
                    if target_actions.shape[0] > sample_idx:
                        target_gates = target_actions[sample_idx, 0]  # [164] - ì¤‘ê°„ ì°¨ì› ì œê±°
                    else:
                        target_gates = torch.tensor([])
                elif len(target_actions.shape) == 2:  # [32, 164] í˜•íƒœ
                    if target_actions.shape[0] > sample_idx:
                        target_gates = target_actions[sample_idx]  # [164]
                    else:
                        target_gates = torch.tensor([])
                elif len(target_actions.shape) == 1:  # [164] í˜•íƒœ
                    target_gates = target_actions  # ì´ë¯¸ 1D í…ì„œ
                else:
                    target_gates = torch.tensor([])
            else:
                target_gates = torch.tensor([])
            
            print(f"   - target_gates.shape: {target_gates.shape if hasattr(target_gates, 'shape') else 'N/A'}")
            
            print(f"\n ëª¨ë¸ ì˜ˆì¸¡ vs ì •ë‹µ (ì²˜ìŒ 5ê°œ ì•¡ì…˜):")
            
            # ê²Œì´íŠ¸ ì˜ˆì¸¡ ë¡œì§“ ì¶”ì¶œ (ìƒ˜í”Œë‹¹)
            sample_gate_logits = outputs['gate_logits'][sample_idx]  # [seq_len, num_gate_types]
            
            for i in range(min(5, num_actions)):
                action_pos = action_positions[i].item()
                pred_gate = predicted_gates[i].item()
                
                # ê²Œì´íŠ¸ ë¡œì§“ ë¶„ì„
                gate_logits_at_pos = gate_logits[i]
                top3_gate_values, top3_gate_indices = torch.topk(gate_logits_at_pos, k=min(3, gate_logits_at_pos.size(-1)))
                top3_gate_probs = torch.softmax(top3_gate_values, dim=-1)
                gate_confidence = top3_gate_probs[0].item()
                
                # ì•ˆì „í•œ ì •ë‹µ ê²Œì´íŠ¸ ì¶”ì¶œ
                if hasattr(target_gates, 'shape') and i < target_gates.shape[0]:
                    if target_gates.dim() == 0:  # ìŠ¤ì¹¼ë¼ í…ì„œ
                        true_gate = int(target_gates.item())
                    elif target_gates.dim() == 1 and i < target_gates.size(0):  # 1D í…ì„œ
                        true_gate = int(target_gates[i].item())
                    else:  # ë‹¤ì°¨ì› í…ì„œ
                        if target_gates.dim() > 1 and target_gates.size(0) > i:
                            true_gate = int(target_gates[i].item() if target_gates[i].dim() == 0 else target_gates[i][0].item())
                        else:
                            true_gate = "N/A"
                else:
                    true_gate = "N/A"
                
                # ìœ„ì¹˜ ì˜ˆì¸¡ (ìµœì í™” ì˜ˆì¸¡ ë°©ì‹)
                position_logits_at_pos = position_preds[i]  # [max_qubits, 2] or similar
                
                # ê²Œì´íŠ¸ íƒ€ì…ì— ë”°ë¼ í•„ìš”í•œ íë¹— ìˆ˜ ê²°ì • (ì˜ˆ: 1íë¹— ë˜ëŠ” 2íë¹— ê²Œì´íŠ¸)
                # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ í•µì‹¬ ìœ„ì¹˜ 1~2ê°œë§Œ ì¶”ì¶œ
                qubit_positions = []
                
                # ìœ„ì¹˜ ì˜ˆì¸¡ ë…¼ë¦¬
                if len(position_logits_at_pos.shape) >= 2 and position_logits_at_pos.shape[0] > 0:
                    # ì²« ë²ˆì§¸ íë¹— ìœ„ì¹˜ (ëª¨ë“  ê²Œì´íŠ¸ì— í•„ìš”)
                    pos1 = torch.argmax(position_logits_at_pos[0]).item()
                    qubit_positions.append(pos1)
                    
                    # 2íë¹— ê²Œì´íŠ¸ì¸ ê²½ìš° ë‘ ë²ˆì§¸ ìœ„ì¹˜ ì¶”ê°€ 
                    # (ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ëª¨ë“  ê²Œì´íŠ¸ì— ëŒ€í•´ ì²« ë‘ ìœ„ì¹˜ í‘œì‹œ)
                    if position_logits_at_pos.shape[0] > 1:
                        pos2 = torch.argmax(position_logits_at_pos[1]).item()
                        qubit_positions.append(pos2)
                
                # íŒŒë¼ë¯¸í„° ì˜ˆì¸¡
                pred_param = parameter_preds[i].item()
                
                # ğŸš€ ì•ˆì „í•œ ì •ë‹µ ìœ„ì¹˜/íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ìƒ˜í”Œ ë‚´ ê²Œì´íŠ¸ë³„)
                true_positions = []
                if isinstance(target_qubits, torch.Tensor):
                    # í…ì„œ í˜•íƒœ ë¶„ì„
                    if len(target_qubits.shape) == 3:  # [batch, gate, pos]
                        if target_qubits.shape[0] > sample_idx and target_qubits.shape[1] > i:
                            # ëª¨ë“  ìœ íš¨í•œ íë¹— ìœ„ì¹˜ ì¶”ì¶œ
                            for j in range(target_qubits.shape[2]):
                                if j < target_qubits[sample_idx][i].shape[0]:
                                    qpos = target_qubits[sample_idx][i][j].item()
                                    if qpos >= 0:  # ìœ íš¨í•œ íë¹— ìœ„ì¹˜ë§Œ í¬í•¨
                                        true_positions.append(int(qpos))
                    elif len(target_qubits.shape) == 2:  # [batch, pos]
                        if target_qubits.shape[0] > sample_idx:
                            qpos = target_qubits[sample_idx][i].item() if i < target_qubits.shape[1] else -1
                            if qpos >= 0:
                                true_positions.append(int(qpos))
            
            # íŒŒë¼ë¯¸í„° ê°’
            true_param = None
            if isinstance(target_params, torch.Tensor):
                if len(target_params.shape) == 2:  # [batch, gate]
                    if target_params.shape[0] > sample_idx and target_params.shape[1] > i:
                        param_val = target_params[sample_idx][i].item()
                        if not torch.isnan(target_params[sample_idx][i]):
                            true_param = float(param_val)
                elif len(target_params.shape) == 1:  # [batch]
                    if i < target_params.shape[0]:
                        param_val = target_params[i].item()
                        if not torch.isnan(target_params[i]):
                            true_param = float(param_val)
                            
                # ìµœì¢… ì¶œë ¥
                print(f"   [{i}] ì‹œí€€ìŠ¤ ìœ„ì¹˜ {action_pos}:")
                
                # ê²Œì´íŠ¸ ì •ë³´ ì¶œë ¥ (ì˜ˆì¸¡ vs ì‹¤ì œ)
                gate_match = "âœ“" if str(pred_gate) == str(true_gate) else "âœ—"
                print(f"       ê²Œì´íŠ¸: ì˜ˆì¸¡={pred_gate} vs ì •ë‹µ={true_gate} {gate_match}")
                print(f"       ê²Œì´íŠ¸ í™•ë¥ : {gate_confidence:.3f}")
                
                # íë¹— ìœ„ì¹˜ ì¶œë ¥ (ì˜ˆì¸¡ vs ì‹¤ì œ)
                pos_match = "âœ“" if qubit_positions == true_positions else "âœ—"
                print(f"       ìœ„ì¹˜: ì˜ˆì¸¡={qubit_positions} vs ì •ë‹µ={true_positions} {pos_match}")
                
                # íŒŒë¼ë¯¸í„° ì¶œë ¥ (ì˜ˆì¸¡ vs ì‹¤ì œ)
                param_match = ""
                if true_param is not None and isinstance(true_param, (int, float)):
                    param_diff = abs(pred_param - true_param)
                    param_match = "âœ“" if param_diff < 0.1 else "âœ—"
                    print(f"       íŒŒë¼ë¯¸í„°: ì˜ˆì¸¡={pred_param:.4f} vs ì •ë‹µ={true_param:.4f} {param_match}")
                else:
                    print(f"       íŒŒë¼ë¯¸í„°: ì˜ˆì¸¡={pred_param:.4f} vs ì •ë‹µ={true_param}")
            
            print(f"[DEBUG] ===== ë¶„ì„ ì™„ë£Œ =====\n")
            
            # ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½
            self.model.train()
            
        except Exception as e:
            print(f"Debug analysis failed: {e}")
            import traceback
            traceback.print_exc()
            # ëª¨ë¸ ìƒíƒœ ë³µì›
            self.model.train()
    
    def _debug_sequence_details(self, batch, outputs, batch_idx, sample_idx=0):
        """[DEBUG] ë°°ì¹˜ë‹¹ 1ê°œ ìƒ˜í”Œì˜ ìƒì„¸í•œ ì‹œí€€ìŠ¤ ë¶„ì„"""
        print(f"\n[DEBUG] ===== ë°°ì¹˜ {batch_idx} ìƒ˜í”Œ {sample_idx} ì‹œí€€ìŠ¤ ë¶„ì„ =====")
        
        # ê¸°ë³¸ ì •ë³´
        input_seq = batch['input_sequence'][sample_idx]  # [seq_len, d_model]
        action_mask = batch['action_prediction_mask'][sample_idx]  # [seq_len] or [1, seq_len]
        if len(action_mask.shape) > 1:
            action_mask = action_mask.squeeze(0)
        
        # ì•¡ì…˜ ì˜ˆì¸¡ ìœ„ì¹˜ ì°¾ê¸°
        action_positions = torch.where(action_mask > 0)[0]
        num_actions = len(action_positions)
        
        print(f" ê¸°ë³¸ ì •ë³´:")
        print(f"   - ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´: {input_seq.shape[0]}")
        print(f"   - ì•¡ì…˜ ì˜ˆì¸¡ ìœ„ì¹˜ ìˆ˜: {num_actions}")
        print(f"   - ì•¡ì…˜ ìœ„ì¹˜ë“¤: {action_positions.tolist()[:10]}{'...' if num_actions > 10 else ''}")
        
        # ëª¨ë¸ ì˜ˆì¸¡ ì¶”ì¶œ
        gate_logits = outputs['gate_logits'][sample_idx]  # [seq_len, num_gate_types]
        position_preds = outputs.get('position_preds', None)
        parameter_preds = outputs.get('parameter_preds', None)
        
        print(f" ëª¨ë¸ ì¶œë ¥ í…ì„œ ì°¨ì›:")
        print(f"   - gate_logits.shape: {gate_logits.shape if gate_logits is not None else 'None'}")
        print(f"   - position_preds.shape: {position_preds[sample_idx].shape if position_preds is not None else 'None'}")
        print(f"   - parameter_preds.shape: {parameter_preds[sample_idx].shape if parameter_preds is not None else 'None'}")
        
        # ì˜ˆì¸¡ëœ ê²Œì´íŠ¸ íƒ€ì… (ìµœê³  í™•ë¥ )
        if gate_logits is not None:
            predicted_gates = torch.argmax(gate_logits, dim=-1)  # [seq_len]
        else:
            predicted_gates = torch.tensor([])
        
        #  ì •ë‹µ ë ˆì´ë¸” ì¶”ì¶œ (ì•ˆì „í•œ í…ì„œ ì ‘ê·¼)
        target_actions = batch.get('target_actions', torch.tensor([]))
        target_qubits = batch.get('target_qubits', [])
        target_params = batch.get('target_params', [])
        
        # ğŸš€ í…ì„œ ì°¨ì› ë””ë²„ê¹…
        print(f" íƒ€ê²Ÿ í…ì„œ ì°¨ì› ë¶„ì„:")
        print(f"   - target_actions.shape: {target_actions.shape if hasattr(target_actions, 'shape') else 'N/A'}")
        print(f"   - target_qubits type: {type(target_qubits)}, shape: {target_qubits.shape if isinstance(target_qubits, torch.Tensor) else 'N/A'}")
        print(f"   - target_params type: {type(target_params)}, shape: {target_params.shape if isinstance(target_params, torch.Tensor) else 'N/A'}")
        
        # ğŸš€ ì•ˆì „í•œ íƒ€ê²Ÿ ê²Œì´íŠ¸ ì¶”ì¶œ
        if isinstance(target_actions, torch.Tensor):
            if len(target_actions.shape) == 3:  # [batch, 1, seq_len] í˜•íƒœ
                if target_actions.shape[0] > sample_idx:
                    target_gates = target_actions[sample_idx, 0]  # [seq_len] - ì¤‘ê°„ ì°¨ì› ì œê±°
                else:
                    target_gates = torch.tensor([])
            elif len(target_actions.shape) == 2:  # [batch, seq_len] í˜•íƒœ
                if target_actions.shape[0] > sample_idx:
                    target_gates = target_actions[sample_idx]  # [seq_len]
                else:
                    target_gates = torch.tensor([])
            elif len(target_actions.shape) == 1:  # [seq_len] í˜•íƒœ
                target_gates = target_actions  # ì´ë¯¸ 1D í…ì„œ
            else:
                target_gates = torch.tensor([])
        else:
            target_gates = torch.tensor([])
        
        print(f"   - ì¶”ì¶œëœ target_gates.shape: {target_gates.shape if hasattr(target_gates, 'shape') else 'N/A'}")
        
        # ì‹¤ì œ ê°’ ë¶„í¬ ë¶„ì„
        if isinstance(target_gates, torch.Tensor) and target_gates.numel() > 0:
            # ìœ ë‹ˆí¬ ê°’ ì¶”ì¶œ
            unique_targets, target_counts = torch.unique(target_gates, return_counts=True)
            print(f"   - unique targets: {unique_targets}")
            print(f"   - target distribution: {target_counts}")
        
        print(f"\n ëª¨ë¸ ì˜ˆì¸¡ vs ì •ë‹µ (ì²˜ìŒ 5ê°œ ì•¡ì…˜):")
        
        # ìœ íš¨í•œ ìœ„ì¹˜ì—ì„œë§Œ ìƒ˜í”Œë§í•˜ê¸°
        if len(action_positions) == 0:
            print("   [!] ì•¡ì…˜ ìœ„ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
            
        for i in range(min(5, num_actions)):
            action_pos = action_positions[i].item()
            
            # í•´ë‹¹ ìœ„ì¹˜ì˜ ê²Œì´íŠ¸ ë¡œì§“ ì¶”ì¶œ
            if gate_logits is None or action_pos >= gate_logits.shape[0]:
                print(f"   [{i}] ì‹œí€€ìŠ¤ ìœ„ì¹˜ {action_pos}: ê²Œì´íŠ¸ ë¡œì§“ ì—†ìŒ!")
                continue
                
            gate_logits_at_pos = gate_logits[action_pos]
            pred_gate = predicted_gates[action_pos].item()
            
            # ê²Œì´íŠ¸ ë¡œì§“ ë¶„ì„
            top3_gate_values, top3_gate_indices = torch.topk(gate_logits_at_pos, k=min(3, gate_logits_at_pos.size(-1)))
            top3_gate_probs = torch.softmax(top3_gate_values, dim=-1)
            gate_confidence = top3_gate_probs[0].item()
            
            # ì •ë‹µ ê°’ ì¶”ì¶œ - ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            true_gate = "N/A"
            if isinstance(target_gates, torch.Tensor) and target_gates.numel() > 0:
                if action_pos < target_gates.shape[0]:
                    true_gate = int(target_gates[action_pos].item())
            
            # ìœ„ì¹˜ ì˜ˆì¸¡ (ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ì¶œ)
            position_info = "N/A"
            if position_preds is not None and sample_idx < position_preds.shape[0]:
                if action_pos < position_preds[sample_idx].shape[0]:
                    pos_pred = position_preds[sample_idx][action_pos]
                    # ìœ„ì¹˜ ì˜ˆì¸¡ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬ (2D ë˜ëŠ” 3D)
                    if pos_pred.dim() == 1:
                        position_info = f"{pos_pred.tolist()}"
                    elif pos_pred.dim() == 2:
                        # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ìœ„ì¹˜ ì¶”ì¶œ
                        if pos_pred.shape[1] >= 2:  # [qubit_idx, pos]
                            positions = []
                            for q_idx in range(min(2, pos_pred.shape[0])):
                                if pos_pred[q_idx].numel() > 0:
                                    pos = torch.argmax(pos_pred[q_idx]).item()
                                    positions.append(pos)
                            position_info = f"{positions}"
            
            # íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ (ìˆëŠ” ê²½ìš°ì—ë§Œ)
            param_info = "N/A"
            if parameter_preds is not None and sample_idx < parameter_preds.shape[0]:
                if action_pos < parameter_preds[sample_idx].shape[0]:
                    param_pred = parameter_preds[sample_idx][action_pos]
                    param_info = f"{param_pred.item():.4f}"
            
            # ìµœì¢… ì¶œë ¥
            print(f"   [{i}] ì‹œí€€ìŠ¤ ìœ„ì¹˜ {action_pos}:")
            print(f"       - ì˜ˆì¸¡ ê²Œì´íŠ¸: {pred_gate} (í™•ë¥ : {gate_confidence:.4f})")
            print(f"       - ì •ë‹µ ê²Œì´íŠ¸: {true_gate}")
            print(f"       - ì˜ˆì¸¡ ìœ„ì¹˜: {position_info}")
            print(f"       - ì˜ˆì¸¡ íŒŒë¼ë¯¸í„°: {param_info}")
    
    def _move_batch_to_device(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        device_batch = {}
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                device_batch[key] = value.to(self.device)
            else:
                device_batch[key] = value
        return device_batch
    
    def save_checkpoint(self, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì›ìì  ì €ì¥ìœ¼ë¡œ ì†ìƒ ë°©ì§€)"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'best_val_loss': self.best_val_loss,
            'config': asdict(self.config)
        }
        
        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ (ì›ìì  ì €ì¥)
        checkpoint_path = self.save_dir / f"checkpoint_step_{self.global_step}.pt"
        temp_path = self.save_dir / f"checkpoint_step_{self.global_step}.pt.tmp"
        
        try:
            torch.save(checkpoint, temp_path)
            temp_path.rename(checkpoint_path)  # ì›ìì  ì´ë™
            print(f"âœ… Checkpoint saved: {checkpoint_path}")
        except Exception as e:
            print(f"âŒ Failed to save checkpoint: {e}")
            if temp_path.exists():
                temp_path.unlink()  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (ì›ìì  ì €ì¥)
        if is_best:
            best_path = self.save_dir / "best_model.pt"
            temp_best_path = self.save_dir / "best_model.pt.tmp"
            
            try:
                torch.save(checkpoint, temp_best_path)
                temp_best_path.rename(best_path)  # ì›ìì  ì´ë™
                print(f"âœ… New best model saved! Val loss: {self.best_val_loss:.4f}")
            except Exception as e:
                print(f"âŒ Failed to save best model: {e}")
                if temp_best_path.exists():
                    temp_best_path.unlink()  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        checkpoint = torch.load(checkpoint_path, map_location=self.config.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        
        print(f"Checkpoint loaded from {checkpoint_path}")


def set_seed(seed: int):
    """ì‹œë“œ ì„¤ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from ..data.quantum_circuit_dataset import DatasetManager, create_dataloaders
    from ..data.embedding_pipeline import create_embedding_pipeline, EmbeddingConfig
    from ..models.decision_transformer import create_decision_transformer
    
    # ì„¤ì •
    config = TrainingConfig(
        d_model=256,
        n_layers=4,
        n_heads=8,
        batch_size=8,
        num_epochs=10,
        use_wandb=False  # í…ŒìŠ¤íŠ¸ìš©
    )
    
    # ì‹œë“œ ì„¤ì •
    set_seed(config.seed)
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    manager = DatasetManager("../data/unified_batch_experiment_results_with_circuits.json")
    train_ds, val_ds, test_ds = manager.split_dataset()
    
    # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ (ìºì‹± í™œì„±í™”)
    embed_config = EmbeddingConfig(d_model=config.d_model, n_gate_types=config.n_gate_types)
    embedding_pipeline = create_embedding_pipeline(embed_config)
    
    # ìºì‹± í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)
    if hasattr(embedding_pipeline, 'enable_cache'):
        embedding_pipeline.enable_cache = True
        print(" ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ìºì‹±ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì½œë ˆì´í„°
    collator = QuantumCircuitCollator(embedding_pipeline)
    
    # ë°ì´í„°ë¡œë”
    train_loader, val_loader, test_loader = create_dataloaders(
        train_ds, val_ds, test_ds, 
        batch_size=config.batch_size,
        num_workers=0
    )
    
    # ì½œë ˆì´í„° ì ìš©
    train_loader.collate_fn = collator
    val_loader.collate_fn = collator
    
    # NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ ìƒì„± (gate ìˆ˜ëŠ” ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •)
    model = DecisionTransformer(
        d_model=config.d_model,
        n_layers=config.n_layers,
        n_heads=config.n_heads,
        n_gate_types=config.n_gate_types,  # ì´ë¯¸ __post_init__ì—ì„œ ì„¤ì •ë¨
        dropout=config.dropout,
        attention_mode=config.attention_mode
    )  
    
    # íŠ¸ë ˆì´ë„ˆ
    trainer = DecisionTransformerTrainer(
        config=config,
        model=model,
        train_loader=train_loader,
        val_loader=val_loader
    )
    
    # í•™ìŠµ ì „ ìºì‹œ í†µê³„ ì´ˆê¸°í™”
    if hasattr(embedding_pipeline, 'clear_cache'):
        embedding_pipeline.clear_cache()
        print(" í•™ìŠµ ì‹œì‘ ì „ ìºì‹œë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
    
    # í•™ìŠµ ì‹œì‘
    print(" ìºì‹± ìµœì í™”ëœ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    trainer.train()
    
    # í•™ìŠµ í›„ ìºì‹œ í†µê³„ ì¶œë ¥
    if hasattr(embedding_pipeline, 'print_cache_stats'):
        print("\n" + "="*50)
        print(" í•™ìŠµ ì™„ë£Œ! ìºì‹œ ì„±ëŠ¥ í†µê³„:")
        embedding_pipeline.print_cache_stats()
        print("="*50)
