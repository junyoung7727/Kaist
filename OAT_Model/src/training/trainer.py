"""
Decision Transformer Training Pipeline
ê°„ë‹¨í•˜ê³  í™•ì¥ì„± ë†’ì€ í•™ìŠµ íŒŒì´í”„ë¼ì¸
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from pathlib import Path
from tqdm import tqdm
from typing import Dict, Any
import wandb
import random
import numpy as np
import sys
from typing import Optional, List, Tuple, Any, Union, Dict  
import os
from dataclasses import dataclass, asdict

from src.models.decision_transformer import DecisionTransformerLoss

# ê³µí†µ ë””ë²„ê·¸ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
from utils.debug_utils import debug_print, debug_tensor_info

# ğŸ† NEW: ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "quantumcommon"))
from gates import QuantumGateRegistry

import time
# wandb ì„ íƒì ìœ¼ë¡œ ì„í¬íŠ¸
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not installed. Logging will be disabled.")
    
# ê²½ë¡œ ì„¤ì • ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# ì ˆëŒ€ ê²½ë¡œ ì„í¬íŠ¸
try:
    from models.decision_transformer import DecisionTransformer, DecisionTransformerLoss
    from data.embedding_pipeline import EmbeddingPipeline, EmbeddingConfig
    from data.quantum_circuit_dataset import CircuitSpec
except ImportError:
    # ìƒëŒ€ ê²½ë¡œ ì„í¬íŠ¸ ì‹œë„
    from ..models.decision_transformer import DecisionTransformer, DecisionTransformerLoss
    from ..data.embedding_pipeline import EmbeddingPipeline, EmbeddingConfig
    from ..data.quantum_circuit_dataset import CircuitSpec


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì •"""
    # ëª¨ë¸ ì„¤ì •
    d_model: int = 512
    n_layers: int = 6
    n_heads: int = 8
    n_gate_types: int = None  # ğŸ† NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •
    dropout: float = 0.1
    attention_mode: str = "standard"  # "standard", "advanced", "hybrid"
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ gate ìˆ˜ë¥¼ ì‹±ê¸€í†¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        if self.n_gate_types is None:
            self.n_gate_types = QuantumGateRegistry.get_singleton_gate_count()
            print(f"ğŸ† TrainingConfig: Using gate vocab singleton, n_gate_types = {self.n_gate_types}")
    
    # í•™ìŠµ ì„¤ì •
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    batch_size: int = 32
    num_epochs: int = 1
    warmup_steps: int = 1000
    
    # ê²€ì¦ ì„¤ì •
    eval_every: int = 500
    save_every: int = 1000
    
    # ê¸°íƒ€
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42
    
    # ë¡œê¹…
    use_wandb: bool = True
    project_name: str = "quantum-decision-transformer"
    run_name: Optional[str] = None


class QuantumCircuitCollator:
    """ë°°ì¹˜ ì½œë ˆì´í„° - CircuitSpecì„ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ë³€í™˜"""
    
    def __init__(self, embedding_pipeline: EmbeddingPipeline):
        self.embedding_pipeline = embedding_pipeline
    
    def __call__(self, batch: List[CircuitSpec]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ì²˜ë¦¬"""

        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ë°°ì¹˜ ì²˜ë¦¬
        embedded_batch = self.embedding_pipeline.process_batch(batch)
        
        if not embedded_batch:
            return {}
        
        # íƒ€ê²Ÿ ì•¡ì…˜ ìƒì„± (ë‹¤ìŒ ê²Œì´íŠ¸ ì˜ˆì¸¡)
        target_actions = self._create_target_actions(embedded_batch)
        embedded_batch['target_actions'] = target_actions
        
        return embedded_batch
            
    
    def _create_target_actions(self, batch_data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ì„ë² ë”©ëœ ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒ ì•¡ì…˜ì„ ì¶”ì¶œí•˜ì—¬ íƒ€ê²Ÿ ìƒì„±"""
        # action_prediction_maskì˜ í˜•íƒœ í™•ì¸
        action_mask_shape = batch_data['action_prediction_mask'].shape
        debug_print(f"Debug: action_prediction_mask shape: {action_mask_shape}")
        
        batch_size = len(batch_data['circuit_id'])
        # input_sequence: [batch, 1, seq_len, d_model] -> seq_lenì€ shape[2]
        max_seq_len = batch_data['input_sequence'].shape[2]
        
        debug_print(f"Debug: Creating target_actions with shape [{batch_size}, {max_seq_len}]")
        target_actions = torch.zeros(batch_size, max_seq_len, dtype=torch.long)
        
        # ì´ë¯¸ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì—ì„œ ê³„ì‚°ëœ target_actions ì‚¬ìš©
        if 'target_actions' in batch_data:
            debug_print("Debug: Using pre-computed target_actions from embedding pipeline")
            # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì—ì„œ ì´ë¯¸ ê³„ì‚°ëœ íƒ€ê²Ÿ ì‚¬ìš©
            pipeline_targets = batch_data['target_actions']
            
            # ì°¨ì› ë§ì¶”ê¸°
            if len(pipeline_targets.shape) == 3:  # [batch, 1, seq_len]
                pipeline_targets = pipeline_targets.squeeze(1)  # [batch, seq_len]
            
            # ë°°ì¹˜ í¬ê¸°ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸°
            target_batch_size = min(batch_size, pipeline_targets.shape[0])
            target_seq_len = min(max_seq_len, pipeline_targets.shape[1])
            
            target_actions[:target_batch_size, :target_seq_len] = pipeline_targets[:target_batch_size, :target_seq_len]
            debug_print(f"Debug: Copied target_actions shape: [{target_batch_size}, {target_seq_len}]")
            
        else:
            debug_print("Warning: No pre-computed target_actions found, using fallback")
            # í´ë°±: State-Action-Reward ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒ Action ì¶”ì¶œ
            for i in range(batch_size):
                # action_prediction_mask ì²˜ë¦¬
                if len(action_mask_shape) == 3:  # [batch_size, 1, seq_len] í˜•íƒœ
                    action_mask = batch_data['action_prediction_mask'][i].squeeze(0)
                elif len(action_mask_shape) == 2:  # [batch_size, seq_len] í˜•íƒœ
                    action_mask = batch_data['action_prediction_mask'][i]
                else:  # 1ì°¨ì› í˜•íƒœ
                    action_mask = batch_data['action_prediction_mask']
                
                debug_print(f"Debug: action_mask[{i}] shape: {action_mask.shape}")
                
                # State-Action-Reward íŒ¨í„´ì—ì„œ ë‹¤ìŒ Action ìœ„ì¹˜ ì°¾ê¸°
                action_positions = torch.where(action_mask)[0]
                for pos_idx, seq_pos in enumerate(action_positions):
                    # ë‹¤ìŒ Action ìœ„ì¹˜ ê³„ì‚° (S-A-R íŒ¨í„´ì—ì„œ Actionì€ 1, 4, 7, 10... ìœ„ì¹˜)
                    if seq_pos % 3 == 1:  # Action ìœ„ì¹˜ì¸ ê²½ìš°
                        next_action_pos = seq_pos + 3  # ë‹¤ìŒ Action ìœ„ì¹˜
                        if next_action_pos < max_seq_len:
                            # ë‹¤ìŒ Actionì˜ ê²Œì´íŠ¸ IDë¥¼ íƒ€ê²Ÿìœ¼ë¡œ (ë”ë¯¸ êµ¬í˜„)
                            target_actions[i][seq_pos] = (seq_pos // 3) % 20  # ì„ì‹œ ê²Œì´íŠ¸ ID
                        else:
                            # ì‹œí€€ìŠ¤ ëì´ë©´ EOS í† í° (19ë²ˆ)
                            target_actions[i][seq_pos] = 19
                
                debug_print(f"Debug: Fallback - set targets for batch {i}")
        
        return target_actions


class DecisionTransformerTrainer:
    """Decision Transformer íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(
        self,
        config: TrainingConfig,
        model: DecisionTransformer,
        train_loader: DataLoader,
        val_loader: DataLoader,
        save_dir: str = "OAT_Model/checkpoints"
    ):
        self.config = config
        self.model = model.to(config.device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¬¸ì œ 1 í•´ê²°: í•™ìŠµë¥  ë³µì› 
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            eps=1e-8,  # ìˆ˜ì¹˜ ì•ˆì •ì„±
            betas=(0.9, 0.95)  # ì•ˆì •ì ì¸ ëª¨ë©˜í…€
        )
        
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=config.num_epochs * len(train_loader)
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.loss_fn = DecisionTransformerLoss()
        
        # í•™ìŠµ ìƒíƒœ
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        # ë¡œê¹… ì´ˆê¸°í™”
        if config.use_wandb and WANDB_AVAILABLE:
            wandb.init(
                project=config.project_name,
                name=config.run_name,
                config=asdict(config)
            )
        elif config.use_wandb and not WANDB_AVAILABLE:
            print("Warning: wandb requested but not available. Install with 'pip install wandb'.")
    
    def train_epoch(self) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc="Training")
        
        for batch in pbar:
            if not batch:  # ë¹ˆ ë°°ì¹˜ ìŠ¤í‚µ
                continue
            
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            batch = self._move_batch_to_device(batch)
            
            # ìˆœì „íŒŒ
            self.optimizer.zero_grad()
            
            outputs = self.model(
                input_sequence=batch['input_sequence'],
                attention_mask=batch['attention_mask'],
                action_prediction_mask=batch['action_prediction_mask']
            )
            
            # ì†ì‹¤ ê³„ì‚°
            # action_prediction_maskì™€ target_actionsë¥¼ squeezeí•´ì„œ [batch, seq_len] í˜•íƒœë¡œ ë§Œë“¤ê¸°
            squeezed_action_mask = batch['action_prediction_mask'].squeeze(1)
            squeezed_target_actions = batch['target_actions'].squeeze(1)
            loss_outputs = self.loss_fn(
                action_logits=outputs['action_logits'],  # ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬
                target_actions=squeezed_target_actions,
                action_prediction_mask=squeezed_action_mask
            )
            
            loss = loss_outputs['loss']
            accuracy = loss_outputs['accuracy']
            
            # ì—­ì „íŒŒ ë° ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            loss.backward()
            
            # ë¬¸ì œ 3 í•´ê²°: ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì™„í™” (0.1 â†’ 1.0)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item()
            total_accuracy += accuracy.item()
            num_batches += 1
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'acc': f"{accuracy.item():.4f}",
                'lr': f"{self.scheduler.get_last_lr()[0]:.2e}"
            })
            
            # ë¡œê¹…
            if self.config.use_wandb:
                wandb.log({
                    'train/loss': loss.item(),
                    'train/accuracy': accuracy.item(),
                    'train/learning_rate': self.scheduler.get_last_lr()[0],
                    'global_step': self.global_step
                })
            
            self.global_step += 1
            
            # ê²€ì¦
            if self.global_step % self.config.eval_every == 0:
                val_metrics = self.validate()
                self.model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if self.global_step % self.config.save_every == 0:
                self.save_checkpoint()
        
        return {
            'loss': total_loss / max(num_batches, 1),
            'accuracy': total_accuracy / max(num_batches, 1)
        }
    
    def validate(self) -> Dict[str, float]:
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                if not batch:
                    continue
                
                batch = self._move_batch_to_device(batch)
                
                outputs = self.model(
                    input_sequence=batch['input_sequence'],
                    attention_mask=batch['attention_mask'],
                    action_prediction_mask=batch['action_prediction_mask']
                )
                
                # ğŸš¨ CRITICAL FIX: ê²€ì¦ ë‹¨ê³„ì—ì„œë„ ë§ˆìŠ¤í¬ ì°¨ì› ìˆ˜ì • í•„ìš”
                # action_prediction_maskì™€ target_actionsë¥¼ squeezeí•´ì„œ [batch, seq_len] í˜•íƒœë¡œ ë§Œë“¤ê¸°
                squeezed_action_mask = batch['action_prediction_mask']
                squeezed_target_actions = batch['target_actions']
                
                # ì°¨ì› ìˆ˜ì •
                if len(squeezed_action_mask.shape) == 3 and squeezed_action_mask.shape[1] == 1:
                    squeezed_action_mask = squeezed_action_mask.squeeze(1)  # [batch, 1, seq_len] -> [batch, seq_len]
                if len(squeezed_target_actions.shape) == 3 and squeezed_target_actions.shape[1] == 1:
                    squeezed_target_actions = squeezed_target_actions.squeeze(1)  # [batch, 1, seq_len] -> [batch, seq_len]
                
                loss_outputs = self.loss_fn(
                    action_logits=outputs['action_logits'],  # [batch, seq_len, n_gate_types]
                    target_actions=squeezed_target_actions,  # [batch, seq_len]
                    action_prediction_mask=squeezed_action_mask  # [batch, seq_len]
                )
                
                total_loss += loss_outputs['loss'].item()
                total_accuracy += loss_outputs['accuracy'].item()
                num_batches += 1
        
        val_metrics = {
            'val_loss': total_loss / max(num_batches, 1),
            'val_accuracy': total_accuracy / max(num_batches, 1)
        }
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_metrics['val_loss'] < self.best_val_loss:
            self.best_val_loss = val_metrics['val_loss']
            self.save_checkpoint(is_best=True)
        
        # ë¡œê¹…
        if self.config.use_wandb:
            wandb.log({**{f'val/{k}': v for k, v in val_metrics.items()}, 'global_step': self.global_step})
        
        print(f"Validation - Loss: {val_metrics['val_loss']:.4f}, Accuracy: {val_metrics['val_accuracy']:.4f}")
        
        return val_metrics
    
    def train(self):
        """ì „ì²´ í•™ìŠµ ë£¨í”„"""
        print(f"Starting training for {self.config.num_epochs} epochs...")
        print(f"Device: {self.config.device}")
        print(f"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        
        for epoch in range(self.config.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.config.num_epochs}")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch()
            
            # ì—í¬í¬ ë¡œê¹…
            print(f"Train - Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.4f}")
            
            if self.config.use_wandb:
                wandb.log({
                    'epoch': epoch + 1,
                    'train/epoch_loss': train_metrics['loss'],
                    'train/epoch_accuracy': train_metrics['accuracy']
                })
        
        print("Training completed!")
        
        # ìµœì¢… ê²€ì¦
        final_val_metrics = self.validate()
        print(f"Final validation - Loss: {final_val_metrics['val_loss']:.4f}, Accuracy: {final_val_metrics['val_accuracy']:.4f}")
    
    def _move_batch_to_device(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        device_batch = {}
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                device_batch[key] = value.to(self.config.device)
            else:
                device_batch[key] = value
        return device_batch
    
    def save_checkpoint(self, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'best_val_loss': self.best_val_loss,
            'config': asdict(self.config)
        }
        
        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸
        checkpoint_path = self.save_dir / f"checkpoint_step_{self.global_step}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        if is_best:
            best_path = self.save_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            print(f"New best model saved! Val loss: {self.best_val_loss:.4f}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        checkpoint = torch.load(checkpoint_path, map_location=self.config.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        
        print(f"Checkpoint loaded from {checkpoint_path}")


def set_seed(seed: int):
    """ì‹œë“œ ì„¤ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from ..data.quantum_circuit_dataset import DatasetManager, create_dataloaders
    from ..data.embedding_pipeline import create_embedding_pipeline, EmbeddingConfig
    from ..models.decision_transformer import create_decision_transformer
    
    # ì„¤ì •
    config = TrainingConfig(
        d_model=256,
        n_layers=4,
        n_heads=8,
        batch_size=8,
        num_epochs=10,
        use_wandb=False  # í…ŒìŠ¤íŠ¸ìš©
    )
    
    # ì‹œë“œ ì„¤ì •
    set_seed(config.seed)
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    manager = DatasetManager("../data/unified_batch_experiment_results_with_circuits.json")
    train_ds, val_ds, test_ds = manager.split_dataset()
    
    # ì„ë² ë”© íŒŒì´í”„ë¼ì¸
    embed_config = EmbeddingConfig(d_model=config.d_model, n_gate_types=config.n_gate_types)
    embedding_pipeline = create_embedding_pipeline(embed_config)
    
    # ì½œë ˆì´í„°
    collator = QuantumCircuitCollator(embedding_pipeline)
    
    # ë°ì´í„°ë¡œë”
    train_loader, val_loader, test_loader = create_dataloaders(
        train_ds, val_ds, test_ds, 
        batch_size=config.batch_size,
        num_workers=0
    )
    
    # ì½œë ˆì´í„° ì ìš©
    train_loader.collate_fn = collator
    val_loader.collate_fn = collator
    
    # ğŸ† NEW: ëª¨ë“ˆëŸ¬ ì–´í…ì…˜ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ ìƒì„± (gate ìˆ˜ëŠ” ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •)
    model = DecisionTransformer(
        d_model=config.d_model,
        n_layers=config.n_layers,
        n_heads=config.n_heads,
        n_gate_types=config.n_gate_types,  # ì´ë¯¸ __post_init__ì—ì„œ ì„¤ì •ë¨
        dropout=config.dropout,
        attention_mode=config.attention_mode
    )  
    
    # íŠ¸ë ˆì´ë„ˆ
    trainer = DecisionTransformerTrainer(
        config=config,
        model=model,
        train_loader=train_loader,
        val_loader=val_loader
    )
    
    # í•™ìŠµ ì‹œì‘
    trainer.train()
