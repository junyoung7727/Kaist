"""
RTG Dataset Loader
ì‚¬ì „ ê³„ì‚°ëœ RTG ê°’ì´ í¬í•¨ëœ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ” ëª¨ë“ˆ
"""

import torch
from torch.utils.data import Dataset, DataLoader
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path

class RTGQuantumDataset(Dataset):
    """ì‚¬ì „ ê³„ì‚°ëœ RTG ê°’ì´ í¬í•¨ëœ ì–‘ì íšŒë¡œ ë°ì´í„°ì…‹"""
    
    def __init__(self, dataset_path: str, max_seq_len: int = 512, d_model: int = 512):
        """
        Args:
            dataset_path: RTG ê°’ì´ í¬í•¨ëœ ë°ì´í„°ì…‹ ê²½ë¡œ
            max_seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            d_model: ëª¨ë¸ ì°¨ì›
        """
        self.max_seq_len = max_seq_len
        self.d_model = d_model
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        with open(dataset_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"ğŸ“Š RTG ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ ì‹œí€€ìŠ¤")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        ë‹¨ì¼ ë°ì´í„° ì•„ì´í…œ ë°˜í™˜
        
        Returns:
            Dict containing:
            - states: [seq_len, d_model] ìƒíƒœ ì‹œí€€ìŠ¤
            - actions: [seq_len, d_model] ì•¡ì…˜ ì‹œí€€ìŠ¤  
            - rtg_rewards: [seq_len] RTG ë¦¬ì›Œë“œ ì‹œí€€ìŠ¤
            - attention_mask: [seq_len] ì–´í…ì…˜ ë§ˆìŠ¤í¬
            - targets: ì •ë‹µ ë ˆì´ë¸”ë“¤
        """
        item = self.data[idx]
        
        # RTG ë¦¬ì›Œë“œ ì‹œí€€ìŠ¤ ì¶”ì¶œ
        rtg_rewards = item.get('rtg_rewards', [])
        seq_len = len(rtg_rewards)
        
        # íŒ¨ë”© ì²˜ë¦¬
        if seq_len > self.max_seq_len:
            # ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ìë¥´ê¸°
            seq_len = self.max_seq_len
            rtg_rewards = rtg_rewards[:self.max_seq_len]
        
        # í…ì„œ ìƒì„± ë° íŒ¨ë”©
        padded_rtg = torch.zeros(self.max_seq_len)
        padded_rtg[:seq_len] = torch.tensor(rtg_rewards[:seq_len], dtype=torch.float32)
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        attention_mask = torch.zeros(self.max_seq_len, dtype=torch.bool)
        attention_mask[:seq_len] = True
        
        # ìƒíƒœì™€ ì•¡ì…˜ ì‹œí€€ìŠ¤ ìƒì„± (í”Œë ˆì´ìŠ¤í™€ë”)
        # TODO: ì‹¤ì œ ë°ì´í„°ì—ì„œ ìƒíƒœ/ì•¡ì…˜ ì¶”ì¶œ ë¡œì§ êµ¬í˜„
        states = torch.randn(self.max_seq_len, self.d_model)
        actions = torch.randn(self.max_seq_len, self.d_model)
        
        # ì •ë‹µ ì†ì„±ê°’ë“¤
        predicted_properties = item.get('predicted_properties', {})
        targets = {}
        for prop_name in ['entanglement', 'fidelity', 'expressibility']:
            if prop_name in predicted_properties:
                prop_values = predicted_properties[prop_name]
                padded_prop = torch.zeros(self.max_seq_len)
                padded_prop[:len(prop_values)] = torch.tensor(prop_values[:seq_len], dtype=torch.float32)
                targets[prop_name] = padded_prop
        
        return {
            'states': states,
            'actions': actions,
            'rtg_rewards': padded_rtg,
            'attention_mask': attention_mask,
            'targets': targets,
            'seq_len': seq_len
        }


class RTGDataLoader:
    """RTG ë°ì´í„°ì…‹ìš© ë°ì´í„° ë¡œë” ë˜í¼"""
    
    def __init__(self, dataset_path: str, batch_size: int = 32, 
                 max_seq_len: int = 512, d_model: int = 512,
                 shuffle: bool = True, num_workers: int = 0):
        """
        Args:
            dataset_path: RTG ë°ì´í„°ì…‹ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            d_model: ëª¨ë¸ ì°¨ì›
            shuffle: ë°ì´í„° ì…”í”Œ ì—¬ë¶€
            num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        """
        self.dataset = RTGQuantumDataset(dataset_path, max_seq_len, d_model)
        
        self.dataloader = DataLoader(
            self.dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self._collate_fn
        )
    
    def _collate_fn(self, batch):
        """ë°°ì¹˜ ë°ì´í„° ì½œë ˆì´íŠ¸ í•¨ìˆ˜"""
        batch_size = len(batch)
        max_seq_len = batch[0]['states'].shape[0]
        d_model = batch[0]['states'].shape[1]
        
        # ë°°ì¹˜ í…ì„œ ì´ˆê¸°í™”
        batch_states = torch.zeros(batch_size, max_seq_len, d_model)
        batch_actions = torch.zeros(batch_size, max_seq_len, d_model)
        batch_rtg_rewards = torch.zeros(batch_size, max_seq_len)
        batch_attention_mask = torch.zeros(batch_size, max_seq_len, dtype=torch.bool)
        
        # ì •ë‹µ ì†ì„±ê°’ë“¤
        batch_targets = {}
        prop_names = ['entanglement', 'fidelity', 'expressibility']
        for prop_name in prop_names:
            batch_targets[prop_name] = torch.zeros(batch_size, max_seq_len)
        
        # ë°°ì¹˜ ë°ì´í„° ì±„ìš°ê¸°
        for i, item in enumerate(batch):
            batch_states[i] = item['states']
            batch_actions[i] = item['actions']
            batch_rtg_rewards[i] = item['rtg_rewards']
            batch_attention_mask[i] = item['attention_mask']
            
            for prop_name in prop_names:
                if prop_name in item['targets']:
                    batch_targets[prop_name][i] = item['targets'][prop_name]
        
        return {
            'states': batch_states,
            'actions': batch_actions,
            'rtg_rewards': batch_rtg_rewards,
            'attention_mask': batch_attention_mask,
            'targets': batch_targets
        }
    
    def __iter__(self):
        return iter(self.dataloader)
    
    def __len__(self):
        return len(self.dataloader)


def create_rtg_dataloaders(train_path: str, val_path: str, test_path: str,
                          batch_size: int = 32, max_seq_len: int = 512, 
                          d_model: int = 512, num_workers: int = 0) -> Tuple[RTGDataLoader, RTGDataLoader, RTGDataLoader]:
    """
    RTG ë°ì´í„° ë¡œë”ë“¤ ìƒì„±
    
    Args:
        train_path: í›ˆë ¨ ë°ì´í„°ì…‹ ê²½ë¡œ
        val_path: ê²€ì¦ ë°ì´í„°ì…‹ ê²½ë¡œ
        test_path: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        batch_size: ë°°ì¹˜ í¬ê¸°
        max_seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        d_model: ëª¨ë¸ ì°¨ì›
        num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        
    Returns:
        (train_loader, val_loader, test_loader)
    """
    train_loader = RTGDataLoader(
        train_path, batch_size=batch_size, max_seq_len=max_seq_len,
        d_model=d_model, shuffle=True, num_workers=num_workers
    )
    
    val_loader = RTGDataLoader(
        val_path, batch_size=batch_size, max_seq_len=max_seq_len,
        d_model=d_model, shuffle=False, num_workers=num_workers
    )
    
    test_loader = RTGDataLoader(
        test_path, batch_size=batch_size, max_seq_len=max_seq_len,
        d_model=d_model, shuffle=False, num_workers=num_workers
    )
    
    return train_loader, val_loader, test_loader


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    train_loader, val_loader, test_loader = create_rtg_dataloaders(
        train_path="processed_data/rtg_train.json",
        val_path="processed_data/rtg_val.json", 
        test_path="processed_data/rtg_test.json",
        batch_size=16,
        max_seq_len=256,
        d_model=512
    )
    
    # ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸
    for batch in train_loader:
        print("ë°°ì¹˜ í˜•íƒœ:")
        print(f"  States: {batch['states'].shape}")
        print(f"  Actions: {batch['actions'].shape}")
        print(f"  RTG Rewards: {batch['rtg_rewards'].shape}")
        print(f"  Attention Mask: {batch['attention_mask'].shape}")
        print(f"  Targets: {[f'{k}: {v.shape}' for k, v in batch['targets'].items()]}")
        break
