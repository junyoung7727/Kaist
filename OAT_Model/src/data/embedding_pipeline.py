"""
Embedding Pipeline Module
CircuitSpec -> Grid Encoder -> Decision Transformer Embedding
"""

import torch
import numpy as np
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import sys
from pathlib import Path

# ì„í¬íŠ¸ ê²½ë¡œ ë¬¸ì œ í•´ê²°
try:
    # ì ˆëŒ€ ê²½ë¡œ ì‹œë„
    from data.quantum_circuit_dataset import CircuitSpec
    from encoding.grid_graph_encoder import GridGraphEncoder
    from encoding.Decision_Transformer_Embed import QuantumGateSequenceEmbedding
except ImportError:
    # ìƒëŒ€ ê²½ë¡œ ì‹œë„
    try:
        from .quantum_circuit_dataset import CircuitSpec
        from ..encoding.grid_graph_encoder import GridGraphEncoder
        from ..encoding.Decision_Transformer_Embed import QuantumGateSequenceEmbedding
    except ImportError:
        # ë¡œì»¬ ê²½ë¡œ ì‹œë„
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent.parent))
        from data.quantum_circuit_dataset import CircuitSpec
        from encoding.grid_graph_encoder import GridGraphEncoder
        from encoding.Decision_Transformer_Embed import QuantumGateSequenceEmbedding

# ğŸ† NEW: ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "quantumcommon"))
from gates import QuantumGateRegistry

@dataclass
class EmbeddingConfig:
    """ì„ë² ë”© ì„¤ì •"""
    d_model: int = 512
    n_gate_types: int = None  # NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •
    n_qubits: int = 10
    max_seq_len: int = 1000
    max_time_steps: int = 50

    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ gate ìˆ˜ë¥¼ ì‹±ê¸€í†¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        if self.n_gate_types is None:
            self.n_gate_types = QuantumGateRegistry.get_singleton_gate_count()
            print(f" EmbeddingConfig: Using gate vocab singleton, n_gate_types = {self.n_gate_types}")


class EmbeddingPipeline:
    """ì™„ì „í•œ ì„ë² ë”© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        
        # ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì´ˆê¸°í™”
        self.gate_registry = QuantumGateRegistry()
        
        # ê²Œì´íŠ¸ vocab ì´ˆê¸°í™”
        self.gate_vocab = self.gate_registry.get_gate_vocab()
        
        # ê²Œì´íŠ¸ ìˆ˜ í™•ì¸ ë° ì„¤ì • ë™ê¸°í™”
        actual_gate_count = len(self.gate_vocab)
        if self.config.n_gate_types != actual_gate_count:
            print(f" Config mismatch: expected {self.config.n_gate_types}, got {actual_gate_count}")
            self.config.n_gate_types = actual_gate_count
        print(f" EmbeddingPipeline initialized with {actual_gate_count} gate types from singleton")
        
        # Grid Encoder ì´ˆê¸°í™”
        self.grid_encoder = GridGraphEncoder()
        
        # Decision Transformer Embedding ì´ˆê¸°í™”
        self.dt_embedding = QuantumGateSequenceEmbedding(
            d_model=config.d_model,
            n_gate_types=config.n_gate_types,
            n_qubits=config.n_qubits,
            max_seq_len=config.max_seq_len
        )
    
    def process_single_circuit(self, circuit_spec: CircuitSpec) -> Dict[str, torch.Tensor]:
        """ë‹¨ì¼ íšŒë¡œ ì²˜ë¦¬"""
        
        # 1. Grid Encoderë¡œ íšŒë¡œ ì¸ì½”ë”©
        encoded_data = self.grid_encoder.encode(circuit_spec)
        
        # 2. ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ ê·¸ë¦¬ë“œ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜
        grid_matrix_data = self.grid_encoder.to_grid_matrix(encoded_data)
        
        # 3. Decision Transformer Embedding ì ìš©
        dt_results = self.dt_embedding.process_grid_matrix_data(grid_matrix_data)
        
        # 4. ë©”íƒ€ë°ì´í„° ì¶”ê°€
        dt_results.update({
            'circuit_id': circuit_spec.circuit_id,
            'num_qubits': circuit_spec.num_qubits,
            'num_gates': len(circuit_spec.gates)
        })
        
        return dt_results
    
    def process_batch(self, circuit_specs: List[CircuitSpec]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        batch_results = []
        
        for spec in circuit_specs:
            try:
                result = self.process_single_circuit(spec)
                batch_results.append(result)
            except Exception as e:
                print(f"Error processing circuit {spec.circuit_id}: {e}")
                continue
        
        if not batch_results:
            return {}
        
        # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ í•©ì¹˜ê¸°
        return self._combine_batch_results(batch_results)
    
    def _combine_batch_results(self, batch_results: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸°"""
        if not batch_results:
            return {}
        
        combined = {}
        
        # í…ì„œ ë°ì´í„° í•©ì¹˜ê¸°
        tensor_keys = ['input_sequence', 'attention_mask', 'action_prediction_mask', 
                      'state_embedded', 'action_embedded', 'reward_embedded', 'target_actions']
        
        for key in tensor_keys:
            if key in batch_results[0]:
                # íŒ¨ë”©ì„ í†µí•œ ë°°ì¹˜ ì²˜ë¦¬ (ê°€ë³€ ê¸¸ì´ ì§€ì›)
                tensors = [result[key] for result in batch_results]
                combined[key] = self._pad_and_stack_tensors(tensors)
        
        # ë©”íƒ€ë°ì´í„° í•©ì¹˜ê¸°
        meta_keys = ['circuit_id', 'num_qubits', 'num_gates', 'episode_time_len', 'sar_sequence_len']
        for key in meta_keys:
            if key in batch_results[0]:
                combined[key] = [result[key] for result in batch_results]
        
        return combined
    
    def _pad_and_stack_tensors(self, tensors: List[torch.Tensor]) -> torch.Tensor:
        """
        ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ í…ì„œë“¤ì„ íŒ¨ë”©í•˜ì—¬ ë°°ì¹˜ë¡œ í•©ì¹˜ê¸°
        
        Args:
            tensors: ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ í…ì„œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            íŒ¨ë”©ëœ ë°°ì¹˜ í…ì„œ
        """
        if not tensors:
            return torch.tensor([])
        
        # ìµœëŒ€ í¬ê¸° ê³„ì‚°
        max_dims = []
        for dim in range(len(tensors[0].shape)):
            max_size = max(tensor.shape[dim] for tensor in tensors)
            max_dims.append(max_size)
        
        # íŒ¨ë”©ëœ í…ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        padded_tensors = []
        for tensor in tensors:
            # íŒ¨ë”© í¬ê¸° ê³„ì‚°
            padding = []
            for dim in reversed(range(len(tensor.shape))):
                pad_size = max_dims[dim] - tensor.shape[dim]
                padding.extend([0, pad_size])  # (left, right) padding
            
            # íŒ¨ë”© ì ìš©
            if any(p > 0 for p in padding):
                padded_tensor = torch.nn.functional.pad(tensor, padding, value=0)
            else:
                padded_tensor = tensor
            
            padded_tensors.append(padded_tensor)
        
        # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ í•©ì¹˜ê¸°
        return torch.stack(padded_tensors, dim=0)


def create_embedding_pipeline(config: EmbeddingConfig = None) -> EmbeddingPipeline:
    """ì„ë² ë”© íŒŒì´í”„ë¼ì¸ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    if config is None:
        config = EmbeddingConfig()
    
    return EmbeddingPipeline(config)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from quantum_circuit_dataset import DatasetManager
    
    # ë°ì´í„°ì…‹ ë¡œë”©
    manager = DatasetManager("OAT_Model/dataset/unified_batch_experiment_results_with_circuits.json")
    circuit_specs = manager.parse_circuits()
    
    # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ìƒì„±
    config = EmbeddingConfig(d_model=256)  # NEW: ì‹±ê¸€í†¤ì—ì„œ ê°€ì ¸ì˜¨ gate ìˆ˜ë¡œ ì„ë² ë”© ë ˆì´ì–´ ì´ˆê¸°í™”
    pipeline = create_embedding_pipeline(config)
    
    # ë‹¨ì¼ íšŒë¡œ í…ŒìŠ¤íŠ¸
    if circuit_specs:
        print("Testing single circuit processing...")
        result = pipeline.process_single_circuit(circuit_specs[0])
        
        print(f"Circuit ID: {result['circuit_id']}")
        print(f"Input sequence shape: {result['input_sequence'].shape}")
        print(f"Attention mask shape: {result['attention_mask'].shape}")
        
        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        print("\nTesting batch processing...")
        batch_result = pipeline.process_batch(circuit_specs[:3])
        
        if batch_result:
            print(f"Batch input sequence shape: {batch_result['input_sequence'].shape}")
            print(f"Batch attention mask shape: {batch_result['attention_mask'].shape}")
