"""
Property Prediction Training with Focused Debugging
ìˆ˜ë ´ ë¬¸ì œ ë¶„ì„ì„ ìœ„í•œ í•µì‹¬ ë””ë²„ê¹…ì´ í¬í•¨ëœ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
import sys
from pathlib import Path
import argparse
import json
from tqdm import tqdm
import time
import os

# Add project paths
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent.parent.parent / "quantumcommon"))

# Import models and config
from models.property_prediction_transformer import (
    PropertyPredictionTransformer, 
    PropertyPredictionConfig,
    PropertyPredictionLoss
)

# Import data
from data.quantum_circuit_dataset import DatasetManager, create_dataloaders

# Import debugging system
from debug.training_debug import create_training_debugger, PropertyTrainingDebugger

# Import gates
from gates import QuantumGateRegistry


class PropertyTrainingAnalyzer:
    """Property Prediction í›ˆë ¨ ë¶„ì„ê¸°"""
    
    def __init__(self, 
                 model_config: PropertyPredictionConfig = None,
                 debug_mode: str = "focused"):  # "focused", "detailed", "minimal"
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ Device: {self.device}")
        
        # ëª¨ë¸ ì„¤ì •
        self.config = model_config or PropertyPredictionConfig(
            d_model=512,
            n_heads=8, 
            n_layers=6,
            dropout=0.3,
            learning_rate=1e-4,
            weight_decay=1e-3
        )
        
        # ë””ë²„ê±° ì„¤ì •
        enable_all_debug = (debug_mode == "detailed")
        self.debugger = create_training_debugger(enable_all=enable_all_debug)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = PropertyPredictionTransformer(self.config)
        self.model.to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜ (ê°€ì¤‘ì¹˜ ì¡°ì •)
        self.criterion = PropertyPredictionLoss(
            entanglement_weight=1.0,
            fidelity_weight=5.0,      # Fidelity ì¤‘ìš”ë„ ì¦ê°€
            expressibility_weight=0.1, # Expressibility ê°€ì¤‘ì¹˜ ê°ì†Œ
            combined_weight=0.5
        )
        
        # ì˜µí‹°ë§ˆì´ì € (ë” ë³´ìˆ˜ì ì¸ ì„¤ì •)
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ (Plateau ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½)
        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True,
            min_lr=1e-6
        )
        
        # í›ˆë ¨ ìƒíƒœ
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.max_patience = 15
        
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"ğŸ¯ ë””ë²„ê·¸ ëª¨ë“œ: {debug_mode}")
    
    def analyze_data_quality(self, dataloader: DataLoader):
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        print("\nğŸ” ë°ì´í„° í’ˆì§ˆ ë¶„ì„...")
        
        total_samples = 0
        property_stats = {
            'entanglement': [],
            'fidelity': [], 
            'expressibility': []
        }
        
        # ìƒ˜í”Œ ìˆ˜ì§‘ (ìµœëŒ€ 1000ê°œ)
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 10:  # 10 ë°°ì¹˜ë§Œ ë¶„ì„
                break
                
            if isinstance(batch, dict) and 'targets' in batch:
                targets = batch['targets']
                for prop in property_stats.keys():
                    if prop in targets:
                        values = targets[prop].cpu().numpy()
                        property_stats[prop].extend(values)
                        
                total_samples += len(targets.get('entanglement', []))
        
        # í†µê³„ ì¶œë ¥
        print(f"   ì´ ë¶„ì„ ìƒ˜í”Œ: {total_samples}")
        for prop, values in property_stats.items():
            if values:
                import numpy as np
                values = np.array(values)
                print(f"   {prop}:")
                print(f"     ë²”ìœ„: [{values.min():.4f}, {values.max():.4f}]")
                print(f"     í‰ê· : {values.mean():.4f} Â± {values.std():.4f}")
                
                # ì´ìƒì¹˜ ì²´í¬
                q1, q3 = np.percentile(values, [25, 75])
                iqr = q3 - q1
                outliers = np.sum((values < q1 - 1.5*iqr) | (values > q3 + 1.5*iqr))
                if outliers > 0:
                    print(f"     âš ï¸ ì´ìƒì¹˜: {outliers}ê°œ ({outliers/len(values)*100:.1f}%)")
    
    def train_epoch(self, train_loader: DataLoader, epoch: int):
        """í›ˆë ¨ ì—í¬í¬"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, batch in enumerate(progress_bar):
            try:
                # ë°°ì¹˜ ì²˜ë¦¬
                if isinstance(batch, dict):
                    circuit_specs = batch.get('circuit_specs', [])
                    targets = batch.get('targets', {})
                else:
                    circuit_specs, targets = batch
                
                # GPUë¡œ ì´ë™
                if isinstance(targets, dict):
                    targets = {k: v.to(self.device) for k, v in targets.items()}
                
                # Forward pass
                self.optimizer.zero_grad()
                predictions = self.model(circuit_specs)
                
                # ì†ì‹¤ ê³„ì‚°
                if isinstance(self.criterion, PropertyPredictionLoss):
                    loss_dict = self.criterion(predictions, targets)
                    loss = loss_dict['total']
                else:
                    loss = self.criterion(predictions, targets)
                
                # Backward pass
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                # ë””ë²„ê¹… ë¡œê¹…
                self.debugger.log_training_step(
                    model=self.model,
                    loss=loss,
                    predictions=predictions,
                    targets=targets,
                    optimizer=self.optimizer,
                    batch_idx=batch_idx,
                    epoch=epoch
                )
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                total_loss += loss.item()
                num_batches += 1
                avg_loss = total_loss / num_batches
                
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.6f}',
                    'Avg': f'{avg_loss:.6f}',
                    'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                })
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return total_loss / max(num_batches, 1)
    
    def validate_epoch(self, val_loader: DataLoader, epoch: int):
        """ê²€ì¦ ì—í¬í¬"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        all_predictions = {'entanglement': [], 'fidelity': [], 'expressibility': []}
        all_targets = {'entanglement': [], 'fidelity': [], 'expressibility': []}
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                try:
                    # ë°°ì¹˜ ì²˜ë¦¬
                    if isinstance(batch, dict):
                        circuit_specs = batch.get('circuit_specs', [])
                        targets = batch.get('targets', {})
                    else:
                        circuit_specs, targets = batch
                    
                    # GPUë¡œ ì´ë™
                    if isinstance(targets, dict):
                        targets = {k: v.to(self.device) for k, v in targets.items()}
                    
                    # Forward pass
                    predictions = self.model(circuit_specs)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    if isinstance(self.criterion, PropertyPredictionLoss):
                        loss_dict = self.criterion(predictions, targets)
                        loss = loss_dict['total']
                    else:
                        loss = self.criterion(predictions, targets)
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                    # ì˜ˆì¸¡ê°’ ìˆ˜ì§‘ (ë¶„ì„ìš©)
                    for prop in all_predictions.keys():
                        if prop in predictions and prop in targets:
                            all_predictions[prop].append(predictions[prop].cpu())
                            all_targets[prop].append(targets[prop].cpu())
                
                except Exception as e:
                    print(f"âŒ ê²€ì¦ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        avg_val_loss = total_loss / max(num_batches, 1)
        
        # ì˜ˆì¸¡ê°’ ê²°í•©
        combined_predictions = {}
        combined_targets = {}
        for prop in all_predictions.keys():
            if all_predictions[prop]:
                combined_predictions[prop] = torch.cat(all_predictions[prop])
                combined_targets[prop] = torch.cat(all_targets[prop])
        
        # ê²€ì¦ ë””ë²„ê¹… ë¡œê¹…
        self.debugger.log_validation_epoch(
            val_loss=avg_val_loss,
            val_predictions=combined_predictions,
            val_targets=combined_targets,
            epoch=epoch
        )
        
        return avg_val_loss
    
    def train(self, 
              data_path: str,
              num_epochs: int = 50,
              batch_size: int = 32,
              save_dir: str = "checkpoints"):
        """ë©”ì¸ í›ˆë ¨ ë£¨í”„"""
        print(f"\nğŸš€ Property Prediction í›ˆë ¨ ì‹œì‘")
        print(f"   ë°ì´í„°: {data_path}")
        print(f"   ì—í¬í¬: {num_epochs}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        try:
            dataset_manager = DatasetManager(data_path)
            train_loader, val_loader, test_loader = create_dataloaders(
                dataset_manager,
                train_batch_size=batch_size,
                val_batch_size=batch_size,
                test_batch_size=batch_size
            )
            
            print(f"   í›ˆë ¨ ë°°ì¹˜: {len(train_loader)}")
            print(f"   ê²€ì¦ ë°°ì¹˜: {len(val_loader)}")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}")
            return
        
        # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
        self.analyze_data_quality(train_loader)
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(save_dir, exist_ok=True)
        
        # í›ˆë ¨ ë£¨í”„
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            
            # í›ˆë ¨
            train_loss = self.train_epoch(train_loader, epoch)
            
            # ê²€ì¦
            val_loss = self.validate_epoch(val_loader, epoch)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step(val_loss)
            
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                
                # ëª¨ë¸ ì €ì¥
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_loss': val_loss,
                    'config': self.config
                }, os.path.join(save_dir, 'best_model.pt'))
                
                print(f"ğŸ’¾ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (val_loss: {val_loss:.6f})")
            else:
                self.patience_counter += 1
            
            # Early stopping
            if self.patience_counter >= self.max_patience:
                print(f"ğŸ›‘ Early stopping (patience: {self.max_patience})")
                break
        
        # ë””ë²„ê·¸ ìš”ì•½ ì €ì¥
        debug_summary_path = os.path.join(save_dir, 'debug_summary.json')
        self.debugger.save_debug_summary(debug_summary_path)
        
        print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {self.best_val_loss:.6f}")
        print(f"   ì´ ì—í¬í¬: {self.current_epoch + 1}")


def main():
    parser = argparse.ArgumentParser(description='Property Prediction Training with Debug')
    parser.add_argument('--data_path', type=str, required=True, help='Dataset path')
    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')
    parser.add_argument('--debug_mode', type=str, default='focused', 
                       choices=['minimal', 'focused', 'detailed'], help='Debug level')
    parser.add_argument('--save_dir', type=str, default='debug_checkpoints', help='Save directory')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ì„¤ì •
    config = PropertyPredictionConfig(
        d_model=512,
        n_heads=8,
        n_layers=6,
        dropout=0.3,
        learning_rate=1e-4,
        weight_decay=1e-3
    )
    
    # í›ˆë ¨ ë¶„ì„ê¸° ìƒì„±
    analyzer = PropertyTrainingAnalyzer(
        model_config=config,
        debug_mode=args.debug_mode
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    analyzer.train(
        data_path=args.data_path,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        save_dir=args.save_dir
    )


if __name__ == "__main__":
    main()
