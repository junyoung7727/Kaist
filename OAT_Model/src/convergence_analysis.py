"""
Property Prediction Convergence Analysis
ìˆ˜ë ´ ë¬¸ì œì˜ í•µì‹¬ ì›ì¸ ë¶„ì„ ë° í•´ê²°ì±…
"""

import torch
import torch.nn as nn
from models.property_prediction_transformer import PropertyPredictionConfig, PropertyPredictionTransformer
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent / "quantumcommon"))


class ConvergenceAnalyzer:
    """ìˆ˜ë ´ ë¬¸ì œ í•µì‹¬ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def analyze_model_capacity(self, config: PropertyPredictionConfig):
        """ëª¨ë¸ ìš©ëŸ‰ ë¶„ì„"""
        model = PropertyPredictionTransformer(config)
        total_params = sum(p.numel() for p in model.parameters())
        
        print(f"ğŸ§  ëª¨ë¸ ìš©ëŸ‰ ë¶„ì„:")
        print(f"   d_model: {config.d_model}")
        print(f"   n_layers: {config.n_layers}")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        
        # ìš©ëŸ‰ ë¬¸ì œ ì§„ë‹¨
        if total_params > 20_000_000:  # 20M ì´ìƒ
            print(f"   âŒ ê³¼ì í•© ìœ„í—˜: íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë„ˆë¬´ ë§ìŒ")
            return "oversized"
        elif total_params < 1_000_000:  # 1M ë¯¸ë§Œ
            print(f"   âŒ ìš©ëŸ‰ ë¶€ì¡±: íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ")
            return "undersized"
        else:
            print(f"   âœ… ì ì ˆí•œ ëª¨ë¸ í¬ê¸°")
            return "optimal"
    
    def recommend_optimal_config(self):
        """ìµœì  ì„¤ì • ê¶Œì¥"""
        print(f"\nğŸ’¡ ìˆ˜ë ´ ê°œì„ ì„ ìœ„í•œ ìµœì  ì„¤ì •:")
        
        # ì‘ì€ ëª¨ë¸ ì„¤ì • (ê³¼ì í•© ë°©ì§€)
        optimal_config = PropertyPredictionConfig(
            d_model=256,        # 512 -> 256 (50% ê°ì†Œ)
            n_heads=8,          # ìœ ì§€
            n_layers=4,         # 6 -> 4 (33% ê°ì†Œ)
            d_ff=1024,          # 2048 -> 1024 (50% ê°ì†Œ)
            dropout=0.4,        # 0.3 -> 0.4 (ê³¼ì í•© ë°©ì§€)
            learning_rate=5e-4, # 1e-4 -> 5e-4 (í•™ìŠµ ì†ë„ ì¦ê°€)
            weight_decay=1e-2   # 1e-3 -> 1e-2 (ì •ê·œí™” ê°•í™”)
        )
        
        capacity_status = self.analyze_model_capacity(optimal_config)
        
        print(f"\nğŸ¯ ê¶Œì¥ í›ˆë ¨ ì„¤ì •:")
        print(f"   í•™ìŠµë¥ : {optimal_config.learning_rate} (ì´ˆê¸°ê°’ ì¦ê°€)")
        print(f"   ê°€ì¤‘ì¹˜ ê°ì‡ : {optimal_config.weight_decay} (ì •ê·œí™” ê°•í™”)")
        print(f"   Dropout: {optimal_config.dropout} (ê³¼ì í•© ë°©ì§€)")
        print(f"   ë°°ì¹˜ í¬ê¸°: 64 (ì•ˆì •ì„±)")
        print(f"   ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: 1.0")
        
        print(f"\nğŸ”§ ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜:")
        print(f"   Entanglement: 1.0")
        print(f"   Fidelity: 10.0 (ì¤‘ìš”ë„ ì¦ê°€)")
        print(f"   Expressibility: 0.05 (í° ê°’ì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ ê°ì†Œ)")
        
        print(f"\nâš™ï¸  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •:")
        print(f"   ReduceLROnPlateau: patience=3, factor=0.5")
        print(f"   Early stopping: patience=10")
        
        return optimal_config
    
    def diagnose_plateau_causes(self):
        """1.5 ì†ì‹¤ ì •ì²´ ì›ì¸ ì§„ë‹¨"""
        print(f"\nğŸ” ê²€ì¦ ì†ì‹¤ 1.5 ì •ì²´ ì›ì¸ ë¶„ì„:")
        
        causes = [
            {
                "ì›ì¸": "ëª¨ë¸ ê³¼ì í•©",
                "ì¦ìƒ": "í›ˆë ¨ ì†ì‹¤ì€ ê°ì†Œí•˜ì§€ë§Œ ê²€ì¦ ì†ì‹¤ ì •ì²´",
                "í•´ê²°ì±…": "ëª¨ë¸ í¬ê¸° ì¶•ì†Œ, Dropout ì¦ê°€, ì •ê·œí™” ê°•í™”"
            },
            {
                "ì›ì¸": "í•™ìŠµë¥  ë¶€ì ì ˆ",
                "ì¦ìƒ": "ì†ì‹¤ì´ íŠ¹ì • ê°’ì—ì„œ ì§„ë™",
                "í•´ê²°ì±…": "ì ì‘ì  í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©"
            },
            {
                "ì›ì¸": "ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜ ë¶ˆê· í˜•",
                "ì¦ìƒ": "ì¼ë¶€ ì†ì„±ë§Œ í•™ìŠµë˜ê³  ë‹¤ë¥¸ ì†ì„± ë¬´ì‹œ",
                "í•´ê²°ì±…": "ì†ì„±ë³„ ê°€ì¤‘ì¹˜ ì¬ì¡°ì •"
            },
            {
                "ì›ì¸": "ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ",
                "ì¦ìƒ": "ë…¸ì´ì¦ˆê°€ ë§ê±°ë‚˜ ì´ìƒì¹˜ ì¡´ì¬",
                "í•´ê²°ì±…": "ë°ì´í„° ì „ì²˜ë¦¬ ë° ì´ìƒì¹˜ ì œê±°"
            },
            {
                "ì›ì¸": "ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ë°œ",
                "ì¦ìƒ": "ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ í¼",
                "í•´ê²°ì±…": "ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ë° ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°œì„ "
            }
        ]
        
        for i, cause in enumerate(causes, 1):
            print(f"   {i}. {cause['ì›ì¸']}")
            print(f"      ì¦ìƒ: {cause['ì¦ìƒ']}")
            print(f"      í•´ê²°ì±…: {cause['í•´ê²°ì±…']}")
            print()
    
    def create_improved_model_config(self):
        """ê°œì„ ëœ ëª¨ë¸ ì„¤ì • ìƒì„±"""
        print(f"\nğŸš€ ê°œì„ ëœ Property Prediction ì„¤ì •:")
        
        # í•µì‹¬ ê°œì„ ì‚¬í•­ ì ìš©
        improved_config = PropertyPredictionConfig(
            # ëª¨ë¸ í¬ê¸° ìµœì í™” (ê³¼ì í•© ë°©ì§€)
            d_model=256,
            n_heads=8,
            n_layers=4,
            d_ff=1024,
            
            # ì •ê·œí™” ê°•í™”
            dropout=0.4,
            weight_decay=1e-2,
            
            # í•™ìŠµ ìµœì í™”
            learning_rate=5e-4,
            warmup_steps=500,
            
            # ë°°ì¹˜ ì„¤ì •
            train_batch_size=64,
            val_batch_size=64,
            
            # ì¶œë ¥ ì„¤ì • (robust_fidelity ì œê±°ë¨)
            property_dim=3  # entanglement, fidelity, expressibility
        )
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
        test_model = PropertyPredictionTransformer(improved_config)
        total_params = sum(p.numel() for p in test_model.parameters())
        
        print(f"   ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        print(f"   ê¸°ì¡´ ëŒ€ë¹„ ê°ì†Œìœ¨: {(58_607_446 - total_params) / 58_607_446 * 100:.1f}%")
        
        return improved_config


def main():
    """ë©”ì¸ ë¶„ì„ ì‹¤í–‰"""
    print("ğŸ” Property Prediction ìˆ˜ë ´ ë¬¸ì œ ì¢…í•© ë¶„ì„\n")
    
    analyzer = ConvergenceAnalyzer()
    
    # 1. í˜„ì¬ ì„¤ì • ë¶„ì„
    current_config = PropertyPredictionConfig()
    print("ğŸ“‹ í˜„ì¬ ì„¤ì • ë¶„ì„:")
    analyzer.analyze_model_capacity(current_config)
    
    # 2. ì •ì²´ ì›ì¸ ì§„ë‹¨
    analyzer.diagnose_plateau_causes()
    
    # 3. ìµœì  ì„¤ì • ê¶Œì¥
    optimal_config = analyzer.recommend_optimal_config()
    
    # 4. ê°œì„ ëœ ì„¤ì • ìƒì„±
    improved_config = analyzer.create_improved_model_config()
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"\nğŸ¯ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•´ê²°ì±…:")
    print(f"   1. ëª¨ë¸ í¬ê¸° ì¶•ì†Œ: d_model=256, n_layers=4")
    print(f"   2. ì •ê·œí™” ê°•í™”: dropout=0.4, weight_decay=1e-2")
    print(f"   3. í•™ìŠµë¥  ì¦ê°€: 5e-4 (ë¹ ë¥¸ ìˆ˜ë ´)")
    print(f"   4. ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¬ì¡°ì •: fidelity=10.0, expressibility=0.05")
    print(f"   5. ì ì‘ì  ìŠ¤ì¼€ì¤„ëŸ¬: ReduceLROnPlateau")
    
    return improved_config


if __name__ == "__main__":
    improved_config = main()
