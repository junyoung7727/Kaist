"""
Hyperparameter Grid Search for Decision Transformer
í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜ ë° ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ë¹„êµ ì‹œìŠ¤í…œ
"""

import torch
import torch.nn as nn
from pathlib import Path
import json
import argparse
from typing import Dict, Any, List, Tuple
import time
import numpy as np
from dataclasses import asdict, dataclass
import itertools
from datetime import datetime
import os
# import pandas as pd  # ì„ íƒì  ì˜ì¡´ì„±

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from src.models.decision_transformer import DecisionTransformer
from src.training.trainer import TrainingConfig, DecisionTransformerTrainer, QuantumCircuitCollator
from src.data.embedding_pipeline import EmbeddingPipeline, EmbeddingConfig
from src.data.quantum_circuit_dataset import DatasetManager, create_dataloaders
from utils.debug_utils import debug_print

# ğŸ† NEW: ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent / "quantumcommon"))
from gates import QuantumGateRegistry


@dataclass
class GridSearchConfig:
    """ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì • (í•µì‹¬ íŒŒë¼ë¯¸í„°ë§Œ)"""
    # í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„
    n_layers_options: List[int] = None
    attention_mode_options: List[str] = None
    
    # ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¼ ê³ ì •)
    d_model: int = 512
    n_heads: int = 8
    dropout: float = 0.1
    learning_rate: float = 1e-4
    
    # í›ˆë ¨ ì„¤ì •
    num_epochs: int = 10
    batch_size: int = 16
    max_experiments: int = 50  # ìµœëŒ€ ì‹¤í—˜ ìˆ˜ ì œí•œ
    
    # ë°ì´í„° ë° ì €ì¥ ê²½ë¡œ
    data_path: str = "data/dummy_experiment_results.json"
    results_dir: str = "grid_search_results"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    def __post_init__(self):
        """ê¸°ë³¸ê°’ ì„¤ì •"""
        if self.n_layers_options is None:
            self.n_layers_options = [4, 6, 8, 10]  # ë ˆì´ì–´ ìˆ˜ê°€ ê°€ì¥ ì¤‘ìš”
        if self.attention_mode_options is None:
            self.attention_mode_options = ["standard", "advanced", "hybrid"]


class HyperparameterGridSearch:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜ í´ë˜ìŠ¤"""
    
    def __init__(self, config: GridSearchConfig):
        self.config = config
        self.device = torch.device(config.device)
        self.results_dir = Path(config.results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        self.experiment_results = []
        self.best_config = None
        self.best_val_loss = float('inf')
        
        print(f"ğŸ”¬ Grid Search initialized")
        print(f"   ğŸ“ Results directory: {self.results_dir}")
        print(f"   ğŸ¯ Device: {self.device}")
    
    def generate_hyperparameter_combinations(self) -> List[Dict[str, Any]]:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„± (í•µì‹¬ íŒŒë¼ë¯¸í„°ë§Œ)"""
        param_names = ['n_layers', 'attention_mode']
        param_values = [
            self.config.n_layers_options,
            self.config.attention_mode_options
        ]
        
        # ëª¨ë“  ì¡°í•© ìƒì„±
        combinations = list(itertools.product(*param_values))
        
        # ì¡°í•©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        valid_combinations = []
        for combo in combinations:
            n_layers, attention_mode = combo
            config_dict = {
                'n_layers': n_layers,
                'attention_mode': attention_mode,
                # ê³ ì • íŒŒë¼ë¯¸í„° ì¶”ê°€
                'd_model': self.config.d_model,
                'n_heads': self.config.n_heads,
                'dropout': self.config.dropout,
                'learning_rate': self.config.learning_rate
            }
            valid_combinations.append(config_dict)
        
        # ìµœëŒ€ ì‹¤í—˜ ìˆ˜ ì œí•œ
        if len(valid_combinations) > self.config.max_experiments:
            print(f"âš ï¸ Too many combinations ({len(valid_combinations)}), limiting to {self.config.max_experiments}")
            # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì œí•œ
            import random
            random.shuffle(valid_combinations)
            valid_combinations = valid_combinations[:self.config.max_experiments]
        
        print(f"ğŸ“Š Generated {len(valid_combinations)} valid hyperparameter combinations")
        return valid_combinations
    
    def create_training_config(self, hyperparams: Dict[str, Any]) -> TrainingConfig:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œë¶€í„° TrainingConfig ìƒì„±"""
        return TrainingConfig(
            d_model=hyperparams['d_model'],  # ê³ ì •ê°’
            n_layers=hyperparams['n_layers'],  # ë³€ìˆ˜
            n_heads=hyperparams['n_heads'],  # ê³ ì •ê°’
            dropout=hyperparams['dropout'],  # ê³ ì •ê°’
            learning_rate=hyperparams['learning_rate'],  # ê³ ì •ê°’
            attention_mode=hyperparams['attention_mode'],  # ë³€ìˆ˜
            
            # ê³ ì • ì„¤ì •
            num_epochs=self.config.num_epochs,
            batch_size=self.config.batch_size,
            device=self.config.device,
            use_wandb=False,  # ê·¸ë¦¬ë“œ ì„œì¹˜ì—ì„œëŠ” wandb ë¹„í™œì„±í™”
            # n_gate_typesëŠ” TrainingConfig.__post_init__ì—ì„œ ìë™ ì„¤ì •ë¨
            
            # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ì„¤ì •
            eval_every=100,
            save_every=1000
        )
    
    def run_single_experiment(self, experiment_id: int, hyperparams: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"\nğŸ§ª Experiment {experiment_id + 1}")
        print(f"   ğŸ”§ Config: {hyperparams}")
        
        start_time = time.time()
        
        try:
            # í›ˆë ¨ ì„¤ì • ìƒì„±
            train_config = self.create_training_config(hyperparams)
            
            # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì„¤ì •
            embedding_config = EmbeddingConfig(
                d_model=train_config.d_model,
                n_gate_types=train_config.n_gate_types
            )
            embedding_pipeline = EmbeddingPipeline(embedding_config)
            
            # ë°ì´í„° ë¡œë” ìƒì„±
            dataset_manager = DatasetManager(self.config.data_path)
            train_loader, val_loader = create_dataloaders(
                dataset_manager=dataset_manager,
                embedding_pipeline=embedding_pipeline,
                batch_size=train_config.batch_size,
                train_split=0.8
            )
            
            # ëª¨ë¸ ìƒì„±
            model = DecisionTransformer(
                d_model=train_config.d_model,
                n_layers=train_config.n_layers,
                n_heads=train_config.n_heads,
                n_gate_types=train_config.n_gate_types,
                dropout=train_config.dropout,
                attention_mode=train_config.attention_mode
            )
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            trainer = DecisionTransformerTrainer(
                config=train_config,
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                save_dir=str(self.results_dir / f"experiment_{experiment_id}")
            )
            
            # í›ˆë ¨ ì‹¤í–‰
            trainer.train()
            
            # ìµœì¢… ê²€ì¦ ì„±ëŠ¥ ì¸¡ì •
            final_val_metrics = trainer.validate()
            
            experiment_time = time.time() - start_time
            
            # ê²°ê³¼ ì •ë¦¬
            result = {
                'experiment_id': experiment_id,
                'hyperparameters': hyperparams,
                'final_val_loss': final_val_metrics['val_loss'],
                'final_val_accuracy': final_val_metrics['val_accuracy'],
                'best_val_loss': trainer.best_val_loss,
                'total_steps': trainer.global_step,
                'experiment_time': experiment_time,
                'success': True,
                'error': None
            }
            
            print(f"   âœ… Success! Val Loss: {final_val_metrics['val_loss']:.4f}, Accuracy: {final_val_metrics['val_accuracy']:.4f}")
            
            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            if final_val_metrics['val_loss'] < self.best_val_loss:
                self.best_val_loss = final_val_metrics['val_loss']
                self.best_config = hyperparams.copy()
                print(f"   ğŸ† New best configuration! Val Loss: {self.best_val_loss:.4f}")
            
        except Exception as e:
            experiment_time = time.time() - start_time
            print(f"   âŒ Failed: {str(e)}")
            
            result = {
                'experiment_id': experiment_id,
                'hyperparameters': hyperparams,
                'final_val_loss': float('inf'),
                'final_val_accuracy': 0.0,
                'best_val_loss': float('inf'),
                'total_steps': 0,
                'experiment_time': experiment_time,
                'success': False,
                'error': str(e)
            }
        
        return result
    
    def run_grid_search(self) -> Dict[str, Any]:
        """ì „ì²´ ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰"""
        print(f"ğŸš€ Starting Hyperparameter Grid Search")
        print(f"   ğŸ“… Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
        combinations = self.generate_hyperparameter_combinations()
        
        total_start_time = time.time()
        
        # ê° ì¡°í•©ì— ëŒ€í•´ ì‹¤í—˜ ì‹¤í–‰
        for i, hyperparams in enumerate(combinations):
            result = self.run_single_experiment(i, hyperparams)
            self.experiment_results.append(result)
            
            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            if (i + 1) % 5 == 0:
                self.save_intermediate_results()
        
        total_time = time.time() - total_start_time
        
        # ìµœì¢… ê²°ê³¼ ì •ë¦¬
        final_results = {
            'config': asdict(self.config),
            'total_experiments': len(combinations),
            'successful_experiments': sum(1 for r in self.experiment_results if r['success']),
            'failed_experiments': sum(1 for r in self.experiment_results if not r['success']),
            'total_time': total_time,
            'best_config': self.best_config,
            'best_val_loss': self.best_val_loss,
            'all_results': self.experiment_results,
            'timestamp': datetime.now().isoformat()
        }
        
        # ê²°ê³¼ ì €ì¥
        self.save_final_results(final_results)
        
        # ìš”ì•½ ì¶œë ¥
        self.print_summary(final_results)
        
        return final_results
    
    def save_intermediate_results(self):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        intermediate_path = self.results_dir / "intermediate_results.json"
        with open(intermediate_path, 'w') as f:
            json.dump(self.experiment_results, f, indent=2)
        print(f"ğŸ’¾ Intermediate results saved to {intermediate_path}")
    
    def save_final_results(self, results: Dict[str, Any]):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        # JSON ì €ì¥
        json_path = self.results_dir / "final_results.json"
        with open(json_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # CSV ì €ì¥ (ë¶„ì„ìš©)
        csv_path = self.results_dir / "results_summary.csv"
        self.save_results_as_csv(csv_path)
        
        print(f"ğŸ’¾ Final results saved to:")
        print(f"   ğŸ“„ JSON: {json_path}")
        print(f"   ğŸ“Š CSV: {csv_path}")
    
    def save_results_as_csv(self, csv_path: Path):
        """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        try:
            import pandas as pd
            rows = []
            for result in self.experiment_results:
                if result['success']:
                    row = {
                        'experiment_id': result['experiment_id'],
                        'final_val_loss': result['final_val_loss'],
                        'final_val_accuracy': result['final_val_accuracy'],
                        'best_val_loss': result['best_val_loss'],
                        'experiment_time': result['experiment_time'],
                        **result['hyperparameters']
                    }
                    rows.append(row)
            
            if rows:
                df = pd.DataFrame(rows)
                df.to_csv(csv_path, index=False)
        except ImportError:
            print("âš ï¸ pandas not available, skipping CSV export. Install with: pip install pandas")
            # ëŒ€ì‹  ê°„ë‹¨í•œ CSV ìˆ˜ë™ ìƒì„±
            self._save_simple_csv(csv_path)
    
    def _save_simple_csv(self, csv_path: Path):
        """
pandas ì—†ì´ ê°„ë‹¨í•œ CSV ì €ì¥"""
        successful_results = [r for r in self.experiment_results if r['success']]
        if not successful_results:
            return
        
        # í—¤ë” ìƒì„±
        headers = ['experiment_id', 'final_val_loss', 'final_val_accuracy', 'best_val_loss', 'experiment_time']
        param_keys = list(successful_results[0]['hyperparameters'].keys())
        headers.extend(param_keys)
        
        with open(csv_path, 'w') as f:
            # í—¤ë” ì“°ê¸°
            f.write(','.join(headers) + '\n')
            
            # ë°ì´í„° ì“°ê¸°
            for result in successful_results:
                row = [
                    str(result['experiment_id']),
                    str(result['final_val_loss']),
                    str(result['final_val_accuracy']),
                    str(result['best_val_loss']),
                    str(result['experiment_time'])
                ]
                for key in param_keys:
                    row.append(str(result['hyperparameters'][key]))
                f.write(','.join(row) + '\n')
    
    def print_summary(self, results: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“ˆ GRID SEARCH SUMMARY")
        print("="*60)
        
        print(f"ğŸ”¬ Total Experiments: {results['total_experiments']}")
        print(f"âœ… Successful: {results['successful_experiments']}")
        print(f"âŒ Failed: {results['failed_experiments']}")
        print(f"â±ï¸ Total Time: {results['total_time']/3600:.2f} hours")
        
        if results['best_config']:
            print(f"\nğŸ† BEST CONFIGURATION:")
            print(f"   ğŸ“Š Validation Loss: {results['best_val_loss']:.4f}")
            for key, value in results['best_config'].items():
                print(f"   ğŸ”§ {key}: {value}")
        
        # ìƒìœ„ 5ê°œ ê²°ê³¼
        successful_results = [r for r in self.experiment_results if r['success']]
        if successful_results:
            top_5 = sorted(successful_results, key=lambda x: x['final_val_loss'])[:5]
            print(f"\nğŸ¥‡ TOP 5 CONFIGURATIONS:")
            for i, result in enumerate(top_5, 1):
                print(f"   {i}. Loss: {result['final_val_loss']:.4f} | "
                      f"Attention: {result['hyperparameters']['attention_mode']} | "
                      f"d_model: {result['hyperparameters']['d_model']} | "
                      f"n_layers: {result['hyperparameters']['n_layers']}")
        
        # ì–´í…ì…˜ ëª¨ë“œë³„ ì„±ëŠ¥ ë¹„êµ
        self.analyze_attention_performance()
    
    def analyze_attention_performance(self):
        """ì–´í…ì…˜ ëª¨ë“œë³„ ì„±ëŠ¥ ë¶„ì„"""
        successful_results = [r for r in self.experiment_results if r['success']]
        if not successful_results:
            return
        
        attention_stats = {}
        for result in successful_results:
            mode = result['hyperparameters']['attention_mode']
            if mode not in attention_stats:
                attention_stats[mode] = []
            attention_stats[mode].append(result['final_val_loss'])
        
        print(f"\nğŸ¯ ATTENTION MODE PERFORMANCE:")
        for mode, losses in attention_stats.items():
            avg_loss = np.mean(losses)
            std_loss = np.std(losses)
            best_loss = min(losses)
            print(f"   {mode.upper()}: Avg {avg_loss:.4f}Â±{std_loss:.4f}, Best {best_loss:.4f} ({len(losses)} experiments)")


def main():
    parser = argparse.ArgumentParser(description="Hyperparameter Grid Search for Decision Transformer")
    parser.add_argument("--data_path", type=str, required=True, help="Path to training data")
    parser.add_argument("--results_dir", type=str, default="grid_search_results", help="Results directory")
    parser.add_argument("--num_epochs", type=int, default=10, help="Number of epochs per experiment")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size")
    parser.add_argument("--max_experiments", type=int, default=50, help="Maximum number of experiments")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    
    # í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •
    parser.add_argument("--n_layers", nargs='+', type=int, default=[4, 6, 8], help="n_layers options (core parameter)")
    parser.add_argument("--attention_modes", nargs='+', type=str, default=["standard", "advanced", "hybrid"], help="attention mode options (core parameter)")
    
    # ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° (í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
    parser.add_argument("--d_model", type=int, default=512, help="d_model (fixed)")
    parser.add_argument("--n_heads", type=int, default=8, help="n_heads (fixed)")
    parser.add_argument("--dropout", type=float, default=0.1, help="dropout (fixed)")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="learning rate (fixed)")
    
    args = parser.parse_args()
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜ ì„¤ì •
    config = GridSearchConfig(
        n_layers_options=args.n_layers,
        attention_mode_options=args.attention_modes,
        
        # ê³ ì • íŒŒë¼ë¯¸í„°
        d_model=args.d_model,
        n_heads=args.n_heads,
        dropout=args.dropout,
        learning_rate=args.learning_rate,
        
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        max_experiments=args.max_experiments,
        data_path=args.data_path,
        results_dir=args.results_dir,
        device=args.device
    )
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
    grid_search = HyperparameterGridSearch(config)
    results = grid_search.run_grid_search()
    
    print(f"\nğŸ‰ Grid search completed! Results saved to {config.results_dir}")


if __name__ == "__main__":
    main()
