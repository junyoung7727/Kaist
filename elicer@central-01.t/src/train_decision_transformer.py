"""
Decision Transformer Training Script
ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ - ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ì‚¬ìš©ë²•
"""

import argparse
import torch
from pathlib import Path
import sys
import os
from dotenv import load_dotenv

load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ (src í´ë”ë¥¼ ìµœìƒìœ„ íŒ¨í‚¤ì§€ë¡œ ì¸ì‹)
src_path = Path(__file__).parent
sys.path.append(str(src_path))

# ìƒìœ„ ë””ë ‰í† ë¦¬ë„ ì¶”ê°€ (OAT_Model í´ë”)
parent_path = src_path.parent
sys.path.append(str(parent_path))

# ì„í¬íŠ¸
from src.training.trainer import DecisionTransformerTrainer, TrainingConfig, set_seed, QuantumCircuitCollator
from src.data.quantum_circuit_dataset import DatasetManager, create_dataloaders
from src.data.embedding_pipeline import create_embedding_pipeline, EmbeddingConfig
from src.models.decision_transformer import create_decision_transformer


def main():
    parser = argparse.ArgumentParser(description="Train Decision Transformer for Quantum Circuits")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--path", type=str, 
                       default=r"C:\Users\jungh\Documents\GitHub\Kaist\OAT_Model\raw_data\merged_data.json",
                       help="Path to unified data JSON file (contains both merged_results and merged_circuits)")
    # ë ˆê±°ì‹œ ì§€ì›ì„ ìœ„í•œ ê°œë³„ íŒŒì¼ ê²½ë¡œ
    parser.add_argument("--merged_results_path", type=str, 
                       default=None,
                       help="Path to merged results JSON file (legacy support)")
    parser.add_argument("--circuits_path", type=str, 
                       default=None,
                       help="Path to circuit specifications JSON file (legacy support)")
    parser.add_argument("--enable_filtering", action="store_true", default=True,
                       help="Enable data quality filtering (remove invalid expressibility data)")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--d_model", type=int, default=512, help="Model dimension")
    parser.add_argument("--n_layers", type=int, default=6, help="Number of transformer layers")
    parser.add_argument("--n_heads", type=int, default=8, help="Number of attention heads")
    parser.add_argument("--n_gate_types", type=int, default=20, help="Number of gate types")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout rate")
    
    # í•™ìŠµ ì„¤ì • (ìµœì í™”ë¨)
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=3e-4, help="Learning rate (ìµœì í™”: 1e-4 â†’ 3e-4)")
    parser.add_argument("--num_epochs", type=int, default=20, help="Number of epochs (ìµœì í™”: 1 â†’ 20)")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="Weight decay")
    
    # ê¸°íƒ€ ì„¤ì •
    parser.add_argument("--device", type=str, default="auto", help="Device (cuda/cpu/auto)")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--save_dir", type=str, default="./checkpoints", help="Checkpoint save directory")
    
    # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    parser.add_argument("--use_amp", action="store_true", default=True, help="Use Automatic Mixed Precision")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="Gradient accumulation steps")
    parser.add_argument("--gradient_checkpointing", action="store_true", default=True, help="Enable gradient checkpointing")
    parser.add_argument("--memory_cleanup_interval", type=int, default=50, help="Memory cleanup interval (batches)")
    
    # ë¡œê¹… ì„¤ì •
    parser.add_argument("--use_wandb", action="store_true",default=True, help="Use Weights & Biases logging")
    parser.add_argument("--project_name", type=str, default="quantum-decision-transformer", help="W&B project name")
    parser.add_argument("--run_name", type=str, default=None, help="W&B run name")
    
    # ë°ì´í„°ì…‹ ë¶„í•  ì„¤ì •
    parser.add_argument("--train_ratio", type=float, default=0.7, help="Training data ratio")
    parser.add_argument("--val_ratio", type=float, default=0.15, help="Validation data ratio")
    parser.add_argument("--test_ratio", type=float, default=0.15, help="Test data ratio")
    
    args = parser.parse_args()
    
    # WANDB_API_KEY í™˜ê²½ë³€ìˆ˜ ê²€ì‚¬
    if args.use_wandb and not os.environ.get("WANDB_API_KEY"):
        print("Warning: WANDB_API_KEY environment variable not set. Disabling wandb logging.")
        args.use_wandb = False
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    print("=" * 60)
    print("ğŸš€ Quantum Decision Transformer Training")
    print("=" * 60)
    print(f"Device: {device}")
    if args.path:
        print(f"Unified data path: {args.path}")
    print(f"Data filtering: {args.enable_filtering}")
    print(f"Model: d_model={args.d_model}, layers={args.n_layers}, heads={args.n_heads}")
    print(f"Training: batch_size={args.batch_size}, epochs={args.num_epochs}, lr={args.learning_rate}")
    print("=" * 60)
    
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)
    
    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    print("ğŸ“Š Loading and preparing dataset...")
    if args.path:
        manager = DatasetManager(
            unified_data_path=args.path
        )
    
    # ë°ì´í„° ë³‘í•© ë° í’ˆì§ˆ í•„í„°ë§ (ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨)
    circuit_data = manager.merge_data(enable_filtering=args.enable_filtering)
    stats = manager.get_dataset_stats()
    
    print(f"âœ… Dataset loaded successfully!")
    print(f"   Total valid circuits: {stats['total_circuits']}")
    print(f"   Qubit range: {stats['qubit_range']}")
    print(f"   Gate range: {stats['gate_range']}")
    print(f"   Fidelity range: {stats['fidelity_range']}")
    if 'entanglement_range' in stats:
        print(f"   Entanglement range: {stats['entanglement_range']}")
    train_dataset, val_dataset, test_dataset = manager.split_dataset(
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio
    )
    
    print(f"ğŸ“Š Dataset split:")
    print(f"   Train: {len(train_dataset)} circuits")
    print(f"   Validation: {len(val_dataset)} circuits")
    print(f"   Test: {len(test_dataset)} circuits")
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸
    if len(train_dataset) > 0:
        sample = train_dataset[0]
        print(f"\nğŸ“‹ Sample circuit info:")
        print(f"   Circuit ID: {sample.circuit_id}")
        print(f"   Qubits: {sample.num_qubits}, Gates: {len(sample.gates)}")
        print(f"   Fidelity: {sample.measurement_result.fidelity:.4f}")
        print(f"   Entanglement: {sample.measurement_result.entanglement:.4f}")
        if sample.measurement_result.expressibility:
            expr = sample.measurement_result.expressibility
            print(f"   Expressibility: {expr.get('expressibility', 'N/A'):.4f}")
            print(f"   KL Divergence: {expr.get('kl_divergence', 'N/A'):.4f}")
    
    # 2. ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì„¤ì •
    print("\nğŸ”§ Setting up embedding pipeline...")
    embed_config = EmbeddingConfig(
        d_model=args.d_model,
        n_gate_types=args.n_gate_types,
        max_seq_len=2000
    )
    

    embedding_pipeline = create_embedding_pipeline(embed_config)
    print("âœ… Embedding pipeline created successfully!")


    # 3. ë°ì´í„°ë¡œë” ìƒì„± (ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨)
    print("\nğŸ“¦ Creating data loaders...")
    train_loader, val_loader, test_loader = create_dataloaders(
        train_dataset, val_dataset, test_dataset,
        batch_size=args.batch_size,
        num_workers=0  # ğŸš€ FIX: RLock pickle ì˜¤ë¥˜ ë°©ì§€ (ìºì‹± ì‹œìŠ¤í…œê³¼ ë©€í‹°í”„ë¡œì„¸ì‹± ì¶©ëŒ)
    )
    
    # ì½œë ˆì´í„° ì„¤ì •
    collator = QuantumCircuitCollator(embedding_pipeline)
    train_loader.collate_fn = collator
    val_loader.collate_fn = collator
    
    print("âœ… Data loaders created successfully!")
    
    # 4. ëª¨ë¸ ìƒì„± (ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨)
    print("\nğŸ¤– Creating Decision Transformer model...")
    model = create_decision_transformer(
        d_model=args.d_model,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        n_gate_types=args.n_gate_types,
        dropout=args.dropout
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print("âœ… Model created successfully!")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    
    # 5. í•™ìŠµ ì„¤ì •
    print("\nâš™ï¸ Setting up training configuration...")
    config = TrainingConfig(
        d_model=args.d_model,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        n_gate_types=args.n_gate_types,
        dropout=args.dropout,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        batch_size=args.batch_size,
        num_epochs=args.num_epochs,
        device=device,
        seed=args.seed,
        use_wandb=args.use_wandb,
        project_name=args.project_name,
        run_name=args.run_name,
        # ğŸš€ AMP ì„¤ì • ëª…ì‹œì  ì¶”ê°€
        use_amp=False,  # Mixed Precision í™œì„±í™”
        gradient_accumulation_steps=1,
        gradient_checkpointing=True
    )
    
    # 6. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ ì‹œì‘ (ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨)
    print("\nğŸ¯ Starting training...")
    try:
        trainer = DecisionTransformerTrainer(
            model=model,
            train_dataloader=train_loader,
            val_dataloader=val_loader,
            config=config,
            embedding_pipeline=embedding_pipeline
        )
        
        # í•™ìŠµ ì‹œì‘
        trainer.train()
        
        print("\nğŸ‰ Training completed successfully!")
        print(f"ğŸ’¾ Checkpoints saved in: {args.save_dir}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Training interrupted by user")


if __name__ == "__main__":
    main()
