"""
Embedding Pipeline Module
CircuitSpec -> Grid Encoder -> Decision Transformer Embedding
"""

import torch
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
import hashlib
import json
import pickle
from pathlib import Path
import threading
import time
import sys
from utils.debug_utils import debug_print

# ğŸš€ DEBUG ëª¨ë“œ ì„¤ì •
DEBUG_MODE = False  # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ê¸°ë³¸ê°’ False

# ì„í¬íŠ¸ ê²½ë¡œ ë¬¸ì œ í•´ê²°
try:
    # ì ˆëŒ€ ê²½ë¡œ ì‹œë„
    from .quantum_circuit_dataset import CircuitSpec
    from ..encoding.grid_graph_encoder import GridGraphEncoder
    from ..encoding.Decision_Transformer_Embed import QuantumGateSequenceEmbedding
except ImportError:
    # ìƒëŒ€ ê²½ë¡œ ì‹œë„
    try:
        from .quantum_circuit_dataset import CircuitSpec
        from ..encoding.grid_graph_encoder import GridGraphEncoder
        from ..encoding.Decision_Transformer_Embed import QuantumGateSequenceEmbedding
    except ImportError:
        # ë¡œì»¬ ê²½ë¡œ ì‹œë„
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent.parent))
        from data.quantum_circuit_dataset import CircuitSpec
        from encoding.grid_graph_encoder import GridGraphEncoder
        from encoding.Decision_Transformer_Embed import QuantumGateSequenceEmbedding

# ğŸ† NEW: ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent.parent.parent / "quantumcommon"))
from gates import QuantumGateRegistry

@dataclass
class BatchMetadata:
    """ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œìš© ë°ì´í„° í´ë˜ìŠ¤"""
    circuit_ids: List[str]
    num_qubits: List[int]
    num_gates: List[int]
    batch_size: int
    timestamp: float

@dataclass
class EmbeddingConfig:
    """ì„ë² ë”© ì„¤ì •"""
    d_model: int = 512
    n_gate_types: int = None  # NEW: gate vocab ì‹±ê¸€í†¤ì—ì„œ ìë™ ì„¤ì •
    n_qubits: int = 10
    max_seq_len: int = 1000
    max_time_steps: int = 50

    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ gate ìˆ˜ë¥¼ ì‹±ê¸€í†¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        if self.n_gate_types is None:
            self.n_gate_types = QuantumGateRegistry.get_singleton_gate_count()
            print(f" EmbeddingConfig: Using gate vocab singleton, n_gate_types = {self.n_gate_types}")


class EmbeddingPipeline:
    """ğŸš€ ì™„ì „í•œ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ (ìºì‹± ì‹œìŠ¤í…œ í†µí•©)"""
    
    def __init__(self, config: EmbeddingConfig, enable_cache: bool = True):
        self.config = config
        self.enable_cache = enable_cache
        
        # ğŸš€ ìºì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._memory_cache = {}
        self._cache_access_order = []
        self._cache_lock = threading.RLock()
        self._max_cache_size = 1000
        self._cache_stats = {'hits': 0, 'misses': 0, 'total': 0}
        
        # ğŸš€ NEW: ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œ ì´ˆê¸°í™”
        self._batch_metadata_cache = {}
        self._batch_cache_access_order = []
        self._max_batch_cache_size = 800
        self._batch_cache_stats = {'hits': 0, 'misses': 0, 'total': 0}
        
        # ê²Œì´íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹±ê¸€í†¤ ì´ˆê¸°í™”
        self.gate_registry = QuantumGateRegistry()
        
        # ê²Œì´íŠ¸ vocab ì´ˆê¸°í™”
        self.gate_vocab = self.gate_registry.get_gate_vocab()
        
        # ê²Œì´íŠ¸ ìˆ˜ í™•ì¸ ë° ì„¤ì • ë™ê¸°í™”
        actual_gate_count = len(self.gate_vocab)
        if self.config.n_gate_types != actual_gate_count:
            print(f" Config mismatch: expected {self.config.n_gate_types}, got {actual_gate_count}")
            self.config.n_gate_types = actual_gate_count
        print(f" ğŸš€ EmbeddingPipeline initialized with {actual_gate_count} gate types (Cache: {enable_cache})")
        
        # Grid Encoder ì´ˆê¸°í™”
        self.grid_encoder = GridGraphEncoder()
        
        # Decision Transformer Embedding ì´ˆê¸°í™”
        self.dt_embedding = QuantumGateSequenceEmbedding(
            d_model=config.d_model,
            n_gate_types=config.n_gate_types,
            n_qubits=config.n_qubits,
            max_seq_len=config.max_seq_len
        )
    
    def _generate_cache_key(self, circuit_spec: CircuitSpec) -> str:
        """ğŸš€ íšŒë¡œ ìŠ¤í™ìœ¼ë¡œë¶€í„° ê³ ìœ í•œ ìºì‹œ í‚¤ ìƒì„±"""
        # ğŸš€ GateOperation ê°ì²´ë“¤ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_gates = []
        for gate in circuit_spec.gates:
            if hasattr(gate, '__dict__'):
                # GateOperation ê°ì²´ì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                gate_dict = {
                    'gate_type': getattr(gate, 'gate_type', str(gate)),
                    'qubits': getattr(gate, 'qubits', []),
                    'parameters': getattr(gate, 'parameters', [])
                }
                serializable_gates.append(gate_dict)
            else:
                # ì´ë¯¸ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœì¸ ê²½ìš°
                serializable_gates.append(str(gate))
        
        circuit_data = {
            'circuit_id': circuit_spec.circuit_id,
            'gates': serializable_gates,
            'num_qubits': circuit_spec.num_qubits,
            'depth': circuit_spec.depth,
            'd_model': self.config.d_model,
            'max_seq_len': self.config.max_seq_len
        }
        data_str = json.dumps(circuit_data, sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, torch.Tensor]]:
        """ğŸš€ ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        if not self.enable_cache:
            return None
            
        with self._cache_lock:
            self._cache_stats['total'] += 1
            
            if cache_key in self._memory_cache:
                cached_data = self._memory_cache[cache_key]
                
                # ğŸš€ NEW: ìºì‹œëœ ë°ì´í„°ì— SAR ê¸°ë°˜ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìºì‹œ ë¬´íš¨í™” (í˜¸í™˜ì„± ë³´ì¥)
                needs_invalidation = False
                
                # num_gatesê°€ ì—†ê±°ë‚˜ sar_sequence_lenì´ ì—†ìœ¼ë©´ ë¬´íš¨í™”
                if 'num_gates' not in cached_data or 'sar_sequence_len' not in cached_data:
                    needs_invalidation = True
                    print(f"âš ï¸  ìºì‹œ ë¬´íš¨í™”: SAR ë©”íƒ€ë°ì´í„° ëˆ„ë½ - {cache_key}")
                
                # original_gate_countê°€ ì—†ìœ¼ë©´ ë¬´íš¨í™” (ìƒˆë¡œìš´ êµ¬ì¡°)
                elif 'original_gate_count' not in cached_data:
                    needs_invalidation = True
                    print(f"âš ï¸  ìºì‹œ ë¬´íš¨í™”: êµ¬ë²„ì „ ë©”íƒ€ë°ì´í„° êµ¬ì¡° - {cache_key}")
                
                if needs_invalidation:
                    # ì˜¤ë˜ëœ ìºì‹œ ë°ì´í„° - ì¬ê³„ì‚° í•„ìš”
                    del self._memory_cache[cache_key]
                    if cache_key in self._cache_access_order:
                        self._cache_access_order.remove(cache_key)
                    self._cache_stats['misses'] += 1
                    return None
                
                # ìºì‹œ íˆíŠ¸
                self._update_cache_access(cache_key)
                self._cache_stats['hits'] += 1
                return cached_data
            
            # ìºì‹œ ë¯¸ìŠ¤
            self._cache_stats['misses'] += 1
            return None
    
    def _put_to_cache(self, cache_key: str, data: Dict[str, torch.Tensor]) -> None:
        """ğŸš€ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
        if not self.enable_cache:
            return
            
        with self._cache_lock:
            # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
            if len(self._memory_cache) >= self._max_cache_size:
                # LRU ì •ì±…: ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = self._cache_access_order.pop(0)
                del self._memory_cache[oldest_key]
            
            self._memory_cache[cache_key] = data
            self._update_cache_access(cache_key)
    
    def _update_cache_access(self, cache_key: str) -> None:
        """ğŸš€ ìºì‹œ ì ‘ê·¼ ìˆœì„œ ì—…ë°ì´íŠ¸ (LRU)"""
        if cache_key in self._cache_access_order:
            self._cache_access_order.remove(cache_key)
        self._cache_access_order.append(cache_key)
    
    def process_single_circuit(self, circuit_spec: CircuitSpec) -> Dict[str, torch.Tensor]:
        """ğŸš€ ë‹¨ì¼ íšŒë¡œ ì²˜ë¦¬ (ìºì‹± ì ìš©)"""
        
        # ğŸš€ ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(circuit_spec)
        
        # ğŸš€ ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            return cached_result
        
        # ğŸš€ ìºì‹œ ë¯¸ìŠ¤ - ìƒˆë¡œ ê³„ì‚°
        start_time = time.time()
        
        # 1. Grid Encoderë¡œ íšŒë¡œ ì¸ì½”ë”©
        encoded_data = self.grid_encoder.encode(circuit_spec)
        
        # 2. ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ ê·¸ë¦¬ë“œ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜
        grid_matrix_data = self.grid_encoder.to_grid_matrix(encoded_data)
        
        # 3. ğŸš€ NEW: ìˆœìˆ˜ ê²Œì´íŠ¸ ìˆ˜ ê¸°ë°˜ ê°„ë‹¨í•œ Decision Transformer Embedding ì ìš©
        original_gate_count = len(circuit_spec.gates)
        # íŒ¨ë”© ê¸¸ì´ëŠ” ë°°ì¹˜ ì²˜ë¦¬ ì‹œì ì—ì„œ ê²°ì •ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” None
        dt_results = self.dt_embedding.process_grid_matrix_data_simple(grid_matrix_data, original_gate_count, max_seq_len=None)
        
        # 4. ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì´ë¯¸ ì˜¬ë°”ë¥¸ ê²Œì´íŠ¸ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë¨)
        
        # SAR ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” ë””ë²„ê¹…ìš©ìœ¼ë¡œë§Œ ë³´ì¡´
        sar_sequence_len = dt_results.get('sar_sequence_len', original_gate_count * 3)
        if hasattr(sar_sequence_len, 'item'):  # í…ì„œì¸ ê²½ìš° ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
            sar_sequence_len = sar_sequence_len.item()
        
        dt_results.update({
            'circuit_id': circuit_spec.circuit_id,
            'num_qubits': circuit_spec.num_qubits,
            'num_gates': original_gate_count,  # ğŸ”§ ì‹¤ì œ ì•¡ì…˜ ìˆ˜ (ì›ë˜ ê²Œì´íŠ¸ ìˆ˜) ì‚¬ìš©
            'original_gate_count': original_gate_count,  # ì›ë³¸ ê²Œì´íŠ¸ ìˆ˜
            'sar_sequence_len': sar_sequence_len  # SAR ì‹œí€€ìŠ¤ ê¸¸ì´ (ë””ë²„ê¹…ìš©)
        })
        
        compute_time = time.time() - start_time
        
        # ğŸš€ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        self._put_to_cache(cache_key, dt_results)
        
        # ğŸ”‡ DEBUG: print(f"âš¡ ìƒˆë¡œ ê³„ì‚°: {circuit_spec.circuit_id} ({compute_time:.3f}s)")
        
        return dt_results
    
    def process_batch(self, circuit_specs: List[CircuitSpec]) -> Dict[str, torch.Tensor]:
        """ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ (ìºì‹± ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì ìš©)"""
        batch_size = len(circuit_specs)
        
        if batch_size == 0:
            return {}
        
        debug_print(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {batch_size}ê°œ íšŒë¡œ")
        
        # ğŸš€ NEW: ë°°ì¹˜ ë‚´ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚° (íŒ¨ë”©ìš©)
        max_gate_count = max(len(spec.gates) for spec in circuit_specs)
        max_seq_len = max_gate_count * 3 + 1  # SAR íŒ¨í„´ + EOS
        debug_print(f"ğŸš€ ë°°ì¹˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_seq_len} (ìµœëŒ€ ê²Œì´íŠ¸ ìˆ˜: {max_gate_count})")
        
        # ê° íšŒë¡œë¥¼ ê°œë³„ ì²˜ë¦¬ (ìµœëŒ€ ê¸¸ì´ ì „ë‹¬)
        batch_results = []
        for i, circuit_spec in enumerate(circuit_specs):
            debug_print(f"  íšŒë¡œ {i+1}/{batch_size} ì²˜ë¦¬ ì¤‘... (ê²Œì´íŠ¸ ìˆ˜: {len(circuit_spec.gates)})")
            result = self._process_single_circuit_with_padding(circuit_spec, max_seq_len)
            batch_results.append(result)
        
        # 3. ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸° (ìµœëŒ€ ê²Œì´íŠ¸ ìˆ˜ ê¸°ì¤€ íŒ¨ë”©)
        return self._combine_batch_results_simple(batch_results, max_gate_count)
    
    def _combine_batch_results_simple(self, batch_results: List[Dict[str, torch.Tensor]], max_gate_count: int) -> Dict[str, torch.Tensor]:
        """ğŸš€ NEW: ìˆœìˆ˜ ê²Œì´íŠ¸ ìˆ˜ ê¸°ë°˜ ê°„ë‹¨í•œ ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸°"""
        
        if not batch_results:
            raise ValueError("âŒ ë¹ˆ ë°°ì¹˜ ê²°ê³¼ì…ë‹ˆë‹¤!")
        
        batch_size = len(batch_results)
        combined = {}
        
        # 1. ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        meta_keys = ['circuit_id', 'num_qubits', 'num_gates']
        for key in meta_keys:
            combined[key] = [result[key] for result in batch_results]
        
        # 2. ë°°ì¹˜ ë‚´ ìµœëŒ€ SAR ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
        max_sar_len = max_gate_count * 3 + 1  # EOS í¬í•¨
        
        print(f"ğŸš€ ë°°ì¹˜ íŒ¨ë”©: ìµœëŒ€ ê²Œì´íŠ¸ ìˆ˜ {max_gate_count} â†’ ìµœëŒ€ SAR ê¸¸ì´ {max_sar_len}")
        
        # 3. í…ì„œ íŒ¨ë”© ë° ìŠ¤íƒ
        tensor_keys = ['input_sequence', 'attention_mask', 'action_prediction_mask', 'target_actions', 'target_qubits', 'target_params']
        
        for key in tensor_keys:
            tensors = [result[key] for result in batch_results]
            
            # ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©
            padded_tensors = []
            for tensor in tensors:
                current_len = tensor.shape[0]
                if current_len < max_sar_len:
                    # íŒ¨ë”© í•„ìš”
                    if key == 'input_sequence':
                        # [seq_len, d_model] â†’ [max_sar_len, d_model]
                        pad_len = max_sar_len - current_len
                        padding = torch.zeros(pad_len, tensor.shape[1], device=tensor.device, dtype=tensor.dtype)
                        padded_tensor = torch.cat([tensor, padding], dim=0)
                    elif key == 'attention_mask':
                        # [seq_len, seq_len] â†’ [max_sar_len, max_sar_len]
                        padded_tensor = torch.zeros(max_sar_len, max_sar_len, device=tensor.device, dtype=torch.bool)
                        padded_tensor[:current_len, :current_len] = tensor
                    elif key == 'action_prediction_mask':
                        # [seq_len] â†’ [max_sar_len]
                        padded_tensor = torch.zeros(max_sar_len, device=tensor.device, dtype=torch.bool)
                        padded_tensor[:current_len] = tensor
                    elif key == 'target_qubits':
                        # [actual_gate_count, 2] â†’ [max_sar_len, 2]
                        pad_len = max_sar_len - current_len
                        padding = torch.full((pad_len, tensor.shape[1]), -1, device=tensor.device, dtype=tensor.dtype)
                        padded_tensor = torch.cat([tensor, padding], dim=0)
                    elif key in ['target_actions', 'target_params']:
                        # [actual_gate_count] â†’ [max_sar_len]
                        pad_len = max_sar_len - current_len
                        if key == 'target_actions':
                            padding = torch.full((pad_len,), -1, device=tensor.device, dtype=tensor.dtype)
                        else:  # target_params
                            padding = torch.zeros(pad_len, device=tensor.device, dtype=tensor.dtype)
                        padded_tensor = torch.cat([tensor, padding], dim=0)
                    else:
                        # Default padding for other tensors
                        pad_len = max_sar_len - current_len
                        if len(tensor.shape) == 1:
                            padding = torch.zeros(pad_len, device=tensor.device, dtype=tensor.dtype)
                        else:
                            padding_shape = (pad_len,) + tensor.shape[1:]
                            padding = torch.zeros(padding_shape, device=tensor.device, dtype=tensor.dtype)
                        padded_tensor = torch.cat([tensor, padding], dim=0)
                else:
                    padded_tensor = tensor
                
                padded_tensors.append(padded_tensor)
            
            # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ ìŠ¤íƒ
            combined[key] = torch.stack(padded_tensors, dim=0)
        
        # 4. ê°œë³„ íšŒë¡œë³„ ì•¡ì…˜ ë§ˆìŠ¤í¬ ì¡°ì •
        if 'action_prediction_mask' in combined and 'num_gates' in combined:
            action_mask = combined['action_prediction_mask']  # [batch_size, max_sar_len]
            gate_counts = combined['num_gates']
            
            debug_print(f"ğŸ”§ ì•¡ì…˜ ë§ˆìŠ¤í¬ ì¡°ì •: {action_mask.shape}")
            
            # ê° íšŒë¡œë³„ë¡œ ì‹¤ì œ ê²Œì´íŠ¸ ìˆ˜ë§Œí¼ë§Œ ì•¡ì…˜ ìœ„ì¹˜ë¥¼ Trueë¡œ ì„¤ì •
            for b in range(batch_size):
                actual_gates = gate_counts[b]
                actual_sar_len = actual_gates * 3 + 1
                
                # ì „ì²´ ë§ˆìŠ¤í¬ë¥¼ Falseë¡œ ì´ˆê¸°í™”
                action_mask[b] = False
                
                # ì‹¤ì œ ì•¡ì…˜ ìœ„ì¹˜ë§Œ Trueë¡œ ì„¤ì • (1::3 íŒ¨í„´)
                for i in range(actual_gates):
                    action_pos = i * 3 + 1  # 1, 4, 7, 10...
                    if action_pos < max_sar_len:
                        action_mask[b, action_pos] = True
            
            combined['action_prediction_mask'] = action_mask
            
            # ê²€ì¦
            total_true = action_mask.sum().item()
            expected_true = sum(gate_counts)
            debug_print(f"âœ… ì•¡ì…˜ ë§ˆìŠ¤í¬ ê²€ì¦: True ìˆ˜ {total_true}, ì˜ˆìƒ {expected_true}")
            
            if total_true != expected_true:
                raise ValueError(f"âŒ ì•¡ì…˜ ë§ˆìŠ¤í¬ ë¶ˆì¼ì¹˜! True ìˆ˜: {total_true}, ì˜ˆìƒ: {expected_true}")
        
        return combined
    
    def _combine_batch_results(self, batch_results: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """ğŸš€ ì—„ê²©í•œ ê²€ì¦ì„ í†µí•œ ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸°"""
        if not batch_results:
            raise ValueError("âŒ CRITICAL ERROR: ë°°ì¹˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
        
        batch_size = len(batch_results)
        combined = {}
        
        # ğŸš€ í•„ìˆ˜ ë©”íƒ€ë°ì´í„° ê²€ì¦
        required_meta_keys = ['circuit_id', 'num_qubits', 'num_gates']
        for key in required_meta_keys:
            if key not in batch_results[0]:
                raise ValueError(f"âŒ CRITICAL ERROR: í•„ìˆ˜ ë©”íƒ€ë°ì´í„° '{key}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ğŸš€ ëª¨ë“  íšŒë¡œì—ì„œ ë©”íƒ€ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        for idx, result in enumerate(batch_results):
            for key in required_meta_keys:
                if key not in result:
                    raise ValueError(f"âŒ CRITICAL ERROR: íšŒë¡œ {idx}ì—ì„œ ë©”íƒ€ë°ì´í„° '{key}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # ğŸš€ ë°ì´í„° íƒ€ì… ë° ê°’ ê²€ì¦
                if key == 'num_qubits':
                    if not isinstance(result[key], int) or result[key] <= 0:
                        raise ValueError(f"âŒ CRITICAL ERROR: íšŒë¡œ {idx}ì˜ num_qubitsê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {result[key]}")
                elif key == 'num_gates':
                    if not isinstance(result[key], int) or result[key] <= 0:
                        raise ValueError(f"âŒ CRITICAL ERROR: íšŒë¡œ {idx}ì˜ num_gatesê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {result[key]}")
        
        # í…ì„œ ë°ì´í„° í•©ì¹˜ê¸°
        tensor_keys = ['input_sequence', 'attention_mask', 'action_prediction_mask', 
                      'state_embedded', 'action_embedded', 'reward_embedded', 
                      'target_actions', 'target_qubits', 'target_params']
        
        for key in tensor_keys:
            if key in batch_results[0]:
                # íŒ¨ë”©ì„ í†µí•œ ë°°ì¹˜ ì²˜ë¦¬ (ê°€ë³€ ê¸¸ì´ ì§€ì›)
                tensors = [result[key] for result in batch_results]
                
                # ğŸ” 215 ê¸¸ì´ ë¯¸ìŠ¤í„°ë¦¬ í•´ê²°ì„ ìœ„í•œ ë””ë²„ê¹…
                if key == 'action_prediction_mask':
                    debug_print(f"ğŸ” === 215 ê¸¸ì´ ë¯¸ìŠ¤í„°ë¦¬ ì¶”ì  ===")
                    debug_print(f"   í‚¤: {key}")
                    debug_print(f"   í…ì„œ ê°œìˆ˜: {len(tensors)}")
                    for i, tensor in enumerate(tensors[:5]):  # ì²˜ìŒ 5ê°œë§Œ
                        debug_print(f"   í…ì„œ {i} í˜•íƒœ: {tensor.shape}")
                    
                    # ìµœëŒ€ ê¸¸ì´ ê³„ì‚°
                    if tensors:
                        max_len_in_batch = max(t.shape[-1] if t.dim() > 0 else 0 for t in tensors)
                        debug_print(f"   ë°°ì¹˜ ë‚´ ìµœëŒ€ ê¸¸ì´: {max_len_in_batch}")
                
                combined[key] = self._pad_and_stack_tensors(tensors)
        
        # ğŸš€ ì—„ê²©í•œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë° ê²€ì¦
        meta_keys = ['circuit_id', 'num_qubits', 'num_gates', 'episode_time_len', 'sar_sequence_len']
        for key in meta_keys:
            if key in batch_results[0]:
                meta_list = [result[key] for result in batch_results]
                
                # ğŸš€ ë©”íƒ€ë°ì´í„° ì™„ì „ì„± ê²€ì¦
                if len(meta_list) != batch_size:
                    raise ValueError(f"âŒ CRITICAL ERROR: ë©”íƒ€ë°ì´í„° '{key}' ê¸¸ì´ ë¶ˆì¼ì¹˜! ì˜ˆìƒ: {batch_size}, ì‹¤ì œ: {len(meta_list)}")
                
                # ğŸš€ None ê°’ ê²€ì¦
                if any(item is None for item in meta_list):
                    none_indices = [i for i, item in enumerate(meta_list) if item is None]
                    raise ValueError(f"âŒ CRITICAL ERROR: ë©”íƒ€ë°ì´í„° '{key}'ì—ì„œ None ê°’ ë°œê²¬! ì¸ë±ìŠ¤: {none_indices}")
                
                combined[key] = meta_list
        
        # ğŸ”§ ì•¡ì…˜ ë§ˆìŠ¤í¬ ë°°ì¹˜ë³„ ì¡°ì • (ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ í›„ ì‹¤í–‰)
        debug_print(f"ğŸ” ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ í›„ í‚¤ í™•ì¸: {list(combined.keys())}")
        if 'action_prediction_mask' in combined and 'num_gates' in combined:
            debug_print(f"ğŸ”§ ì•¡ì…˜ ë§ˆìŠ¤í¬ ë°°ì¹˜ë³„ ì¡°ì • ì‹œì‘")
            debug_print(f"   ì›ë³¸ ë§ˆìŠ¤í¬ í˜•íƒœ: {combined['action_prediction_mask'].shape}")
            debug_print(f"   num_gates: {combined['num_gates']}")
            
            adjusted_mask, actual_action_counts = self._adjust_action_mask_for_batch(
                combined['action_prediction_mask'], 
                combined['num_gates']
            )
            combined['action_prediction_mask'] = adjusted_mask
            
            # ğŸ”§ ë©”íƒ€ë°ì´í„°ë¥¼ ì‹¤ì œ ì•¡ì…˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸ (ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ ë°˜ì˜)
            debug_print(f"ğŸ”§ ë©”íƒ€ë°ì´í„° num_gates ì—…ë°ì´íŠ¸:")
            debug_print(f"   ì›ë³¸ num_gates í•©: {sum(combined['num_gates'])}")
            debug_print(f"   ì‹¤ì œ ì•¡ì…˜ ìˆ˜ í•©: {actual_action_counts}")
            
            # ê° íšŒë¡œë³„ë¡œ ì‹¤ì œ ì‚¬ìš©ëœ ì•¡ì…˜ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
            actual_gates_per_circuit = []
            action_idx = 0
            for batch_idx, original_gates in enumerate(combined['num_gates']):
                # í•´ë‹¹ íšŒë¡œì˜ ì‹¤ì œ ì•¡ì…˜ ìˆ˜ ê³„ì‚°
                sar_seq_len = original_gates * 3
                actual_seq_len = min(sar_seq_len + 1, combined['action_prediction_mask'].shape[1])
                actual_actions = len(torch.arange(1, actual_seq_len, 3))
                actual_gates_per_circuit.append(actual_actions)
                debug_print(f"   íšŒë¡œ {batch_idx}: {original_gates} â†’ {actual_actions} ê²Œì´íŠ¸")
            
            combined['num_gates'] = actual_gates_per_circuit
        else:
            debug_print(f"âš ï¸ ì•¡ì…˜ ë§ˆìŠ¤í¬ ì¡°ì • ê±´ë„ˆëœ€:")
            debug_print(f"   action_prediction_mask ì¡´ì¬: {'action_prediction_mask' in combined}")
            debug_print(f"   num_gates ì¡´ì¬: {'num_gates' in combined}")
        
        # ğŸš€ ìµœì¢… ì¼ê´€ì„± ê²€ì¦ (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
        if DEBUG_MODE and 'num_qubits' in combined and 'num_gates' in combined:
            total_expected_gates = sum(combined['num_gates'])
            
            # target_actions í…ì„œê°€ ìˆë‹¤ë©´ í¬ê¸° ê²€ì¦
            if 'target_actions' in combined:
                actual_total_elements = combined['target_actions'].numel()
                print(f"ğŸ” ë°°ì¹˜ ê²€ì¦: ì˜ˆìƒ ê²Œì´íŠ¸ ìˆ˜ {total_expected_gates}, ì‹¤ì œ ìš”ì†Œ ìˆ˜ {actual_total_elements}")
        
        # ë°°ì¹˜ ìƒì„± ì™„ë£Œ ë¡œê·¸ë„ ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ
        if DEBUG_MODE:
            print(f"âœ… ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {batch_size}ê°œ íšŒë¡œ, íë¹— ìˆ˜ {combined['num_qubits']}, ê²Œì´íŠ¸ ìˆ˜ {combined['num_gates']}")
        
        # ğŸš€ NEW: ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œì— ì €ì¥
        if self.enable_cache and 'circuit_id' in combined and 'num_qubits' in combined and 'num_gates' in combined:
            self._cache_batch_metadata(combined)
        
        return combined
    
    def _process_single_circuit_with_padding(self, circuit_spec: CircuitSpec, max_seq_len: int) -> Dict[str, torch.Tensor]:
        """ğŸš€ ë‹¨ì¼ íšŒë¡œ ì²˜ë¦¬ (íŒ¨ë”© ê¸¸ì´ ì§€ì •)"""
        
        # ğŸš€ ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(circuit_spec)
        
        # ğŸš€ ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„ (íŒ¨ë”© ê¸¸ì´ í¬í•¨)
        padded_cache_key = f"{cache_key}_padded_{max_seq_len}"
        cached_result = self._get_from_cache(padded_cache_key)
        if cached_result is not None:
            return cached_result
        
        # ğŸš€ ìºì‹œ ë¯¸ìŠ¤ - ìƒˆë¡œ ê³„ì‚°
        start_time = time.time()
        
        # 1. Grid Encoderë¡œ íšŒë¡œ ì¸ì½”ë”©
        encoded_data = self.grid_encoder.encode(circuit_spec)
        
        # 2. ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ ê·¸ë¦¬ë“œ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜
        grid_matrix_data = self.grid_encoder.to_grid_matrix(encoded_data)
        
        # 3. ğŸš€ NEW: íŒ¨ë”© ê¸¸ì´ë¥¼ í¬í•¨í•œ Decision Transformer Embedding ì ìš©
        original_gate_count = len(circuit_spec.gates)
        dt_results = self.dt_embedding.process_grid_matrix_data_simple(grid_matrix_data, original_gate_count, max_seq_len)
        
        # 4. ë©”íƒ€ë°ì´í„° ì¶”ê°€
        dt_results.update({
            'circuit_id': circuit_spec.circuit_id,
            'num_qubits': circuit_spec.num_qubits,
            'num_gates': original_gate_count,
            'episode_time_len': dt_results.get('episode_time_len', original_gate_count),
            'sar_sequence_len': original_gate_count * 3
        })
        
        # ğŸš€ ìºì‹œì— ì €ì¥
        processing_time = time.time() - start_time
        self._put_to_cache(padded_cache_key, dt_results)
        
        debug_print(f"  íšŒë¡œ {circuit_spec.circuit_id} ì²˜ë¦¬ ì™„ë£Œ (íŒ¨ë”©: {max_seq_len}, ì‹œê°„: {processing_time:.3f}ì´ˆ)")
        
        return dt_results
    
    def _generate_batch_cache_key(self, circuit_ids: List[str]) -> str:
        """ğŸš€ ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œ í‚¤ ìƒì„±"""
        # íšŒë¡œ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ì—¬ ìˆœì„œì— ë¬´ê´€í•œ í‚¤ ìƒì„±
        sorted_ids = sorted(circuit_ids)
        key_string = "|".join(sorted_ids)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _cache_batch_metadata(self, batch_data: Dict[str, Any]) -> None:
        """ğŸš€ ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œì— ì €ì¥"""
        if not self.enable_cache:
            return
            
        circuit_ids = batch_data['circuit_id']
        num_qubits = batch_data['num_qubits']
        num_gates = batch_data['num_gates']
        
        cache_key = self._generate_batch_cache_key(circuit_ids)
        
        with self._cache_lock:
            # ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = BatchMetadata(
                circuit_ids=circuit_ids.copy(),
                num_qubits=num_qubits.copy(),
                num_gates=num_gates.copy(),
                batch_size=len(circuit_ids),
                timestamp=time.time()
            )
            
            # ìºì‹œì— ì €ì¥
            self._batch_metadata_cache[cache_key] = metadata
            
            # LRU ê´€ë¦¬
            if cache_key in self._batch_cache_access_order:
                self._batch_cache_access_order.remove(cache_key)
            self._batch_cache_access_order.append(cache_key)
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            while len(self._batch_metadata_cache) > self._max_batch_cache_size:
                oldest_key = self._batch_cache_access_order.pop(0)
                del self._batch_metadata_cache[oldest_key]
    
    def get_cached_batch_metadata(self, circuit_ids: List[str]) -> Optional[BatchMetadata]:
        """ğŸš€ ìºì‹œëœ ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        if not self.enable_cache:
            return None
            
        cache_key = self._generate_batch_cache_key(circuit_ids)
        
        with self._cache_lock:
            self._batch_cache_stats['total'] += 1
            
            if cache_key in self._batch_metadata_cache:
                # ìºì‹œ íˆíŠ¸
                self._batch_cache_stats['hits'] += 1
                
                # LRU ì—…ë°ì´íŠ¸
                self._batch_cache_access_order.remove(cache_key)
                self._batch_cache_access_order.append(cache_key)
                
                return self._batch_metadata_cache[cache_key]
            else:
                # ìºì‹œ ë¯¸ìŠ¤
                self._batch_cache_stats['misses'] += 1
                return None
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ğŸš€ ìºì‹œ í†µê³„ ë°˜í™˜ (ì„ë² ë”© + ë°°ì¹˜ ë©”íƒ€ë°ì´í„°)"""
        with self._cache_lock:
            hit_rate = self._cache_stats['hits'] / max(self._cache_stats['total'], 1) * 100
            batch_hit_rate = self._batch_cache_stats['hits'] / max(self._batch_cache_stats['total'], 1) * 100
            
            return {
                'cache_enabled': self.enable_cache,
                # ì„ë² ë”© ìºì‹œ í†µê³„
                'embedding_cache': {
                    'total_requests': self._cache_stats['total'],
                    'cache_hits': self._cache_stats['hits'],
                    'cache_misses': self._cache_stats['misses'],
                    'hit_rate_percent': hit_rate,
                    'cache_size': len(self._memory_cache),
                },
                # ğŸš€ NEW: ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œ í†µê³„
                'batch_metadata_cache': {
                    'total_requests': self._batch_cache_stats['total'],
                    'cache_hits': self._batch_cache_stats['hits'],
                    'cache_misses': self._batch_cache_stats['misses'],
                    'hit_rate_percent': batch_hit_rate,
                    'cache_size': len(self._batch_metadata_cache),
                },
                'max_cache_size': self._max_cache_size
            }
    
    def clear_cache(self) -> None:
        # ğŸ”§ ê°•ì œ ìºì‹œ ë¦¬ì…‹ (ì•¡ì…˜ ë§ˆìŠ¤í¬ ë°°ì¹˜ë³„ ì¡°ì • ë°˜ì˜)
        if self.enable_cache:
            print("ğŸ”§ ê°•ì œ ìºì‹œ ë¦¬ì…‹: ì•¡ì…˜ ë§ˆìŠ¤í¬ ë°°ì¹˜ë³„ ì¡°ì • ë¡œì§ ì¶”ê°€ - ê° íšŒë¡œì˜ ì‹¤ì œ ê¸¸ì´ì— ë§ëŠ” ë§ˆìŠ¤í¬ ìƒì„±")
            self._memory_cache.clear()
            self._batch_metadata_cache.clear()
            self._cache_access_order.clear()
            self._cache_stats = {'hits': 0, 'misses': 0, 'total': 0}
            self._batch_cache_access_order.clear()
            self._batch_cache_stats = {'hits': 0, 'misses': 0, 'total': 0}
            
            # NEW: ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œ ì´ˆê¸°í™”
            self._batch_metadata_cache.clear()
            self._batch_cache_access_order.clear()
            self._batch_cache_stats = {'hits': 0, 'misses': 0, 'total': 0}
            
            print("ëª¨ë“  ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤ (ì„ë² ë”© + ë°°ì¹˜ ë©”íƒ€ë°ì´í„°).")
            print("ğŸš€ ëª¨ë“  ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤ (ì„ë² ë”© + ë°°ì¹˜ ë©”íƒ€ë°ì´í„°).")
    
    def print_cache_stats(self) -> None:
        """ğŸš€ ìºì‹œ í†µê³„ ì¶œë ¥ (ì„ë² ë”© + ë°°ì¹˜ ë©”íƒ€ë°ì´í„°)"""
        stats = self.get_cache_stats()
        print(f"\nğŸš€ ìºì‹œ í†µê³„:")
        print(f"   - ìºì‹œ í™œì„±í™”: {stats['cache_enabled']}")
        
        # ì„ë² ë”© ìºì‹œ í†µê³„
        embedding_stats = stats['embedding_cache']
        print(f"\n   ğŸ“Š ì„ë² ë”© ìºì‹œ:")
        print(f"      - ì´ ìš”ì²­: {embedding_stats['total_requests']}")
        print(f"      - ìºì‹œ íˆíŠ¸: {embedding_stats['cache_hits']}")
        print(f"      - ìºì‹œ ë¯¸ìŠ¤: {embedding_stats['cache_misses']}")
        print(f"      - íˆíŠ¸ìœ¨: {embedding_stats['hit_rate_percent']:.1f}%")
        print(f"      - ìºì‹œ í¬ê¸°: {embedding_stats['cache_size']}/{stats['max_cache_size']}")
        
        # ğŸš€ NEW: ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œ í†µê³„
        batch_stats = stats['batch_metadata_cache']
        print(f"\n   ğŸ¯ ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ìºì‹œ:")
        print(f"      - ì´ ìš”ì²­: {batch_stats['total_requests']}")
        print(f"      - ìºì‹œ íˆíŠ¸: {batch_stats['cache_hits']}")
        print(f"      - ìºì‹œ ë¯¸ìŠ¤: {batch_stats['cache_misses']}")
        print(f"      - íˆíŠ¸ìœ¨: {batch_stats['hit_rate_percent']:.1f}%")
        print(f"      - ìºì‹œ í¬ê¸°: {batch_stats['cache_size']}/{self._max_batch_cache_size}")
    
    def _pad_and_stack_tensors(self, tensors: List[torch.Tensor]) -> torch.Tensor:
        """
        ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ í…ì„œë“¤ì„ íŒ¨ë”©í•˜ì—¬ ë°°ì¹˜ë¡œ í•©ì¹˜ê¸°
        
        Args:
            tensors: ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ í…ì„œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            íŒ¨ë”©ëœ ë°°ì¹˜ í…ì„œ
        """
        if not tensors:
            return torch.tensor([])
        
        # ìµœëŒ€ í¬ê¸° ê³„ì‚°
        max_dims = []
        for dim in range(len(tensors[0].shape)):
            max_size = max(tensor.shape[dim] for tensor in tensors)
            max_dims.append(max_size)
            
            # ğŸ” ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¨ì› ë””ë²„ê¹… (ë§ˆì§€ë§‰ ì°¨ì›)
            if dim == len(tensors[0].shape) - 1:
                debug_print(f"ğŸ” === ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¨ì› ë¶„ì„ ===")
                debug_print(f"   ì°¨ì› {dim} (ì‹œí€€ìŠ¤ ê¸¸ì´)")
                debug_print(f"   ìµœëŒ€ í¬ê¸°: {max_size}")
                debug_print(f"   ê°œë³„ í…ì„œ ê¸¸ì´ë“¤:")
                for i, tensor in enumerate(tensors[:10]):  # ì²˜ìŒ 10ê°œë§Œ
                    debug_print(f"     í…ì„œ {i}: {tensor.shape[dim]}")
                if len(tensors) > 10:
                    debug_print(f"     ... (ì´ {len(tensors)}ê°œ í…ì„œ)")
                debug_print(f"   ëª¨ë“  ê¸¸ì´: {[t.shape[dim] for t in tensors]}")
        
        # Boolean í…ì„œ ì—¬ë¶€ í™•ì¸
        is_bool_tensor = tensors[0].dtype == torch.bool
        
        # íŒ¨ë”©ëœ í…ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        padded_tensors = []
        for tensor in tensors:
            # íŒ¨ë”© í¬ê¸° ê³„ì‚°
            padding = []
            for dim in reversed(range(len(tensor.shape))):
                pad_size = max_dims[dim] - tensor.shape[dim]
                padding.extend([0, pad_size])  # (left, right) padding
            
            # íŒ¨ë”© ì ìš©
            if any(p > 0 for p in padding):
                # Boolean ë§ˆìŠ¤í¬ì˜ ê²½ìš° False(0)ë¡œ íŒ¨ë”©, ë‹¤ë¥¸ í…ì„œëŠ” 0ìœ¼ë¡œ íŒ¨ë”©
                pad_value = False if is_bool_tensor else 0
                padded_tensor = torch.nn.functional.pad(tensor, padding, value=pad_value)
            else:
                padded_tensor = tensor
            
            padded_tensors.append(padded_tensor)
        
        # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ í•©ì¹˜ê¸°
        return torch.stack(padded_tensors, dim=0)
    
    def _adjust_action_mask_for_batch(self, action_mask: torch.Tensor, num_gates_list: List[int]) -> torch.Tensor:
        """
        ë°°ì¹˜ë³„ë¡œ ì•¡ì…˜ ë§ˆìŠ¤í¬ë¥¼ ê° íšŒë¡œì˜ ì‹¤ì œ ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
        
        Args:
            action_mask: [batch_size, max_seq_len] íŒ¨ë”©ëœ ì•¡ì…˜ ë§ˆìŠ¤í¬
            num_gates_list: ê° íšŒë¡œì˜ ì‹¤ì œ ê²Œì´íŠ¸ ìˆ˜ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì¡°ì •ëœ ì•¡ì…˜ ë§ˆìŠ¤í¬
        """
        debug_print(f"ğŸ”§ === ì•¡ì…˜ ë§ˆìŠ¤í¬ ë°°ì¹˜ë³„ ì¡°ì • í•¨ìˆ˜ ì‹œì‘ ===")
        debug_print(f"   action_mask íƒ€ì…: {type(action_mask)}")
        debug_print(f"   action_mask í˜•íƒœ: {action_mask.shape if hasattr(action_mask, 'shape') else 'No shape'}")
        debug_print(f"   action_mask ë‚´ìš©: {action_mask}")
        debug_print(f"   num_gates_list: {num_gates_list}")
        
        # í˜•íƒœ ê²€ì¦ ë° ìˆ˜ì •
        if not hasattr(action_mask, 'shape'):
            raise ValueError(f"âŒ action_maskê°€ í…ì„œê°€ ì•„ë‹™ë‹ˆë‹¤: {type(action_mask)}")
        
        # 3ì°¨ì›ì¸ ê²½ìš° 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (ì¤‘ê°„ ì°¨ì› ì œê±°)
        if len(action_mask.shape) == 3:
            debug_print(f"   3ì°¨ì› ì•¡ì…˜ ë§ˆìŠ¤í¬ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜: {action_mask.shape}")
            action_mask = action_mask.squeeze(1)  # ì¤‘ê°„ ì°¨ì› ì œê±°

        
        batch_size, max_seq_len = action_mask.shape
        debug_print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}, ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_seq_len}")
        
        adjusted_mask = torch.zeros_like(action_mask)
        total_expected_actions = 0
        
        for batch_idx, num_gates in enumerate(num_gates_list):
            # SAR êµ¬ì¡°ì—ì„œ ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
            # num_gatesëŠ” ì‹¤ì œ ê²Œì´íŠ¸ ìˆ˜ì´ë¯€ë¡œ, SAR ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” num_gates * 3
            sar_seq_len = num_gates * 3
            actual_seq_len = min(sar_seq_len + 1, max_seq_len)  # EOS í† í° í¬í•¨, ìµœëŒ€ ê¸¸ì´ ì œí•œ
            
            # ì•¡ì…˜ ìœ„ì¹˜ë§Œ Trueë¡œ ì„¤ì • (1::3 íŒ¨í„´)
            action_positions = torch.arange(1, actual_seq_len, 3, device=action_mask.device)
            action_positions = action_positions[action_positions < max_seq_len]  # ë²”ìœ„ ë‚´ ìœ„ì¹˜ë§Œ
            
            if len(action_positions) > 0:
                adjusted_mask[batch_idx, action_positions] = True
                total_expected_actions += len(action_positions)
            
            debug_print(f"   íšŒë¡œ {batch_idx}: gates={num_gates}, sar_len={sar_seq_len}, actions={len(action_positions)}")
        
        debug_print(f"ğŸ”§ ì•¡ì…˜ ë§ˆìŠ¤í¬ ë°°ì¹˜ë³„ ì¡°ì • ì™„ë£Œ:")
        debug_print(f"   ì›ë³¸ True ìˆ˜: {action_mask.sum().item()}")
        debug_print(f"   ì¡°ì • í›„ True ìˆ˜: {adjusted_mask.sum().item()}")
        debug_print(f"   ì˜ˆìƒ True ìˆ˜: {sum(num_gates_list)} (ë©”íƒ€ë°ì´í„° í•©)")
        debug_print(f"   ì‹¤ì œ ì•¡ì…˜ ìˆ˜: {total_expected_actions}")
        
        return adjusted_mask, total_expected_actions


def create_embedding_pipeline(config: EmbeddingConfig = None, enable_cache: bool = True) -> EmbeddingPipeline:
    """ğŸš€ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ íŒ©í† ë¦¬ í•¨ìˆ˜ (ìºì‹± ì§€ì›)"""
    if config is None:
        config = EmbeddingConfig()
    
    # ğŸš€ ìºì‹±ì´ í™œì„±í™”ëœ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = EmbeddingPipeline(config, enable_cache=enable_cache)
    
    if enable_cache:
        print(f"ğŸš€ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ! (ìºì‹±: {enable_cache}, ìµœëŒ€ ìºì‹œ í¬ê¸°: {pipeline._max_cache_size})")
    
    return pipeline


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from quantum_circuit_dataset import DatasetManager
    
    # ë°ì´í„°ì…‹ ë¡œë”©
    manager = DatasetManager("OAT_Model/dataset/unified_batch_experiment_results_with_circuits.json")
    circuit_specs = manager.parse_circuits()
    
    # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ìƒì„±
    config = EmbeddingConfig(d_model=256)  # NEW: ì‹±ê¸€í†¤ì—ì„œ ê°€ì ¸ì˜¨ gate ìˆ˜ë¡œ ì„ë² ë”© ë ˆì´ì–´ ì´ˆê¸°í™”
    pipeline = create_embedding_pipeline(config)
    
    # ë‹¨ì¼ íšŒë¡œ í…ŒìŠ¤íŠ¸
    if circuit_specs:
        print("Testing single circuit processing...")
        result = pipeline.process_single_circuit(circuit_specs[0])
        
        print(f"Circuit ID: {result['circuit_id']}")
        print(f"Input sequence shape: {result['input_sequence'].shape}")
        print(f"Attention mask shape: {result['attention_mask'].shape}")
        
        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        print("\nTesting batch processing...")
        batch_result = pipeline.process_batch(circuit_specs[:3])
        
        if batch_result:
            print(f"Batch input sequence shape: {batch_result['input_sequence'].shape}")
            print(f"Batch attention mask shape: {batch_result['attention_mask'].shape}")
